import os
import sys
import uuid
import json
import time
import logging
import tempfile
import traceback
import threading
import concurrent.futures
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple
import pandas as pd
import numpy as np
from flask import (
    Flask,
    render_template,
    request,
    jsonify,
    send_file,
    send_from_directory,
    url_for,
    flash,
    redirect,
)
import shutil
import postprocessor
import processor
import feishu_utils
import re

# 定义配置文件路径
CONFIG_FILE = "config.json"

# 初始化 Flask 应用
app = Flask(__name__)

# --- 设置 Secret Key --- #
# 对于生产环境，强烈建议从环境变量或配置文件读取一个固定且保密的密钥
# 例如: app.secret_key = os.environ.get('FLASK_SECRET_KEY', 'a_very_secret_dev_key')
# 为了方便本地运行，这里使用 os.urandom 生成一个随机密钥
# 注意：每次重启服务密钥都会改变，这会导致之前的 session 失效
app.secret_key = os.urandom(24)
print(f"Flask app secret key set randomly.")

# 配置上传文件存储路径
app.config["UPLOAD_FOLDER"] = "uploads"
# 配置处理结果输出路径
app.config["OUTPUT_FOLDER"] = "outputs"
# 确保上传和输出目录存在
os.makedirs(app.config["UPLOAD_FOLDER"], exist_ok=True)
os.makedirs(app.config["OUTPUT_FOLDER"], exist_ok=True)

# 使用字典在内存中存储任务状态和进度 (简单实现)
# 注意：在生产环境中，建议使用更健壮的方案，如 Redis 或数据库
tasks = {}

# --- 数据处理模块的配置 (现在作为后备默认值) ---
# 主要配置将从前端请求中获取
DEFAULT_PROCESSOR_CONFIG = {
    # 定义最终输出 Excel 文件需要包含的目标列名
    "TARGET_COLUMNS": [
        "公司名称",
        "联系人",
        "职位",
        "电话",
        "来源",  # *** 新增：将"来源"添加到默认目标列 ***
        # "邮箱",
        # "地址",
    ],
    # 从环境变量获取 DeepSeek API Key，如果未设置则使用默认值 (仅供示例)
    "DEEPSEEK_API_KEY": os.environ.get(
        "DEEPSEEK_API_KEY",
        "sk-9df835c781904b2289663567d05c94d9",  # <<<--- 从环境变量获取或直接替换
    ),
    # DeepSeek API 的接入点
    "DEEPSEEK_API_ENDPOINT": "https://api.deepseek.com/chat/completions",
    # 调用 LLM API 时，每次处理的行数 (批处理大小)
    "BATCH_SIZE": 160,  # 可根据需要调整
    # LLM API 调用时，允许生成的最大 token 数量
    "MAX_COMPLETION_TOKENS": 8192,  # 可根据 BATCH_SIZE 和模型限制调整
    "API_TIMEOUT": 180,  # 默认API超时
}

# --- 飞书配置 (后备默认值) ---
DEFAULT_FEISHU_CONFIG = {
    "APP_ID": "cli_a36634dc16b8d00e",
    "APP_SECRET": "RoXYTnSBGGsLLyvONbSCYe15Jm6bv5Xn",
    "APP_TOKEN": "XyUFbxc8JaDkTJscEigcbkxgnqe",
    "TABLE_IDS": [
        "tblEGrUKq8KKOPAc",
        "tbltHzIYuD95qhGv",
        "tbl94LyQMqtz0X45",
        "tblJPUJJEQw0VEYz",
    ],
    "ADD_TARGET_TABLE_IDS": [],  # 新增：用于添加记录的特定 Table ID 列表 (可选)
    "COMPANY_NAME_COLUMN": "企业名称",
    "PHONE_NUMBER_COLUMN": "电话",
    "REMARK_COLUMN_NAME": "备注",
    "RELATED_COMPANY_COLUMN_NAME": "关联公司名称(LLM)",  # 定义新列名
}

# --- 后处理默认列配置 (新) ---
DEFAULT_POST_PROCESSING_COLS = {
    "check_duplicate_phones": {
        "post_phone_col_for_dup_phones": "电话"  # 默认尝试使用"电话"列
    },
    "check_duplicate_companies": {
        "post_phone_col_for_dup_comp": "电话",  # 默认尝试使用"电话"列
        "post_company_col_for_dup_comp": "公司名称",  # 默认尝试使用"公司名称"列
    },
}


# --- 加载/初始化配置 ---
def load_config():
    """尝试从 config.json 加载配置，如果失败则使用默认值并保存。"""
    try:
        if os.path.exists(CONFIG_FILE):
            with open(CONFIG_FILE, "r", encoding="utf-8") as f:
                config_from_file = json.load(f)
                # 简单验证一下结构
                if (
                    "llm_config" in config_from_file
                    and "feishu_config" in config_from_file
                    # 也要检查新的 post_processing_config (为了保存) 和 defaults
                    and "post_processing_config" in config_from_file
                    # and "feishu_config" in config_from_file and "ADD_TARGET_TABLE_ID" in config_from_file["feishu_config"] # Add check for new key? Optional.
                ):
                    print(f"成功从 {CONFIG_FILE} 加载配置。")
                    # 确保即使旧配置文件没有 ADD_TARGET_TABLE_ID 也能正常加载
                    loaded_feishu_config = config_from_file["feishu_config"]
                    if "ADD_TARGET_TABLE_IDS" not in loaded_feishu_config:
                        loaded_feishu_config["ADD_TARGET_TABLE_IDS"] = (
                            DEFAULT_FEISHU_CONFIG["ADD_TARGET_TABLE_IDS"]
                        )

                    # 返回加载的配置
                    return (
                        config_from_file["llm_config"],
                        loaded_feishu_config,  # 使用处理过的 feishu_config
                        config_from_file.get(
                            "post_processing_config",
                            DEFAULT_POST_PROCESSING_COLS.copy(),
                        ),  # 加载保存的后处理默认值
                    )
                else:
                    print(f"警告: {CONFIG_FILE} 文件格式不完整，使用默认配置。")
        else:
            print(f"配置文件 {CONFIG_FILE} 不存在，使用默认配置。")
    except (json.JSONDecodeError, IOError) as e:
        print(f"加载配置文件 {CONFIG_FILE} 时出错: {e}，使用默认配置。")

    # 如果加载失败或文件不存在，则使用默认值并尝试保存
    default_config = {
        "llm_config": DEFAULT_PROCESSOR_CONFIG.copy(),
        "feishu_config": DEFAULT_FEISHU_CONFIG.copy(),
        "post_processing_config": DEFAULT_POST_PROCESSING_COLS.copy(),  # 保存时也包含后处理默认值
    }
    save_config(default_config)  # 尝试写入默认配置
    return DEFAULT_PROCESSOR_CONFIG, DEFAULT_FEISHU_CONFIG, DEFAULT_POST_PROCESSING_COLS


def save_config(config_data):
    """将配置数据写入 config.json 文件。"""
    try:
        with open(CONFIG_FILE, "w", encoding="utf-8") as f:
            json.dump(config_data, f, ensure_ascii=False, indent=4)
        print(f"配置已成功保存到 {CONFIG_FILE}。")
        return True
    except IOError as e:
        print(f"保存配置文件 {CONFIG_FILE} 时出错: {e}")
        return False


# 应用启动时加载配置
CURRENT_LLM_CONFIG, CURRENT_FEISHU_CONFIG, CURRENT_POST_PROCESSING_CONFIG = (
    load_config()
)

# 启动时检查 API Key (可以移到 load_config 内部或保持在这里)
# (保持原样)

# *** 新增：确保加载的配置也包含"来源" ***
if "来源" not in CURRENT_LLM_CONFIG.get("TARGET_COLUMNS", []):
    print("   * 配置加载后：将缺失的 '来源' 添加到 TARGET_COLUMNS。")
    CURRENT_LLM_CONFIG.setdefault("TARGET_COLUMNS", []).append("来源")


# 定义根路由，用于显示主页面 (index.html)
@app.route("/")
def index():
    """
    渲染应用程序主页面，提供文件上传和配置设置界面。

    显示Web界面，允许用户上传Excel/CSV文件进行处理，
    配置LLM、飞书和后处理参数，并查看任务处理进度。
    从服务器加载默认配置参数并传递给模板用于初始化界面表单。

    Returns:
        HTML: 渲染后的index.html页面，包含所有配置参数
    """
    # print(f"Rendering index with post_processing_defaults: {CURRENT_POST_PROCESSING_CONFIG}") # Debug
    return render_template(
        "index.html",
        default_llm_config=CURRENT_LLM_CONFIG,
        default_feishu_config=CURRENT_FEISHU_CONFIG,
        default_post_processing_cols=CURRENT_POST_PROCESSING_CONFIG,  # 将加载的默认后处理配置传递给模板
    )


# 定义文件上传路由，接收 POST 请求
@app.route("/upload", methods=["POST"])
def upload_files():
    """
    处理用户上传的Excel/CSV文件并启动后台处理任务。

    接收前端上传的文件并保存到任务专属目录，解析表单中的LLM、飞书和
    后处理配置参数，创建唯一任务ID，启动后台线程执行数据处理流程。
    处理流程包括LLM处理、飞书数据获取、数据合并和后处理检查。

    Request:
        - files[]: 上传的Excel/CSV文件列表
        - config_target_columns: 目标列名称列表(JSON)
        - config_api_key: DeepSeek API密钥
        - config_batch_size: 批处理大小
        - config_max_tokens: 最大Token数
        - config_api_timeout: API超时时间
        - feishu_*: 多个飞书API配置参数
        - post_process_config: 后处理配置(JSON)

    Returns:
        JSON: 包含task_id的响应，用于前端查询任务进度
    """
    # 基础验证：检查请求中是否包含文件部分
    if "files[]" not in request.files:
        return jsonify({"error": "请求中缺少 files[] 文件部分"}), 400
    # 获取所有上传的文件列表
    files = request.files.getlist("files[]")
    # 验证是否选择了文件
    if not files or all(f.filename == "" for f in files):
        return jsonify({"error": "未选择任何文件"}), 400

    # 为本次任务生成唯一的 ID
    task_id = str(uuid.uuid4())
    # 根据 task_id 创建独立的上传和输出子目录
    upload_dir = os.path.join(app.config["UPLOAD_FOLDER"], task_id)
    output_dir = os.path.join(app.config["OUTPUT_FOLDER"], task_id)
    os.makedirs(upload_dir, exist_ok=True)
    os.makedirs(output_dir, exist_ok=True)

    input_file_paths = []
    try:
        # 遍历上传的文件列表
        for file in files:
            # 确保文件存在且有文件名
            if file and file.filename:
                # 安全考虑：可以使用 secure_filename 清理文件名
                # from werkzeug.utils import secure_filename
                # filename = secure_filename(file.filename)
                filename = file.filename
                # 构造文件保存路径
                filepath = os.path.join(upload_dir, filename)
                # 保存文件
                file.save(filepath)
                # 将保存后的文件路径添加到列表
                input_file_paths.append(filepath)
            else:
                print(f"跳过无效的文件条目: {file}")

        # 如果没有成功保存任何文件
        if not input_file_paths:
            # (可选) 清理已创建的空目录
            try:
                if os.path.exists(upload_dir):
                    os.rmdir(upload_dir)
                if os.path.exists(output_dir):
                    os.rmdir(output_dir)
            except OSError:
                pass  # 忽略清理错误
            return jsonify({"error": "没有有效的上传文件"}), 400

        # --- 从前端请求中获取配置参数 ---
        try:
            # LLM Processor Config
            target_columns_json = request.form.get(
                "config_target_columns", "[]"
            )  # Use new name
            target_columns = json.loads(target_columns_json)
            if not isinstance(target_columns, list) or not all(
                isinstance(s, str) for s in target_columns
            ):
                target_columns = CURRENT_LLM_CONFIG[
                    "TARGET_COLUMNS"
                ]  # Use updated default
                print(
                    f"警告: 前端发送的 target_columns 格式无效，使用服务器默认值: {target_columns}"
                )

            # *** 新增：强制确保"来源"列包含在任务配置中 ***
            source_column_name = "来源"
            if source_column_name not in target_columns:
                print(f"   * 任务配置：强制添加 '{source_column_name}' 到目标列。")
                target_columns.append(source_column_name)
            # *** 新增结束 ***

            # 如果前端未提供 API Key，则使用默认配置中的 Key
            api_key = request.form.get(
                "config_api_key", CURRENT_LLM_CONFIG["DEEPSEEK_API_KEY"]
            ).strip()  # Use new name
            if not api_key:  # 如果用户清空了输入框，仍然使用加载的默认值
                api_key = CURRENT_LLM_CONFIG["DEEPSEEK_API_KEY"]
                print(f"警告: 前端未提供有效 API Key，使用服务器默认 Key (可能无效)")

            # 对于数字类型，增加 try-except 保证健壮性
            try:
                batch_size = int(request.form.get("config_batch_size"))
            except (ValueError, TypeError):
                batch_size = CURRENT_LLM_CONFIG["BATCH_SIZE"]

            try:
                max_tokens = int(request.form.get("config_max_tokens"))
            except (ValueError, TypeError):
                max_tokens = CURRENT_LLM_CONFIG["MAX_COMPLETION_TOKENS"]

            try:
                api_timeout = int(request.form.get("config_api_timeout"))
            except (ValueError, TypeError):
                api_timeout = CURRENT_LLM_CONFIG["API_TIMEOUT"]

            # 可选: 添加更严格的参数验证 (例如 batch_size > 0)
            if batch_size <= 0:
                batch_size = CURRENT_LLM_CONFIG["BATCH_SIZE"]
            if max_tokens <= 0:
                max_tokens = CURRENT_LLM_CONFIG["MAX_COMPLETION_TOKENS"]
            if api_timeout <= 5:
                api_timeout = CURRENT_LLM_CONFIG["API_TIMEOUT"]  # 使用加载的默认超时

            # 获取阿里云百炼API相关配置
            dashscope_api_key = request.form.get(
                "config_dashscope_api_key",
                CURRENT_LLM_CONFIG.get("DASHSCOPE_API_KEY", ""),
            ).strip()

            bailian_model_name = request.form.get(
                "config_bailian_model_name",
                CURRENT_LLM_CONFIG.get("BAILIAN_MODEL_NAME", "qwen-plus"),
            ).strip()

            bailian_completion_window = request.form.get(
                "config_bailian_completion_window",
                CURRENT_LLM_CONFIG.get("BAILIAN_COMPLETION_WINDOW", "24h"),
            ).strip()

            # 创建本次任务的动态配置字典
            dynamic_config = {
                "TARGET_COLUMNS": target_columns,  # 目标列，默认值见DEFAULT_PROCESSOR_CONFIG
                "DEEPSEEK_API_KEY": api_key,  # DeepSeek API Key
                "DEEPSEEK_API_ENDPOINT": CURRENT_LLM_CONFIG.get(
                    "DEEPSEEK_API_ENDPOINT", "https://api.deepseek.com/chat/completions"  # 默认DeepSeek API端点
                ),
                "BATCH_SIZE": batch_size,  # 批处理大小，默认160
                "MAX_COMPLETION_TOKENS": max_tokens,  # 最大token数，默认8192
                "API_TIMEOUT": api_timeout,  # 超时，默认180
                "DASHSCOPE_API_KEY": dashscope_api_key,  # 百炼API Key
                "BAILIAN_MODEL_NAME": CURRENT_LLM_CONFIG.get(
                    "BAILIAN_MODEL_NAME", "qwen-turbo-latest"  # 默认百炼模型名
                ),
                "BAILIAN_COMPLETION_WINDOW": CURRENT_LLM_CONFIG.get(
                    "BAILIAN_COMPLETION_WINDOW", "24h"  # 默认24小时窗口
                ),
                "BAILIAN_BASE_URL": CURRENT_LLM_CONFIG.get(
                    "BAILIAN_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1"  # 默认百炼API地址
                ),
                "BAILIAN_BATCH_ENDPOINT": CURRENT_LLM_CONFIG.get(
                    "BAILIAN_BATCH_ENDPOINT", "/v1/chat/completions"  # 默认百炼批处理端点
                ),
            }
            print(
                f"任务 {task_id} 使用配置: TargetCols={target_columns}, BatchSize={batch_size}, MaxTokens={max_tokens}, Timeout={api_timeout}"
            )

            # 打印是否使用百炼API
            print(
                f"任务 {task_id} 使用百炼API: {bool(dashscope_api_key and dashscope_api_key.startswith('sk-'))}, "
                f"API Key状态: {'有效' if dashscope_api_key and dashscope_api_key.startswith('sk-') else '无效或未提供'}"
            )

            # Feishu Config (使用加载的默认值)
            feishu_app_id = request.form.get(
                "feishu_app_id", CURRENT_FEISHU_CONFIG.get("APP_ID", "")  # 飞书AppId，默认空
            ).strip()
            feishu_app_secret = request.form.get(
                "feishu_app_secret", CURRENT_FEISHU_CONFIG.get("APP_SECRET", "")  # 飞书AppSecret，默认空
            ).strip()
            feishu_app_token = request.form.get(
                "feishu_app_token", CURRENT_FEISHU_CONFIG.get("APP_TOKEN", "")  # 飞书AppToken，默认空
            ).strip()
            feishu_table_ids_json = request.form.get("feishu_table_ids", "[]")
            feishu_table_ids = json.loads(feishu_table_ids_json)
            if not isinstance(feishu_table_ids, list) or not all(
                isinstance(s, str) for s in feishu_table_ids
            ):
                feishu_table_ids = CURRENT_FEISHU_CONFIG["TABLE_IDS"]
                print(f"警告: 前端发送的 feishu_table_ids 格式无效，使用加载的默认值。")
            # 不再从表单读取这些列名，直接使用加载的默认值
            feishu_company_col = CURRENT_FEISHU_CONFIG.get(
                "COMPANY_NAME_COLUMN", "企业名称"
            )
            feishu_phone_col = CURRENT_FEISHU_CONFIG.get("PHONE_NUMBER_COLUMN", "电话")
            feishu_remark_col = CURRENT_FEISHU_CONFIG.get("REMARK_COLUMN_NAME", "备注")

            # Post Processing Choices (now a config object)
            post_process_config_json = request.form.get(
                "post_process_config", "{}"
            )  # Get the JSON string
            try:
                post_process_config = json.loads(post_process_config_json)  # Parse JSON
                if not isinstance(post_process_config, dict):
                    print(
                        f"警告: 前端发送的 post_process_config 不是有效的 JSON 对象，使用空配置。"
                    )
                    post_process_config = {}
            except json.JSONDecodeError:
                print(f"警告: 前端发送的 post_process_config 格式无效，使用空配置。")
                post_process_config = {}

            # 合并所有配置到一个字典
            task_config = {
                "llm_config": dynamic_config,  # LLM 相关配置
                "feishu_config": {
                    "APP_ID": feishu_app_id,
                    "APP_SECRET": feishu_app_secret,
                    "APP_TOKEN": feishu_app_token,
                    "TABLE_IDS": feishu_table_ids,
                    "ADD_TARGET_TABLE_IDS": CURRENT_FEISHU_CONFIG[
                        "ADD_TARGET_TABLE_IDS"
                    ],  # Add the new list
                    "COMPANY_NAME_COLUMN": feishu_company_col,
                    "PHONE_NUMBER_COLUMN": feishu_phone_col,
                    "REMARK_COLUMN_NAME": feishu_remark_col,
                    "RELATED_COMPANY_COLUMN_NAME": CURRENT_FEISHU_CONFIG[
                        "RELATED_COMPANY_COLUMN_NAME"
                    ],  # Ensure this is included
                },
                "post_processing_config": post_process_config,  # Store the entire post-processing config object
            }
            # 打印部分配置信息用于调试
            print(
                f"任务 {task_id} 使用配置: LLM Batch={dynamic_config.get('BATCH_SIZE')}, "
                f"Feishu Tables={len(feishu_table_ids)}, Add Targets={len(CURRENT_FEISHU_CONFIG['ADD_TARGET_TABLE_IDS'])}, "
                f"Feishu Remark Col={feishu_remark_col}, "
                f"Post Config={post_process_config}"  # Updated log
            )

        except (ValueError, TypeError, json.JSONDecodeError) as config_err:
            print(f"解析前端配置参数时出错: {config_err}，将使用默认配置。")
            # 如果解析出错，回退到默认配置 (需要合并默认的 LLM 和 Feishu 配置)
            task_config = {
                "llm_config": CURRENT_LLM_CONFIG.copy(),
                "feishu_config": CURRENT_FEISHU_CONFIG.copy(),
                "post_processing_config": {},  # Default empty post-processing config on error
            }

        # 定义最终输出的 Excel 文件名
        output_filename = f"consolidated_{task_id}.xlsx"
        # 构造完整的输出文件路径
        output_filepath = os.path.join(output_dir, output_filename)

        # 初始化任务状态信息
        tasks[task_id] = {
            "status": "Queued",  # 状态：已入队
            "progress": 0,  # 进度百分比
            "total_files": len(input_file_paths),  # 总文件数
            "files_processed": 0,  # 已处理文件数
            "result_file": None,  # 结果文件名 (处理完成后设置)
            "error": None,  # 错误信息 (如果发生错误)
        }

        print(f"为 {len(input_file_paths)} 个文件启动后台任务 {task_id}")
        # 创建并启动一个后台线程来执行实际的数据处理
        thread = threading.Thread(
            target=run_processing,  # 线程执行的目标函数
            args=(
                task_id,
                input_file_paths,
                output_filepath,
                task_config,
            ),  # 传递合并后的总配置
        )
        thread.start()

        # 返回任务 ID 给前端，以便后续查询进度
        return jsonify({"task_id": task_id})

    except Exception as e:
        print(f"保存任务 {task_id} 的上传文件时出错: {e}")
        # 保存文件出错时，尝试清理已创建的目录
        try:
            if os.path.exists(upload_dir):
                os.rmdir(
                    upload_dir
                )  # 如果目录非空，rmdir 会失败，更健壮的做法是用 shutil.rmtree
            if os.path.exists(output_dir):
                os.rmdir(output_dir)  # 同上
        except OSError as rm_err:
            print(f"上传文件保存失败后进行清理时出错: {rm_err}")
        return jsonify({"error": "保存上传文件失败"}), 500


# 定义一个回调函数，用于更新任务进度
def update_task_progress(
    task_id, status_msg, progress_pct, files_processed, total_files
):
    """由 processor 模块调用的回调函数，用于更新全局 tasks 字典中的任务状态。"""
    if task_id in tasks:
        tasks[task_id]["status"] = status_msg
        # 确保进度值在 0 到 100 之间
        tasks[task_id]["progress"] = max(0, min(100, int(progress_pct)))
        tasks[task_id]["files_processed"] = files_processed
        tasks[task_id]["total_files"] = total_files  # 顺便更新总文件数，以防万一
        # 在服务器控制台也打印进度信息，方便调试
        print(
            f"任务 {task_id} 进度: {status_msg} - {progress_pct}% ({files_processed}/{total_files} 文件)"
        )
    else:
        # 如果任务 ID 未知 (理论上不应发生)，打印警告
        print(f"警告: 尝试为未知的任务 ID 更新进度: {task_id}")


# 这个函数在后台线程中运行，负责调用核心处理逻辑
def run_processing(task_id, input_files, output_file, config):
    """在后台线程中运行的包装函数，调用 processor 的主处理函数并更新任务状态。"""
    try:
        # 进程开始时更新任务状态
        update_task_progress(task_id, "Processing Started", 0, 0, len(input_files))

        # 定义一个内部回调函数，用于将处理进度转发到全局的 update_task_progress
        def progress_callback(status_msg, progress_pct, files_processed, total_files):
            update_task_progress(
                task_id, status_msg, progress_pct, files_processed, total_files
            )

        # 调用主处理函数
        result = processor.process_files_and_consolidate(
            input_files,
            output_file,
            config["llm_config"],  # 仅传递 LLM 相关配置
            progress_callback,
        )

        # 检查是否使用了阿里云百炼Batch API (返回类型为元组)
        if isinstance(result, tuple) and len(result) == 2:
            batch_id, batch_task_id = result
            print(f"已提交阿里云百炼Batch任务 (ID: {batch_id})，开始轮询状态...")

            # 更新任务状态
            update_task_progress(
                task_id,
                f"Batch Job Submitted: {batch_id}",
                60,
                len(input_files),
                len(input_files),
            )

            # 轮询任务状态
            poll_interval = 15  # 轮询间隔(秒)
            max_poll_attempts = 480  # 最大轮询次数(120分钟)
            for attempt in range(max_poll_attempts):
                try:
                    # 检查用户是否已取消任务
                    if task_id in tasks and tasks[task_id].get("cancelled", False):
                        print(f"任务 {task_id} 被用户取消")
                        update_task_progress(
                            task_id,
                            "Cancelled",
                            100,
                            len(input_files),
                            len(input_files),
                        )
                        return

                    # 等待一段时间后再查询
                    time.sleep(poll_interval)

                    # 查询批处理任务状态
                    status_info = processor.check_bailian_job_status(
                        batch_id, config["llm_config"]
                    )
                    status = status_info.get("status")

                    # 根据状态更新进度
                    progress_message = f"Batch Processing: {status}"
                    progress_pct = 70  # 默认进度

                    if status == "completed":
                        print(f"百炼Batch任务已完成，正在下载结果...")
                        update_task_progress(
                            task_id,
                            "Batch Completed - Downloading Results",
                            75,
                            len(input_files),
                            len(input_files),
                        )
                        break
                    elif status in ["failed", "expired", "cancelled"]:
                        error_msg = f"百炼Batch任务异常终止: {status}"
                        print(error_msg)
                        tasks[task_id]["error"] = error_msg
                        update_task_progress(
                            task_id,
                            f"Error: Batch {status}",
                            100,
                            len(input_files),
                            len(input_files),
                        )
                        return
                    else:
                        # 轮询中，更新状态
                        update_task_progress(
                            task_id,
                            progress_message,
                            progress_pct,
                            len(input_files),
                            len(input_files),
                        )
                        print(f"百炼Batch任务状态: {status}，继续轮询...")

                except Exception as poll_err:
                    print(f"轮询百炼Batch任务状态时出错: {poll_err}")
                    time.sleep(poll_interval)  # 出错后仍继续轮询

            # 轮询结束，检查是否超时
            if attempt >= max_poll_attempts - 1:
                error_msg = "百炼Batch任务处理超时"
                print(error_msg)
                tasks[task_id]["error"] = error_msg
                update_task_progress(
                    task_id,
                    "Error: Batch Timeout",
                    100,
                    len(input_files),
                    len(input_files),
                )
                return

            # 任务成功完成，下载并处理结果
            try:
                update_task_progress(
                    task_id,
                    "Processing Batch Results",
                    80,
                    len(input_files),
                    len(input_files),
                )

                # 读取原始合并数据
                task_dir = os.path.join("uploads", task_id)
                original_df_path = os.path.join(upload_dir, "original_merged.csv")
                original_df = pd.read_csv(original_df_path)

                # 下载并处理百炼批处理结果
                processed_df = processor.download_and_process_bailian_results(
                    batch_id, original_df, task_id, config["llm_config"]
                )

                # 继续处理飞书数据和后处理
                update_task_progress(
                    task_id,
                    "Fetching Feishu Data",
                    85,
                    len(input_files),
                    len(input_files),
                )
                print("从飞书拉取数据...")

                # 从飞书拉取数据
                feishu_df = feishu_utils.fetch_and_prepare_feishu_data(
                    config["feishu_config"]
                )

                # 合并处理后的数据和飞书数据
                if feishu_df is not None and not feishu_df.empty:
                    print(f"已拉取 {len(feishu_df)} 条飞书数据，正在合并...")
                    # 如果两个DataFrame都有，则合并
                    if processed_df is not None and not processed_df.empty:
                        # 合并前先确保列名一致
                        for col in processed_df.columns:
                            if col not in feishu_df.columns:
                                feishu_df[col] = ""
                        for col in feishu_df.columns:
                            if col not in processed_df.columns:
                                processed_df[col] = ""
                        # 合并
                        combined_df = pd.concat(
                            [processed_df, feishu_df], ignore_index=True
                        )
                        print(f"合并后共 {len(combined_df)} 行数据。")
                    else:
                        combined_df = feishu_df
                        print(f"仅使用飞书数据 {len(combined_df)} 行。")
                else:
                    print("未获取到飞书数据或连接失败，仅使用处理后的数据。")
                    combined_df = (
                        processed_df if processed_df is not None else pd.DataFrame()
                    )

                # 应用后处理
                update_task_progress(
                    task_id,
                    "Applying Post-Processing",
                    90,
                    len(input_files),
                    len(input_files),
                )

                # 调用 postprocessor.apply_post_processing
                final_df, update_ids = postprocessor.apply_post_processing(combined_df, config)

                # 创建多Sheet Excel文件
                update_task_progress(
                    task_id,
                    "Creating Multi-Sheet Excel",
                    95,
                    len(input_files),
                    len(input_files),
                )

                # 生成最终Excel文件，传递update_ids参数
                postprocessor.create_multi_sheet_excel(final_df, output_file, config, update_ids=update_ids)

                # 更新任务状态为完成
                result_filename = os.path.basename(output_file)
                tasks[task_id]["result_file"] = result_filename
                update_task_progress(
                    task_id, "Completed", 100, len(input_files), len(input_files)
                )
                print(f"任务 {task_id} 已完成，结果保存为: {result_filename}")

            except Exception as post_process_err:
                print(f"处理百炼Batch结果时发生错误: {post_process_err}")
                import traceback

                traceback.print_exc()
                tasks[task_id][
                    "error"
                ] = f"处理百炼批处理结果时出错: {str(post_process_err)}"
                update_task_progress(
                    task_id,
                    "Error Processing Results",
                    100,
                    len(input_files),
                    len(input_files),
                )
                return

        # 传统同步处理方式 (现在不再使用)
        else:
            # 如果正常处理完成，获取已保存的结果文件路径
            result_filepath = result  # 处理成功时返回输出文件路径

            # 处理完成后，再过滤飞书数据 (因为此时已添加了 local_row_id)
            update_task_progress(
                task_id, "Fetching Feishu Data", 30, len(input_files), len(input_files)
            )
            print("从飞书拉取数据...")
            # 调用 feishu_utils.fetch_and_prepare_feishu_data
            feishu_df = feishu_utils.fetch_and_prepare_feishu_data(
                config["feishu_config"]
            )

            # 加载处理后的数据，用于后续合并
            update_task_progress(
                task_id,
                "Loading Processed Data",
                40,
                len(input_files),
                len(input_files),
            )
            try:
                processed_df = pd.read_excel(result_filepath)
                print(f"已加载处理后的数据 {len(processed_df)} 行。")
            except Exception as e:
                print(f"读取处理后的数据时出错: {e}")
                processed_df = None

            # 合并处理后的数据和飞书数据
            update_task_progress(
                task_id, "Merging Data", 50, len(input_files), len(input_files)
            )

            if feishu_df is not None and not feishu_df.empty:
                print(f"已拉取 {len(feishu_df)} 条飞书数据，正在合并...")
                # 如果两个DataFrame都有，则合并
                if processed_df is not None and not processed_df.empty:
                    # 合并前先确保列名一致
                    for col in processed_df.columns:
                        if col not in feishu_df.columns:
                            feishu_df[col] = ""
                    for col in feishu_df.columns:
                        if col not in processed_df.columns:
                            processed_df[col] = ""

                    combined_df = pd.concat(
                        [processed_df, feishu_df], ignore_index=True
                    )
                    print(f"合并后共 {len(combined_df)} 行数据。")
                else:
                    combined_df = feishu_df
                    print(f"仅使用飞书数据 {len(combined_df)} 行。")
            else:
                print("未获取到飞书数据或连接失败，仅使用处理后的数据。")
                combined_df = (
                    processed_df if processed_df is not None else pd.DataFrame()
                )

            # 应用后处理
            update_task_progress(
                task_id,
                "Applying Post-Processing",
                75,
                len(input_files),
                len(input_files),
            )
            # 调用 postprocessor.apply_post_processing
            final_df = postprocessor.apply_post_processing(combined_df, config)

            # 创建多Sheet Excel文件
            update_task_progress(
                task_id,
                "Creating Multi-Sheet Excel",
                90,
                len(input_files),
                len(input_files),
            )
            # 生成最终Excel文件
            postprocessor.create_multi_sheet_excel(final_df, output_file, config)

            # 更新任务状态为完成
            result_filename = os.path.basename(output_file)
            tasks[task_id]["result_file"] = result_filename
            update_task_progress(
                task_id, "Completed", 100, len(input_files), len(input_files)
            )
            print(f"任务 {task_id} 已完成，结果保存为: {result_filename}")

    except Exception as e:
        # 如果处理过程中发生异常，记录错误并更新任务状态
        error_msg = str(e)
        print(f"处理任务 {task_id} 时发生错误: {error_msg}")
        import traceback

        traceback.print_exc()
        if task_id in tasks:
            tasks[task_id]["error"] = error_msg
            update_task_progress(
                task_id, f"Error: {error_msg}", 100, 0, len(input_files)
            )
        print(f"任务 {task_id} 因错误而终止: {error_msg}")


# 定义进度查询路由
@app.route("/progress/<task_id>")
def progress(task_id):
    """
    查询指定任务ID的处理进度和状态。

    前端通过定期轮询此API获取任务进度，进度信息包括百分比完成度、
    当前处理阶段描述、已处理文件数量等。当任务完成时会返回结果文件名，
    用于前端生成下载链接。如果任务ID不存在则返回404错误。

    Path Params:
        - task_id: 任务唯一标识符UUID

    Returns:
        JSON: 包含任务状态、进度百分比、处理文件信息和结果文件名(如果完成)
    """
    task = tasks.get(task_id)
    if not task:
        # 如果任务 ID 不存在 (可能已完成很久被清理，或从未存在)
        # 返回 404 错误，并提供一个表示任务未知或结束的状态
        return (
            jsonify(
                {
                    "status": "未知或已过期的任务",
                    "progress": 100,  # 让前端停止轮询
                    "error": "任务 ID 未找到。",
                }
            ),
            404,
        )
    # 返回当前任务的状态信息
    return jsonify(task)


# 定义文件下载路由
@app.route("/download/<task_id>/<filename>")
def download_file(task_id, filename):
    """
    下载指定任务的处理结果文件。

    根据任务ID和文件名从输出目录提供文件下载。包含安全检查防止路径遍历攻击，
    确保文件存在且属于指定的任务。各种错误情况会返回相应的HTTP错误码和信息。

    Path Params:
        - task_id: 任务唯一标识符UUID
        - filename: 要下载的文件名，如"final_xxx.xlsx"

    Returns:
        File: 处理结果文件的下载响应
        或 JSON错误信息: 文件不存在/目录不存在/文件名无效
    """
    # 构造该任务的输出目录路径
    directory = os.path.join(app.config["OUTPUT_FOLDER"], task_id)
    print(f"尝试下载: {filename} 从目录 {directory}")

    # 检查输出目录是否存在
    if not os.path.exists(directory):
        print(f"下载失败: 目录未找到 - {directory}")
        return jsonify({"error": "任务输出目录未找到"}), 404

    # 安全性检查: 防止路径遍历攻击 (尽管这里路径是内部构造的，但加上更好)
    if ".." in filename or filename.startswith("/"):
        print(f"下载失败: 文件名无效 - {filename}")
        return jsonify({"error": "文件名无效"}), 400

    # 构造完整的文件路径
    file_path = os.path.join(directory, filename)
    # 检查文件是否存在
    if not os.path.isfile(file_path):
        print(f"下载失败: 文件未找到 - {file_path}")
        return jsonify({"error": "结果文件未找到"}), 404

    try:
        # 使用 Flask 的 send_from_directory 函数安全地发送文件
        # as_attachment=True 会让浏览器弹出下载对话框
        return send_from_directory(directory, filename, as_attachment=True)
    except Exception as e:
        # 处理发送文件过程中的其他潜在错误
        print(f"为任务 {task_id} 发送文件 {filename} 时出错: {e}")
        return jsonify({"error": "服务器发送文件时出错"}), 500


# 新增：保存配置的路由
@app.route("/save_config", methods=["POST"])
def save_config_route():
    """
    保存用户定义的配置为系统默认值。

    接收JSON格式的配置数据，包含LLM、飞书和后处理配置，
    更新内存中的配置变量并写入config.json文件，
    使配置在下次应用启动时自动生效作为默认值。

    Request Body:
        JSON对象，必须包含三个键:
        - llm_config: LLM处理相关配置(API密钥、批量大小等)
        - feishu_config: 飞书API配置(ID、密钥、表IDs等)
        - post_processing_config: 数据后处理配置

    Returns:
        JSON: 表示保存成功或失败的消息
    """
    if not request.is_json:
        return jsonify({"success": False, "error": "Request must be JSON"}), 400

    new_config_data = request.get_json()

    # 基础验证传入的数据结构 (可以根据需要做得更细致)
    if (
        not isinstance(new_config_data, dict)
        or "llm_config" not in new_config_data
        or "feishu_config" not in new_config_data
        or "post_processing_config" not in new_config_data  # 确保新配置也包含后处理部分
    ):
        return (
            jsonify({"success": False, "error": "Invalid config data structure"}),
            400,
        )

    # 更新当前的配置变量 (这样下次渲染页面时就是新的默认值了)
    # 注意：这里直接修改全局变量，简单但不适合复杂应用
    global CURRENT_LLM_CONFIG, CURRENT_FEISHU_CONFIG, CURRENT_POST_PROCESSING_CONFIG
    # 更新LLM和飞书配置
    CURRENT_LLM_CONFIG.update(new_config_data.get("llm_config", {}))
    CURRENT_FEISHU_CONFIG.update(new_config_data.get("feishu_config", {}))
    # 特别注意：保存的是用户界面上当前的后处理设置作为 *新的默认值*
    CURRENT_POST_PROCESSING_CONFIG = new_config_data.get("post_processing_config", {})
    print(
        f"Updated default post-processing config to: {CURRENT_POST_PROCESSING_CONFIG}"
    )  # Debug

    # 将合并后的完整配置写入文件
    full_config_to_save = {
        "llm_config": CURRENT_LLM_CONFIG,
        "feishu_config": CURRENT_FEISHU_CONFIG,
        "post_processing_config": CURRENT_POST_PROCESSING_CONFIG,  # 保存更新后的后处理默认值
    }

    if save_config(full_config_to_save):
        return jsonify({"success": True, "message": "配置已成功保存为默认值！"})
    else:
        return (
            jsonify({"success": False, "error": "保存配置文件时发生服务器错误。"}),
            500,
        )


# 新增：上传用户修改后的文件的路由
@app.route("/upload_edited/<task_id>", methods=["POST"])
def upload_edited_file(task_id):
    """
    上传用户手动编辑后的Excel文件。

    在处理完成后，用户可能会下载结果进行人工修改和校正，
    此API用于接收修改后的文件，保存到任务目录并更新任务状态。
    上传后的文件将用于后续差异比较和飞书同步操作。

    Path Params:
        - task_id: 任务唯一标识符UUID

    Request:
        - edited_file: 用户编辑后的Excel文件

    Returns:
        JSON: 上传成功或失败信息，成功时包含编辑后的文件名
    """
    if task_id not in tasks:
        return jsonify({"success": False, "error": "任务 ID 不存在或已过期。"}), 404

    # 检查是否有文件上传
    if "edited_file" not in request.files:
        return (
            jsonify({"success": False, "error": "请求中缺少 'edited_file' 文件部分。"}),
            400,
        )

    file = request.files["edited_file"]

    if file.filename == "":
        return jsonify({"success": False, "error": "未选择任何文件。"}), 400

    if file:  # 再次确认文件存在
        # 构建保存路径 (保存在该任务的输出目录下)
        output_dir = os.path.join(app.config["OUTPUT_FOLDER"], task_id)
        if not os.path.exists(output_dir):
            # 如果输出目录意外丢失，尝试重新创建？或直接报错
            print(
                f"警告: 任务 {task_id} 的输出目录 {output_dir} 不存在，正在尝试创建。"
            )
            os.makedirs(output_dir, exist_ok=True)

        edited_filename = f"edited_{task_id}.xlsx"
        edited_filepath = os.path.join(output_dir, edited_filename)

        try:
            file.save(edited_filepath)
            print(f"任务 {task_id}: 已保存用户上传的修改后文件到 {edited_filepath}")

            # 更新任务状态，表示已收到修改后的文件
            tasks[task_id]["edited_file_uploaded"] = True
            tasks[task_id]["edited_file_path"] = edited_filepath
            # 可以在这里更新状态消息，提示用户可以进行差异检查
            # update_task_progress(task_id, "阶段4: 已上传调整后文件，可进行差异检查", 100, ...) # 进度可能不需要更新

            return jsonify(
                {
                    "success": True,
                    "message": "调整后的文件上传成功！",
                    "edited_filename": edited_filename,
                }
            )
        except Exception as e:
            print(f"任务 {task_id}: 保存上传的修改后文件时出错: {e}")
            traceback.print_exc()
            return (
                jsonify({"success": False, "error": "保存文件时发生服务器错误。"}),
                500,
            )

    return jsonify({"success": False, "error": "上传失败，未知错误。"}), 500


# 新增：执行差异比较的路由
@app.route("/check_diff/<task_id>", methods=["GET"])
def check_differences(task_id):
    """
    显示多Sheet页Excel文件中的Sheet页内容摘要。

    读取最终处理结果(多Sheet页Excel文件)，提取各Sheet页的摘要统计信息，
    包括每个Sheet页的行数和包含的数据概况。这些信息可以帮助用户了解
    数据处理后的分类结果，以便决定是否需要同步到飞书。

    Path Params:
        - task_id: 任务唯一标识符UUID

    Returns:
        JSON: Excel内容摘要对象，包含:
            - sheet_info: 各Sheet页的统计信息
            - total_rows: 所有Sheet页的总行数
            - columns: Excel文件包含的列名列表
    """
    if task_id not in tasks:
        return jsonify({"success": False, "error": "任务 ID 不存在或已过期。"}), 404

    task_info = tasks[task_id]

    # 检查必要的文件是否存在
    final_filename = task_info.get("result_file")  # 最终文件名

    if not final_filename:
        return jsonify({"success": False, "error": "找不到处理结果文件信息。"}), 404

    output_dir = os.path.join(app.config["OUTPUT_FOLDER"], task_id)
    final_filepath = os.path.join(output_dir, final_filename)

    if not os.path.exists(final_filepath):
        print(f"错误: 找不到任务 {task_id} 的结果文件: {final_filepath}")
        return jsonify({"success": False, "error": "找不到多Sheet页Excel文件。"}), 404

    print(f"任务 {task_id}: 开始读取多Sheet页Excel文件: '{final_filepath}'")

    try:
        # 读取Excel文件的各个Sheet
        sheet_info = {}
        total_rows = 0
        all_columns = set()
        record_id_col = "record_id"

        with pd.ExcelFile(final_filepath) as xls:
            sheets = xls.sheet_names
            print(f"  文件包含以下Sheet页: {sheets}")

            for sheet_name in sheets:
                # 读取每个Sheet
                phone_col = CURRENT_FEISHU_CONFIG.get("PHONE_NUMBER_COLUMN", "电话")
                converters = {phone_col: str}
                df = pd.read_excel(xls, sheet_name=sheet_name, converters=converters)
                row_count = len(df)
                total_rows += row_count

                # 收集列名
                for col in df.columns:
                    all_columns.add(col)

                # 统计具体信息
                sheet_stats = {
                    "row_count": row_count,
                    "columns": list(df.columns),
                }

                # 对特殊Sheet页做额外统计
                if sheet_name in ["新增", "更新"]:
                    # 统计record_id情况
                    if record_id_col in df.columns:
                        empty_record_ids = df[
                            df[record_id_col].fillna("").astype(str).str.strip() == ""
                        ].shape[0]
                        valid_record_ids = row_count - empty_record_ids
                        sheet_stats["empty_record_ids"] = empty_record_ids
                        sheet_stats["valid_record_ids"] = valid_record_ids

                sheet_info[sheet_name] = sheet_stats

        # 构建响应
        result = {
            "success": True,
            "sheet_info": sheet_info,
            "total_rows": total_rows,
            "columns": list(all_columns),
        }

        print(
            f"任务 {task_id}: 多Sheet页Excel文件读取完成，共 {total_rows} 行数据，分布在 {len(sheets)} 个Sheet页"
        )
        return jsonify(result)

    except FileNotFoundError as fnf_err:
        print(f"任务 {task_id}: 读取Excel文件时出错: {fnf_err}")
        return (
            jsonify({"success": False, "error": f"读取文件失败: {fnf_err.filename}"}),
            500,
        )
    except Exception as e:
        print(f"任务 {task_id}: 读取Excel文件时发生错误: {e}")
        traceback.print_exc()
        return (
            jsonify({"success": False, "error": "读取Excel文件时发生服务器错误。"}),
            500,
        )


# 新增：将差异同步回飞书的路由
@app.route("/sync_to_feishu/<task_id>", methods=["POST"])
def sync_to_feishu(task_id):
    """
    将数据同步到飞书多维表格。

    基于多Sheet页Excel文件执行两种操作:
    1. 添加新记录 - 从"新增"Sheet页读取数据
    2. 更新记录 - 将"更新"Sheet页的数据与原始数据Sheet比较，只更新有差异的字段

    不再执行删除操作。

    执行前会验证配置、检查飞书表空间容量并格式化数据，
    同步结果会详细记录成功和失败的操作数量。

    Path Params:
        - task_id: 任务唯一标识符UUID

    Requires:
        - 已上传的编辑后文件(包含多Sheet页)
        - 有效的飞书API配置

    Returns:
        JSON: 同步操作结果，包含添加/更新/删除的记录数和错误信息
    """
    # --- 常量定义 ---
    FEISHU_ROW_LIMIT = 50000

    if task_id not in tasks:
        return jsonify({"success": False, "error": "任务 ID 不存在或已过期。"}), 404

    task_info = tasks[task_id]
    print(f"任务 {task_id}: 开始执行同步到飞书操作 (使用多Sheet页Excel)...")

    # --- 检查必需的信息和配置 --- #
    edited_filepath = task_info.get("edited_file_path")

    if not edited_filepath or not os.path.exists(edited_filepath):
        # Edited 文件必须存在
        output_dir = os.path.join(app.config["OUTPUT_FOLDER"], task_id)
        edited_filename_fallback = f"edited_{task_id}.xlsx"
        edited_filepath_fallback = os.path.join(output_dir, edited_filename_fallback)
        if os.path.exists(edited_filepath_fallback):
            edited_filepath = edited_filepath_fallback
            task_info["edited_file_path"] = edited_filepath  # Update cache
        else:
            print(
                f"错误: 找不到任务 {task_id} 的多Sheet页文件: {edited_filepath_fallback}"
            )
            return (
                jsonify(
                    {
                        "success": False,
                        "error": "找不到多Sheet页文件，无法执行同步操作。",
                    }
                ),
                404,
            )

    if (
        "config" not in task_info
        or "feishu_config" not in task_info["config"]
        or "llm_config" not in task_info["config"]
    ):
        return (
            jsonify(
                {"success": False, "error": "任务配置信息缺失，无法获取飞书或LLM配置。"}
            ),
            500,
        )

    # --- 获取配置 --- #
    feishu_config = task_info["config"]["feishu_config"]
    llm_config = task_info["config"]["llm_config"]
    app_id = feishu_config.get("APP_ID")
    app_secret = feishu_config.get("APP_SECRET")
    app_token = feishu_config.get("APP_TOKEN")
    primary_table_ids = feishu_config.get("TABLE_IDS")
    add_target_table_ids = feishu_config.get("ADD_TARGET_TABLE_IDS", [])
    target_columns = llm_config.get("TARGET_COLUMNS", [])

    # --- 配置验证 --- #
    if not app_id or not app_secret:
        msg = "飞书 App ID 或 App Secret 未配置"
    elif not app_token or not primary_table_ids:
        msg = "飞书 Base App Token 或主 Table IDs 配置缺失"
    else:
        msg = None
    if msg:
        print(f"❌ 配置错误: {msg}")
        flash(msg, "error")
        return jsonify({"success": False, "message": msg, "summary": {"errors": [msg]}})

    # --- 准备操作列表和结果字典 --- #
    feishu_id_col = "record_id"
    local_id_col = "local_row_id"
    records_to_update = []
    records_to_add = []
    results = {
        "updated": 0,
        "added": 0,
        "update_errors": 0,
        "add_errors": 0,
        "errors": [],
        "diff_details": [],  # 新增：记录差异详情
    }

    try:
        # --- 1. 从多Sheet页Excel文件读取各Sheet页数据 --- #
        print(f"  步骤 1: 从 '{edited_filepath}' 读取各Sheet页数据...")
        try:
            # 读取Excel文件的Sheet页
            with pd.ExcelFile(edited_filepath) as xls:
                sheets = xls.sheet_names
                print(f"    -> 文件包含以下Sheet页: {sheets}")

                # 确保所需Sheet都存在
                if "原始数据" not in sheets:
                    print(f"    -> 警告: 未找到'原始数据'Sheet页，无法进行差异比较")

                # 读取原始数据Sheet页
                df_original = pd.DataFrame()
                if "原始数据" in sheets:
                    # 添加converters参数确保电话列为字符串
                    phone_col = feishu_config.get("PHONE_NUMBER_COLUMN", "电话")
                    converters = {phone_col: str}
                    df_original = pd.read_excel(
                        xls, sheet_name="原始数据", converters=converters
                    )
                    print(f"    -> 从'原始数据'Sheet读取了 {len(df_original)} 行数据")
                    # 确保record_id列存在并清理格式
                    if feishu_id_col in df_original.columns:
                        df_original[feishu_id_col] = (
                            df_original[feishu_id_col]
                            .fillna("")
                            .astype(str)
                            .str.strip()
                        )
                        # 处理"none"值
                        df_original.loc[
                            df_original[feishu_id_col].str.lower() == "none",
                            feishu_id_col,
                        ] = ""
                    else:
                        print(
                            f"    -> 警告: '原始数据'Sheet中缺少'{feishu_id_col}'列，可能影响差异比较"
                        )

                # 读取"新增"Sheet页
                df_add = pd.DataFrame()
                if "新增" in sheets:
                    # 添加converters参数确保电话列为字符串
                    phone_col = feishu_config.get("PHONE_NUMBER_COLUMN", "电话")
                    converters = {phone_col: str}
                    df_add = pd.read_excel(
                        xls, sheet_name="新增", converters=converters
                    )
                    print(f"    -> 从'新增'Sheet读取了 {len(df_add)} 行数据")
                else:
                    print(f"    -> 未找到'新增'Sheet，跳过新增操作")

                # 读取"更新"Sheet页
                df_update = pd.DataFrame()
                if "更新" in sheets:
                    # 添加converters参数确保电话列为字符串
                    phone_col = feishu_config.get("PHONE_NUMBER_COLUMN", "电话")
                    converters = {phone_col: str}
                    df_update = pd.read_excel(
                        xls, sheet_name="更新", converters=converters
                    )
                    print(f"    -> 从'更新'Sheet读取了 {len(df_update)} 行数据")
                else:
                    print(f"    -> 未找到'更新'Sheet，跳过更新操作")

            # 准备新增记录
            if not df_add.empty:
                # 确保record_id列存在 (在新增数据中应该是空的)
                if feishu_id_col not in df_add.columns:
                    df_add[feishu_id_col] = ""
                else:
                    df_add[feishu_id_col] = (
                        df_add[feishu_id_col].fillna("").astype(str).str.strip()
                    )
                    # 处理"none"值
                    df_add.loc[
                        df_add[feishu_id_col].str.lower() == "none", feishu_id_col
                    ] = ""

                # 筛选确保只处理record_id为空的行
                df_add = df_add[df_add[feishu_id_col] == ""]
                print(f"    -> '新增'Sheet中有 {len(df_add)} 行有效数据(record_id为空)")

                # 准备新增数据 (与原代码相同)
                for _, row in df_add.iterrows():
                    add_payload = {"fields": {}}
                    for col in target_columns:
                        if col in row.index:
                            value = row[col]
                            # 忽略 None 值和空字符串
                            if (
                                pd.isna(value)
                                or value is None
                                or (isinstance(value, str) and not value.strip())
                            ):
                                continue

                            # 处理不同类型的值
                            if isinstance(value, (list, dict)):
                                value_str = json.dumps(value, ensure_ascii=False)
                            else:
                                value_str = str(value)

                            add_payload["fields"][col] = value_str

                    # 只有当fields非空时才添加到列表
                    if add_payload["fields"]:
                        records_to_add.append(add_payload)

                print(f"    -> 准备了 {len(records_to_add)} 条记录待新增。")

            # 准备更新记录 - 改为与原始数据对比
            if not df_update.empty:
                # 确保record_id列和table_id列存在 (在更新数据中不应该为空)
                if feishu_id_col not in df_update.columns:
                    print(
                        f"    -> 警告: '更新'Sheet中缺少 '{feishu_id_col}' 列，无法执行更新操作"
                    )
                elif "table_id" not in df_update.columns:
                    print(
                        f"    -> 警告: '更新'Sheet中缺少 'table_id' 列，无法执行更新操作"
                    )
                else:
                    # 清理record_id和table_id
                    df_update[feishu_id_col] = (
                        df_update[feishu_id_col].fillna("").astype(str).str.strip()
                    )
                    df_update["table_id"] = (
                        df_update["table_id"].fillna("").astype(str).str.strip()
                    )

                    # 处理"none"值
                    df_update.loc[
                        df_update[feishu_id_col].str.lower() == "none", feishu_id_col
                    ] = ""
                    df_update.loc[
                        df_update["table_id"].str.lower() == "none", "table_id"
                    ] = ""

                    # 筛选有效记录 - 同时需要有record_id和table_id
                    df_update = df_update[
                        (df_update[feishu_id_col] != "") & (df_update["table_id"] != "")
                    ]
                    print(
                        f"    -> '更新'Sheet中有 {len(df_update)} 行同时包含有效record_id和table_id"
                    )

                    # 直接从更新sheet构建更新记录
                    for _, update_row in df_update.iterrows():
                        record_id = update_row[feishu_id_col]
                        table_id = update_row["table_id"]

                        # 创建更新记录
                        update_payload = {
                            "record_id": record_id,
                            "table_id": table_id,
                            "fields": {},
                        }

                        # 添加所有需要更新的字段
                        for col in target_columns:
                            if (
                                col in update_row.index
                                and col != feishu_id_col
                                and col != "table_id"
                            ):
                                value = update_row[col]
                                # 忽略None值和空字符串
                                if pd.isna(value) or value is None:
                                    continue

                                # 处理不同类型的值
                                if isinstance(value, (list, dict)):
                                    processed_value = json.dumps(
                                        value, ensure_ascii=False
                                    )
                                else:
                                    processed_value = str(value)

                                update_payload["fields"][col] = processed_value

                        # 只有当fields非空时才添加到更新列表
                        if update_payload["fields"]:
                            records_to_update.append(update_payload)

                    print(
                        f"    -> 直接从'更新'Sheet提取了 {len(records_to_update)} 条记录准备更新。"
                    )

            print(
                f"    -> 最终准备: {len(records_to_add)} 条新增记录和 {len(records_to_update)} 条更新记录。"
            )

        except FileNotFoundError:
            raise FileNotFoundError(f"找不到调整后的文件 '{edited_filepath}'。")
        except Exception as read_err:
            raise Exception(f"读取调整后文件 '{edited_filepath}' 时出错: {read_err}")

        # --- 2. 执行飞书API操作 --- #
        print(f"\n  步骤 2: 开始执行飞书 API 操作...")

        # 获取access_token
        try:
            access_token = feishu_utils.get_access_token(app_id, app_secret)
            if not access_token:
                raise ValueError("无法获取飞书访问令牌，无法继续同步。")
        except Exception as token_err:
            print(f"     ❌ 获取飞书访问令牌时发生错误: {token_err}")
            results["errors"].append(f"获取飞书令牌失败: {token_err}")
            flash(f"同步失败：无法获取飞书访问令牌。错误: {token_err}", "error")
            return (
                jsonify(
                    {
                        "success": False,
                        "message": f"获取飞书访问令牌失败: {token_err}",
                        "summary": results,
                    }
                ),
                500,
            )

        final_add_target_table_id = None
        records_to_add_validated = records_to_add.copy()

        # --- 批量更新 ---
        if records_to_update:
            print(f"      准备更新 {len(records_to_update)} 条记录...")

            # 按table_id分组
            update_records_by_table = {}
            for record in records_to_update:
                table_id = record.get("table_id", "")
                record_id = record.get("record_id", "")

                if not table_id or not record_id:
                    print(f"      ⚠️ 跳过缺少table_id或record_id的记录: {record}")
                    continue

                if table_id not in update_records_by_table:
                    update_records_by_table[table_id] = []

                # 创建更新记录的副本，移除table_id字段(飞书API不需要)
                update_record = record.copy()
                if "table_id" in update_record:
                    del update_record["table_id"]

                update_records_by_table[table_id].append(update_record)

            # 对每个表格分别执行更新
            total_success = 0
            total_error = 0
            all_errors = []

            for table_id, table_records in update_records_by_table.items():
                print(
                    f"      正在更新表格 {table_id} 中的 {len(table_records)} 条记录..."
                )
                try:
                    update_result = feishu_utils.batch_update_records(
                        app_token,
                        table_id,
                        table_records,
                        app_id,
                        app_secret,
                    )
                    total_success += update_result.get("success_count", 0)
                    total_error += update_result.get("error_count", 0)
                    all_errors.extend(update_result.get("errors", []))

                    print(
                        f"      表格 {table_id} 更新结果: 成功={update_result.get('success_count', 0)}, 失败={update_result.get('error_count', 0)}"
                    )
                except Exception as e:
                    print(f"      ❌ 更新表格 {table_id} 时出错: {e}")
                    total_error += len(table_records)
                    all_errors.append(f"更新表格 {table_id} 失败: {e}")

            # 汇总更新结果
            update_results = {
                "success_count": total_success,
                "error_count": total_error,
                "errors": all_errors,
            }

            # 更新结果统计
            results["updated"] = update_results.get("success_count", 0)
            results["update_errors"] = update_results.get("error_count", 0)
            if update_results.get("errors"):
                results["errors"].extend(update_results["errors"])

        # --- 批量新增 (行数检查逻辑) ---
        if records_to_add_validated:
            print(
                f"      开始确定新增目标表格 (共 {len(records_to_add_validated)} 条待新增)..."
            )
            num_to_add = len(records_to_add_validated)
            target_found = False
            if add_target_table_ids:
                print(
                    f"        检测到 {len(add_target_table_ids)} 个指定的新增目标表格，将按顺序检查空间..."
                )
                for target_id in add_target_table_ids:
                    print(f"          > 检查表格 {target_id} 的行数限制...")
                    try:
                        current_count = feishu_utils.get_table_record_count(
                            access_token, app_token, target_id
                        )
                        if current_count is None:
                            print(
                                f"          ⚠️ 无法获取表格 {target_id} 记录数，跳过。"
                            )
                            continue
                        estimated_new_count = current_count + num_to_add
                        print(
                            f"          > 表格 {target_id} 当前: {current_count}, 新增后预估: {estimated_new_count} (上限: {FEISHU_ROW_LIMIT})"
                        )
                        if estimated_new_count <= FEISHU_ROW_LIMIT:
                            print(
                                f"          ✅ 表格 {target_id} 空间足够，选定为新增目标。"
                            )
                            final_add_target_table_id = target_id
                            target_found = True
                            break
                        else:
                            print(f"          ❌ 表格 {target_id} 空间不足。")
                    except Exception as count_err:
                        print(
                            f"          ⚠️ 获取表格 {target_id} 记录数时出错: {count_err}，跳过。"
                        )
                        continue
                if not target_found:
                    error_msg = f"所有指定的新增目标表格 ({', '.join(add_target_table_ids)}) 均已满或无法检查，无法新增 {num_to_add} 条记录。"
                    print(f"     ❌ {error_msg}")
                    results["add_errors"] += num_to_add
                    results["errors"].append(error_msg)
                    records_to_add_validated = []
            else:
                target_id = primary_table_ids[0]
                print(f"        未指定新增目标表格，将尝试写入主表格: {target_id}")
                print(f"          > 检查表格 {target_id} 的行数限制...")
                try:
                    current_count = feishu_utils.get_table_record_count(
                        access_token, app_token, target_id
                    )
                    if current_count is None:
                        print(
                            f"          ⚠️ 无法获取表格 {target_id} 记录数，将尝试写入，但可能失败。"
                        )
                        final_add_target_table_id = target_id
                        target_found = True
                    else:
                        estimated_new_count = current_count + num_to_add
                        print(
                            f"          > 表格 {target_id} 当前: {current_count}, 新增后预估: {estimated_new_count} (上限: {FEISHU_ROW_LIMIT})"
                        )
                        if estimated_new_count <= FEISHU_ROW_LIMIT:
                            print(
                                f"          ✅ 主表格 {target_id} 空间足够，选定为新增目标。"
                            )
                            final_add_target_table_id = target_id
                            target_found = True
                        else:
                            error_msg = f"主表格 {target_id} 空间不足，无法新增 {num_to_add} 条记录 (当前 {current_count} 行，上限 {FEISHU_ROW_LIMIT})。"
                            print(f"     ❌ {error_msg}")
                            results["add_errors"] += num_to_add
                            results["errors"].append(error_msg)
                            records_to_add_validated = []
                except Exception as count_err:
                    print(
                        f"          ⚠️ 获取主表格 {target_id} 记录数时出错: {count_err}，将尝试写入，但可能失败。"
                    )
                    final_add_target_table_id = target_id
                    target_found = True

            # --- 执行新增 (如果仍有数据且找到目标) ---
            if records_to_add_validated and target_found and final_add_target_table_id:
                print(
                    f"      尝试向表格 {final_add_target_table_id} 新增 {len(records_to_add_validated)} 条记录..."
                )
                try:
                    add_result = feishu_utils.batch_add_records(
                        app_token,
                        final_add_target_table_id,
                        records_to_add_validated,
                        app_id,
                        app_secret,
                    )
                    results["added"] = add_result.get("success_count", 0)
                    results["add_errors"] = add_result.get(
                        "error_count", len(records_to_add_validated) - results["added"]
                    )
                    if add_result.get("errors"):
                        results["errors"].extend(
                            [
                                f"新增失败 (表 {final_add_target_table_id}): {e}"
                                for e in add_result["errors"]
                            ]
                        )
                    print(
                        f"        新增结果: 成功 {results['added']}, 失败 {results['add_errors']}"
                    )
                except Exception as add_err:
                    print(f"     ❌ 调用批量新增时发生错误: {add_err}")
                    results["add_errors"] += len(records_to_add_validated)
                    results["errors"].append(
                        f"批量新增API调用失败 (表 {final_add_target_table_id}): {add_err}"
                    )
            elif records_to_add_validated and not target_found:
                print(f"     ⚠️ 跳过新增：未找到合适的目标表格。")
            elif not records_to_add_validated:
                print(
                    f"     ℹ️ 跳过新增：没有待新增的有效记录（可能因行数限制被阻止）。"
                )

    # --- 统一错误处理 --- #
    except FileNotFoundError as fnf_err:
        print(f"任务 {task_id}: 读取编辑文件时出错: {fnf_err}")
        flash(f"读取文件失败: {fnf_err.filename}", "error")
        return (
            jsonify(
                {
                    "success": False,
                    "error": f"读取编辑文件失败: {fnf_err.filename}",
                    "summary": results,
                }
            ),
            500,
        )
    except ValueError as val_err:
        print(f"任务 {task_id}: 数据准备或验证失败: {val_err}")
        flash(f"数据错误: {val_err}", "error")
        return (
            jsonify(
                {
                    "success": False,
                    "error": f"数据准备或验证失败: {val_err}",
                    "summary": results,
                }
            ),
            400,
        )
    except Exception as outer_sync_err:
        print(f"任务 {task_id}: 同步过程中发生意外错误: {outer_sync_err}")
        traceback.print_exc()
        results["errors"].append(f"同步过程中发生服务器错误: {outer_sync_err}")
        flash("同步过程中发生内部服务器错误。", "error")
        return (
            jsonify(
                {
                    "success": False,
                    "message": "同步过程中发生内部服务器错误。",
                    "summary": results,
                }
            ),
            500,
        )

    # --- 组装最终响应 --- #
    total_success = results["updated"] + results["added"]
    total_errors = results["update_errors"] + results["add_errors"]
    total_errors += len(
        [
            e
            for e in results["errors"]
            if "调用失败" in e or "表格" in e or "配置" in e or "数据错误" in e
        ]
    )

    success_msg = (
        f"同步完成: 新增 {results['added']} 条, 更新 {results['updated']} 条。"
    )

    # 如果有差异详情，添加到响应中
    if results["diff_details"]:
        results["diff_count"] = len(results["diff_details"])

    if total_errors > 0:
        error_details = []
        if results["update_errors"] > 0:
            error_details.append(f"更新失败 {results['update_errors']} 条")
        if results["add_errors"] > 0:
            error_details.append(f"新增失败 {results['add_errors']} 条")
        other_errors = [e for e in results["errors"] if "调用失败" not in e]
        if other_errors:
            error_details.append(
                f"其他错误 {len(other_errors)} 条: {'; '.join(other_errors[:2])}"
                + ("..." if len(other_errors) > 2 else "")
            )
        error_summary = "部分操作失败: " + "，".join(error_details) + "."
        final_message = success_msg + f" {error_summary}"
        print(f"任务 {task_id}: 同步部分失败。Summary: {results}")
        flash(final_message, "warning")
        return jsonify({"success": True, "message": final_message, "summary": results})
    else:
        print(f"任务 {task_id}: 同步成功。Summary: {results}")
        flash(success_msg, "success")
        return jsonify({"success": True, "message": success_msg, "summary": results})


# Python 标准入口点
if __name__ == "__main__":
    print("启动 Flask 应用...")

    # 加载配置以确保应用启动时变量是最新的
    CURRENT_LLM_CONFIG, CURRENT_FEISHU_CONFIG, CURRENT_POST_PROCESSING_CONFIG = (
        load_config()
    )

    # 再次检查 API Key 并显示警告 (如果需要)
    if not CURRENT_LLM_CONFIG.get("DEEPSEEK_API_KEY") or not CURRENT_LLM_CONFIG.get(
        "DEEPSEEK_API_KEY", ""
    ).startswith("sk-"):
        print("\n********************************************************************")
        print("*** 警告: DEEPSEEK_API_KEY 未设置或格式无效! ***")
        print("***          LLM 处理很可能会失败。             ***")
        print("***          请设置 DEEPSEEK_API_KEY 环境变量   ***")
        print("***          或直接在 app.py 中更新。           ***")
        print("********************************************************************\n")
    # 运行 Flask 开发服务器
    # host='0.0.0.0' 使其在局域网内可访问 (而不仅仅是本机 127.0.0.1)
    # port=5000 指定端口号
    # debug=True 启用调试模式 (代码更改后自动重载，显示详细错误页面)
    # threaded=True 允许多线程处理请求 (对于后台任务是必需的)
    app.run(host="0.0.0.0", port=5100, debug=True, threaded=True)
