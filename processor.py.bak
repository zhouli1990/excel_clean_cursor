# -*- coding: utf-8 -*-
import requests
import pandas as pd
import time
import numpy as np
import os
import json
import traceback  # 用于打印详细的错误堆栈信息
import math  # 用于计算批处理数量
import uuid  # Added import
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, Dict, List, Optional, Tuple, Union
from pathlib import Path
import openai  # 新增: 阿里云百炼 OpenAI 兼容接口

# === Logger Setup ===
# Create a logger instance
logger = logging.getLogger(__name__)
# Set the logging level (e.g., INFO, DEBUG, WARNING)
logger.setLevel(logging.INFO)
# Create a handler (e.g., StreamHandler to output to console)
if not logger.handlers:  # Avoid adding multiple handlers if reloaded
    handler = logging.StreamHandler()
    # Create a formatter and set it for the handler
    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    handler.setFormatter(formatter)
    # Add the handler to the logger
    logger.addHandler(handler)

# === 配置项 (这些值现在由 app.py 传入) ===
# DEEPSEEK_API_KEY = "YOUR_API_KEY"       # 从 app.py 获取
# TARGET_COLUMNS = [...]                  # 从 app.py 获取
# BATCH_SIZE = 160                        # 从 app.py 获取
# MAX_COMPLETION_TOKENS = 8192            # 从 app.py 获取
# DEEPSEEK_API_ENDPOINT = "https://api.deepseek.com/chat/completions" # 从 app.py 获取
# DASHSCOPE_API_KEY = ""                  # 从 app.py 获取
# BAILIAN_BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1" # 从 app.py 获取
# BAILIAN_BATCH_ENDPOINT = "/v1/chat/completions" # 从 app.py 获取
# BAILIAN_MODEL_NAME = "qwen-plus"        # 从 app.py 获取
# BAILIAN_COMPLETION_WINDOW = "24h"       # 从 app.py 获取


def read_input_file(
    file_path: str,  # required_columns: Optional[List[str]] = None # Removed this parameter
) -> pd.DataFrame:
    """
    Reads an Excel or CSV file into a Pandas DataFrame.
    # Removed validation of required columns.
    Adds a unique local_row_id to each row.
    *** Removed adding '来源' column here. ***
    """
    logger.info(f"Reading file: {file_path}")
    try:
        if file_path.endswith((".xlsx", ".xls")):
            # 添加converters参数确保电话列为字符串
            # 注意：由于这里无法访问config，使用所有可能的电话列名作为字符串转换
            possible_phone_cols = [
                "电话",
                "手机",
                "电话号码",
                "手机号码",
                "联系方式",
                "电话/手机",
            ]
            converters = {col: str for col in possible_phone_cols}
            df = pd.read_excel(file_path, engine="openpyxl", converters=converters)
        elif file_path.endswith(".csv"):
            try:
                df = pd.read_csv(file_path, encoding="utf-8")
            except UnicodeDecodeError:
                logger.warning(f"UTF-8 decoding failed for {file_path}, trying gbk.")
                df = pd.read_csv(file_path, encoding="gbk")
        else:
            raise ValueError(
                "Unsupported file format. Please use Excel (.xlsx, .xls) or CSV (.csv)."
            )

        logger.info(f"Successfully read {len(df)} rows from {file_path}.")

        # Clean column names (strip whitespace)
        df.columns = df.columns.str.strip()
        logger.debug(f"Original columns: {list(df.columns)}")

        # Validate required columns - REMOVED
        # if required_columns:
        #     missing_cols = [col for col in required_columns if col not in df.columns]
        #     if missing_cols:
        #         raise ValueError(f"Missing required columns in {file_path}: {', '.join(missing_cols)}")
        #     logger.info(f"All required columns {required_columns} found in {file_path}.")

        # Add local_row_id
        if "local_row_id" not in df.columns:
            df["local_row_id"] = [str(uuid.uuid4()) for _ in range(len(df))]
            logger.info(f"Added 'local_row_id' column to DataFrame from {file_path}.")
        else:
            # Handle case where column might exist but contain NaNs or duplicates
            existing_ids = df["local_row_id"].dropna().astype(str)
            if len(existing_ids) != len(df) or existing_ids.duplicated().any():
                logger.warning(
                    f"'local_row_id' column exists in {file_path} but contains nulls or duplicates. Regenerating IDs."
                )
                df["local_row_id"] = [str(uuid.uuid4()) for _ in range(len(df))]

        # *** REMOVED adding '来源' column here ***
        # df["来源"] = os.path.basename(file_path)

        # Fill NaN values with empty strings BEFORE processing
        df = df.fillna("")  # Important: Prevents issues with NaN in string operations

        return df

    except FileNotFoundError:
        # 文件路径不存在
        print(f"   ❌ 文件未找到: {file_path}")
        return None
    except ImportError as e:
        # 缺少核心库 (通常是 pandas)
        print(f"   ❌ 缺少必要的库: {e}。请确保已安装 pandas, openpyxl, xlrd。")
        return None
    except Exception as e:
        # 捕获所有其他未预料到的异常
        print(f"   ❌ 读取文件 '{file_path}' 时发生未知错误:")
        # 打印详细的错误堆栈信息
        print(traceback.format_exc())
        return None


def create_bailian_batch_input(df: pd.DataFrame, task_id: str, config: dict) -> str:
    """
    为阿里云百炼Batch API创建输入文件。
    将DataFrame中的数据按BATCH_SIZE分批处理，每批创建一个请求，并写入JSONL文件。

    Args:
        df: 包含需要处理的数据的DataFrame。
        task_id: 任务ID，用于创建输出目录。
        config: 包含配置信息的字典。

    Returns:
        str: 创建的JSONL文件路径。
    """
    print(f"   >> 开始准备百炼Batch API输入文件...")

    # 获取目标列和模型名称配置
    target_columns_config = config.get(
        "TARGET_COLUMNS", ["公司名称", "联系人", "职位", "电话", "来源"]
    )
    source_column_name = "来源"
    model_name = config.get("BAILIAN_MODEL_NAME", "qwen-plus")
    batch_endpoint = config.get("BAILIAN_BATCH_ENDPOINT", "/v1/chat/completions")
    max_tokens = config.get("MAX_COMPLETION_TOKENS", 8192)
    # 获取批处理大小
    batch_size = config.get("BATCH_SIZE", 100)

    # 准备给LLM的目标列(不含来源)
    llm_target_columns = [
        col for col in target_columns_config if col != source_column_name
    ]

    # 确保任务目录存在
    task_dir = os.path.join("uploads", task_id)
    os.makedirs(task_dir, exist_ok=True)

    # 创建JSONL文件路径
    jsonl_path = os.path.join(task_dir, "batch_input.jsonl")

    # 获取源列标题
    source_headers = df.columns.astype(str).tolist()
    source_headers_json = json.dumps(source_headers, ensure_ascii=False)

    # 构建系统提示词(system prompt)
    system_content = f"""
你是一个专业的数据处理引擎。你的核心任务是从用户提供的源数据中，提取信息，并严格按照指定的目标模式 (Target Schema) 进行标准化。
**重要：你的输出必须是一个有效的 JSON 数组，每个数组元素是一个对象，包含以下键值对：**
{llm_target_columns}
**当源数据中电话/手机号列包含多个号码时（可能用分号、逗号、空格等分隔），你必须将该行拆分为多行，每行对应一个手机号，其他信息保持一致。**
**手机号分隔符识别规则：**
- 支持的分隔符包括：中英文分号（;；）、中英文逗号（,，）、空格、斜杠（/）
- 一行中可能包含多个不同类型的分隔符

**处理规则:**
1.提取与映射: 结合源数据标题和内容，将信息映射到 Target Schema 字段。找不到信息则对应值设为 ""。所有值必须为字符串。
2.格式整理: 去除多余空格和不必要的标点符号。
3.规范化: 电话号码应去除非数字字符，保留纯数字形式。
4.批量处理: 你将收到多行数据，需要处理每一行并返回JSON数组，包含所有处理结果。
5.最终输出格式: 只返回JSON数组，不要有任何额外的解释。
"""

    # 按照batch_size拆分数据
    total_rows = len(df)
    batch_count = (total_rows + batch_size - 1) // batch_size  # 向上取整
    print(
        f"   >> 将 {total_rows} 行数据分为 {batch_count} 个批次，每批最多 {batch_size} 行"
    )

    # 将DataFrame的批次转换为请求，写入JSONL文件
    with open(jsonl_path, "w", encoding="utf-8") as f:
        for batch_idx in range(batch_count):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, total_rows)
            batch_df = df.iloc[start_idx:end_idx]

            # 获取批次中所有行的local_row_id，用作custom_id
            batch_row_ids = batch_df["local_row_id"].tolist()
            batch_custom_id = f"batch_{batch_idx}_{len(batch_row_ids)}"

            # 将批次数据转换为用户可理解的文本格式
            batch_content = []
            for _, row in batch_df.iterrows():
                row_str = "\n".join(
                    [f"{k}: {v}" for k, v in row.items() if k != "local_row_id"]
                )
                # 添加local_row_id作为行标识
                row_str += f"\n行ID: {row['local_row_id']}"
                batch_content.append(row_str)

            # 合并所有行内容
            all_rows_content = "\n\n--- 行分隔符 ---\n\n".join(batch_content)

            # 用户提示词(user prompt)
            user_content = f"""
这是本次需要处理的源数据表格的列标题:
{source_headers_json}

这是需要处理的 {len(batch_df)} 行源数据:
{all_rows_content}

请根据你在System指令中被赋予的角色和规则处理这些数据，并返回标准化的JSON数组。
每个处理结果必须包含原始行的"行ID"值，以便我们能够追踪结果与原始数据的对应关系。
"""

            # 构建消息数组
            messages = [
                {"role": "system", "content": system_content},
                {"role": "user", "content": user_content},
            ]

            # 构建API请求体
            body = {
                "model": model_name,
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": 1.0,  # 可配置
            }

            # 构建JSONL记录
            record = {
                "custom_id": batch_custom_id,  # 使用批次ID作为custom_id
                "method": "POST",
                "url": batch_endpoint,
                "body": body,
                "row_ids": batch_row_ids,  # 保存批次中所有行的row_id
            }

            # 写入JSONL文件
            f.write(json.dumps(record, ensure_ascii=False) + "\n")

    print(
        f"   ✅ 已成功创建百炼Batch API输入文件: {jsonl_path} (包含 {batch_count} 个批次请求)"
    )
    return jsonl_path


def submit_bailian_batch_job(jsonl_path: str, task_id: str, config: dict) -> str:
    """
    提交阿里云百炼Batch任务。

    Args:
        jsonl_path: JSONL输入文件路径。
        task_id: 任务ID。
        config: 包含配置信息的字典。

    Returns:
        str: Batch任务ID。
    """
    print(f"   >> 开始提交百炼Batch任务...")

    # 获取配置
    api_key = config.get("DASHSCOPE_API_KEY", "")
    base_url = config.get(
        "BAILIAN_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1"
    )
    batch_endpoint = config.get("BAILIAN_BATCH_ENDPOINT", "/v1/chat/completions")
    completion_window = config.get("BAILIAN_COMPLETION_WINDOW", "24h")

    if not api_key or not api_key.startswith("sk-"):
        print("   ❌ 错误: 百炼API Key未提供或格式无效。")
        raise ValueError("百炼API Key无效")

    try:
        # 初始化OpenAI客户端
        client = openai.OpenAI(api_key=api_key, base_url=base_url)

        # 上传JSONL文件
        print(f"   >> 上传输入文件...")
        with open(jsonl_path, "rb") as file:
            file_object = client.files.create(file=file, purpose="batch")
        print(f"   ✓ 文件上传成功! 文件ID: {file_object.id}")

        # 创建Batch任务
        print(f"   >> 创建Batch任务 (endpoint: {batch_endpoint})...")
        batch = client.batches.create(
            input_file_id=file_object.id,
            endpoint=batch_endpoint,
            completion_window=completion_window,
        )
        print(f"   ✅ Batch任务创建成功! 任务ID: {batch.id}")

        # 将batch_id保存到任务目录(方便后续查询)
        task_dir = os.path.join("uploads", task_id)
        with open(os.path.join(task_dir, "batch_id.txt"), "w") as f:
            f.write(batch.id)

        return batch.id

    except Exception as e:
        print(f"   ❌ 提交百炼Batch任务失败: {e}")
        print(traceback.format_exc())
        raise


def check_bailian_job_status(batch_id: str, config: dict) -> dict:
    """
    检查阿里云百炼Batch任务状态。

    Args:
        batch_id: Batch任务ID。
        config: 包含配置信息的字典。

    Returns:
        dict: 包含任务状态信息的字典。
    """
    print(f"   >> 检查Batch任务状态 (ID: {batch_id})...")

    # 获取配置
    api_key = config.get("DASHSCOPE_API_KEY", "")
    base_url = config.get(
        "BAILIAN_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1"
    )

    try:
        # 初始化OpenAI客户端
        client = openai.OpenAI(api_key=api_key, base_url=base_url)

        # 查询任务状态
        batch = client.batches.retrieve(batch_id=batch_id)
        print(f"   >> Batch任务状态: {batch.status}")

        return {
            "status": batch.status,
            "created_at": batch.created_at,
            "expires_at": batch.expires_at,
            "output_file_id": getattr(batch, "output_file_id", None),
            "error_file_id": getattr(batch, "error_file_id", None),
        }

    except Exception as e:
        print(f"   ❌ 检查Batch任务状态失败: {e}")
        print(traceback.format_exc())
        raise


def download_and_process_bailian_results(
    batch_id: str, original_df: pd.DataFrame, task_id: str, config: dict
) -> pd.DataFrame:
    """
    下载并处理阿里云百炼Batch任务结果。

    Args:
        batch_id: Batch任务ID。
        original_df: 原始数据DataFrame，包含local_row_id。
        task_id: 任务ID。
        config: 包含配置信息的字典。

    Returns:
        pd.DataFrame: 处理后的DataFrame。
    """
    print(f"   >> 开始下载并处理百炼Batch任务结果...")

    # 获取配置
    api_key = config.get("DASHSCOPE_API_KEY", "")
    base_url = config.get(
        "BAILIAN_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1"
    )
    target_columns_config = config.get(
        "TARGET_COLUMNS", ["公司名称", "联系人", "职位", "电话", "来源"]
    )
    source_column_name = "来源"

    # 准备给LLM的目标列(不含来源)
    llm_target_columns = [
        col for col in target_columns_config if col != source_column_name
    ]

    # 确保任务目录存在
    task_dir = os.path.join("uploads", task_id)
    os.makedirs(task_dir, exist_ok=True)

    try:
        # 初始化OpenAI客户端
        client = openai.OpenAI(api_key=api_key, base_url=base_url)

        # 最终检查任务状态并获取文件ID
        batch = client.batches.retrieve(batch_id=batch_id)
        if batch.status != "completed":
            raise ValueError(f"Batch任务状态为 {batch.status}，而非 completed")

        # 获取结果文件ID
        output_file_id = batch.output_file_id
        error_file_id = getattr(batch, "error_file_id", None)

        # 下载结果文件
        print(f"   >> 下载结果文件 (ID: {output_file_id})...")
        output_content = client.files.content(output_file_id)
        output_path = os.path.join(task_dir, "batch_result.jsonl")
        output_content.write_to_file(output_path)
        print(f"   ✓ 结果文件已保存到: {output_path}")

        # 如果有错误文件，也下载
        if error_file_id:
            print(f"   >> 下载错误文件 (ID: {error_file_id})...")
            error_content = client.files.content(error_file_id)
            error_path = os.path.join(task_dir, "batch_error.jsonl")
            error_content.write_to_file(error_path)
            print(f"   ✓ 错误文件已保存到: {error_path}")

        # 处理结果文件
        print(f"   >> 开始解析结果文件...")
        results = []
        # 读取提交的批次请求中保存的row_ids映射
        batch_row_ids_map = {}

        # 从输入文件中恢复批次到行ID的映射
        input_path = os.path.join(task_dir, "batch_input.jsonl")
        if os.path.exists(input_path):
            with open(input_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        record = json.loads(line)
                        custom_id = record.get("custom_id")
                        row_ids = record.get("row_ids", [])
                        if custom_id and row_ids:
                            batch_row_ids_map[custom_id] = row_ids
                    except Exception as e:
                        print(f"   ⚠️ 读取输入文件时出错: {e}")
                        continue

        # 打印找到的批次映射关系
        print(f"   ✓ 找到 {len(batch_row_ids_map)} 个批次的行ID映射")
        for custom_id, row_ids in batch_row_ids_map.items():
            print(f"   >> 批次 {custom_id} 包含 {len(row_ids)} 个行ID")

        # 解析结果JSONL文件
        with open(output_path, "r", encoding="utf-8") as f:
            for line_number, line in enumerate(f, start=1):
                try:
                    # 解析JSONL行
                    record = json.loads(line)
                    custom_id = record.get("custom_id")
                    print(f"   >> 处理批次 {custom_id} 的结果...")

                    # 获取对应批次的row_ids
                    batch_row_ids = batch_row_ids_map.get(custom_id, [])
                    if not batch_row_ids:
                        print(f"   ⚠️ 警告: 找不到批次 {custom_id} 的行ID映射")
                        continue

                    # 提取LLM返回的内容
                    content = None
                    if "response" in record and "body" in record["response"]:
                        content = (
                            record["response"]["body"]["choices"][0]
                            .get("message", {})
                            .get("content", "")
                        )
                    elif "body" in record and "choices" in record["body"]:
                        content = (
                            record["body"]["choices"][0]
                            .get("message", {})
                            .get("content", "")
                        )

                    if not content:
                        print(f"   ⚠️ 警告: 批次 {custom_id} 的结果没有content字段")
                        continue

                    # 打印原始content的前200个字符
                    print(f"   >> 原始content内容(前200字符): {content[:200]}...")

                    try:
                        # 尝试解析JSON数组内容
                        llm_results = json.loads(content)
                        if not isinstance(llm_results, list):
                            # 如果不是数组，尝试包装成数组
                            if isinstance(llm_results, dict):
                                llm_results = [llm_results]
                            else:
                                raise ValueError(
                                    f"LLM返回的不是有效的JSON数组或对象: {content[:100]}..."
                                )

                        print(
                            f"   ✓ 批次 {custom_id} 返回了 {len(llm_results)} 个结果项"
                        )

                        # 处理返回的每个结果项
                        for i, result_item in enumerate(llm_results):
                            # 打印第一个结果项示例
                            if i == 0:
                                print(
                                    f"   >> 结果项示例: {json.dumps(result_item, ensure_ascii=False)[:200]}..."
                                )

                            # 填充行ID
                            row_id = None

                            # 如果结果项数量与行ID数量相等，直接按顺序分配
                            if len(llm_results) == len(batch_row_ids):
                                row_id = batch_row_ids[i]
                            else:
                                # 尝试从结果项中提取行ID
                                if "行ID" in result_item:
                                    row_id = result_item.get("行ID")
                                elif "行id" in result_item:
                                    row_id = result_item.get("行id")
                                elif "rowid" in result_item:
                                    row_id = result_item.get("rowid")
                                elif "row_id" in result_item:
                                    row_id = result_item.get("row_id")

                                # 如果仍找不到行ID，且结果数小于行ID数，尝试按顺序分配
                                if not row_id and i < len(batch_row_ids):
                                    row_id = batch_row_ids[i]

                            if not row_id:
                                print(
                                    f"   ⚠️ 无法确定结果项的行ID，将跳过此项: {json.dumps(result_item, ensure_ascii=False)[:100]}..."
                                )
                                continue

                            # 确保所有目标列都存在
                            result_row = {col: "" for col in llm_target_columns}
                            for col in llm_target_columns:
                                target_col_key = col
                                # 定义可能的列名映射
                                mappings = {
                                    "公司名称": [
                                        "企业名称",
                                        "公司",
                                        "企业",
                                        "company",
                                        "organization",
                                    ],
                                    "联系人": [
                                        "姓名",
                                        "人员",
                                        "名字",
                                        "contact",
                                        "name",
                                    ],
                                    "职位": [
                                        "职务",
                                        "岗位",
                                        "title",
                                        "position",
                                        "job",
                                    ],
                                    "电话": [
                                        "手机",
                                        "联系方式",
                                        "mobile",
                                        "phone",
                                        "tel",
                                    ],
                                }

                                # 先尝试直接匹配
                                if col in result_item:
                                    result_row[col] = str(result_item[col])
                                else:
                                    # 尝试使用映射查找
                                    for target, alternatives in mappings.items():
                                        if col == target:
                                            for alt in alternatives:
                                                if alt in result_item:
                                                    result_row[col] = str(
                                                        result_item[alt]
                                                    )
                                                    break

                            # 添加local_row_id
                            result_row["local_row_id"] = row_id
                            results.append(result_row)

                    except json.JSONDecodeError as e:
                        print(
                            f"   ⚠️ 无法解析批次 {custom_id} 的JSON内容: {e}\n内容示例: {content[:200]}..."
                        )
                        # 为批次中的每一行添加错误标记
                        for row_id in batch_row_ids:
                            error_row = {
                                col: "LLM_OUTPUT_PARSE_ERROR"
                                for col in llm_target_columns
                            }
                            error_row["local_row_id"] = row_id
                            results.append(error_row)
                except Exception as e:
                    print(f"   ⚠️ 处理结果文件第{line_number}行时出错: {e}")
                    print(f"   >> 错误行内容(前200字符): {line[:200]}...")
                    print(traceback.format_exc())
                    continue

        # 处理错误文件(如果存在)
        if error_file_id and os.path.exists(error_path):
            print(f"   >> 开始解析错误文件...")
            with open(error_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        record = json.loads(line)
                        custom_id = record.get("custom_id")
                        error_message = record.get("error", {}).get(
                            "message", "Unknown error"
                        )
                        print(f"   ⚠️ 批次请求 {custom_id} 失败: {error_message}")

                        # 获取对应批次的row_ids
                        batch_row_ids = batch_row_ids_map.get(custom_id, [])

                        # 为批次中的每一行添加错误标记
                        for row_id in batch_row_ids:
                            error_row = {
                                col: "BATCH_PROCESSING_ERROR"
                                for col in llm_target_columns
                            }
                            error_row["local_row_id"] = row_id
                            results.append(error_row)
                    except Exception as e:
                        print(f"   ⚠️ 处理错误文件行时出错: {e}")
                        continue

        # 创建结果DataFrame
        print(f"   >> 创建结果DataFrame...")
        results_df = pd.DataFrame(results)

        # 打印解析结果的行数和列
        if not results_df.empty:
            print(
                f"   ✓ 解析得到 {len(results_df)} 行数据，列名: {list(results_df.columns)}"
            )
        else:
            print(f"   ⚠️ 警告: 处理后的结果为空DataFrame")
            # 返回空DataFrame，但确保列名符合预期
            empty_df = pd.DataFrame(columns=list(llm_target_columns) + ["local_row_id"])
            # 如果没有任何结果，返回空DataFrame
            return empty_df

        # 有些行可能在LLM处理中被拆分成多行(手机号拆分)，需要标记这些情况
        # 获取所有具有重复local_row_id的行
        dup_local_ids = results_df["local_row_id"].duplicated(keep=False)
        if dup_local_ids.any():
            print(
                f"   ℹ️ 发现 {dup_local_ids.sum()} 行有重复的local_row_id (可能是手机号拆分)"
            )

        # 检查是否有行ID丢失
        original_ids = set(original_df["local_row_id"].astype(str))
        result_ids = set(results_df["local_row_id"].astype(str))
        missing_ids = original_ids - result_ids
        if missing_ids:
            print(f"   ⚠️ 有 {len(missing_ids)} 行数据在处理过程中丢失")
            # 可以选择为丢失的行添加错误标记
            for missing_id in missing_ids:
                error_row = {
                    col: "ROW_MISSING_IN_RESULTS" for col in llm_target_columns
                }
                error_row["local_row_id"] = missing_id
                # 添加到结果DataFrame
                results_df = pd.concat(
                    [results_df, pd.DataFrame([error_row])], ignore_index=True
                )

        # 为results_df添加来源列 - 从原始DataFrame中获取
        print(f"   >> 添加来源信息...")
        # 创建local_row_id到来源的映射
        source_map = {}
        for _, row in original_df.iterrows():
            source_map[str(row["local_row_id"])] = row.get("来源", "未知来源")

        # 添加来源列
        results_df[source_column_name] = (
            results_df["local_row_id"].astype(str).map(source_map)
        )
        # 对于映射失败的行(来源为NaN)，填充默认值
        results_df[source_column_name] = results_df[source_column_name].fillna(
            "未知来源"
        )

        # 重新排序列，确保标准列序
        standard_cols = list(llm_target_columns) + [source_column_name]
        other_cols = [
            col
            for col in results_df.columns
            if col not in standard_cols and col != "local_row_id"
        ]
        final_cols = standard_cols + other_cols + ["local_row_id"]
        results_df = results_df.reindex(
            columns=[col for col in final_cols if col in results_df.columns]
        )

        # 保存处理后的数据
        results_path = os.path.join(task_dir, "processed_results.csv")
        results_df.to_csv(results_path, index=False)
        print(f"   ✅ 处理后的结果已保存到: {results_path}")

        return results_df

    except Exception as e:
        print(f"   ❌ 处理百炼Batch结果时发生错误: {e}")
        print(traceback.format_exc())
        raise


def extract_standardize_batch_with_llm(
    batch_rows: list[dict],  # 当前批次的数据行 (字典列表)
    source_headers: list,  # 源文件的列标题列表
    target_columns: list,  # 目标输出列名列表
    api_key: str,  # DeepSeek API Key
    api_endpoint: str,  # DeepSeek API 端点 URL
    max_tokens: int,  # API 调用允许的最大完成 token 数
    timeout: int,  # API 请求的超时时间 (秒)
) -> Union[List[Dict], None]:
    """
    使用 DeepSeek LLM API 对一批数据进行信息提取和标准化。
    包含重试逻辑和常见的 API 错误处理。

    Args:
        batch_rows: 当前批次的数据，每个元素是一个字典，代表一行。
        source_headers: 源 Excel/CSV 的列标题列表。
        target_columns: 需要提取和生成的标准列名列表。
        api_key: DeepSeek API 密钥。
        api_endpoint: DeepSeek API 的 URL。
        max_tokens: API 调用时 `max_tokens` 参数。
        timeout: API 请求的超时时间 (秒)。

    Returns:
        Union[List[Dict], None]: 成功处理则返回标准化后的数据列表 (可能包含错误标记)，
                           如果 API 调用失败或发生严重错误，则返回 None。
                           如果 API Key 无效，则返回包含 API_KEY_ERROR 标记的列表。
    """
    # 如果输入批次为空，直接返回空列表
    if not batch_rows:
        return []

    # 检查 API Key 是否有效
    if not api_key or not api_key.startswith("sk-"):
        print("⚠️ DeepSeek API Key 缺失或无效。跳过 LLM 处理。")
        # 为批次中的每一行返回一个错误标记字典
        return [{col: "API_KEY_ERROR" for col in target_columns} for _ in batch_rows]

    # --- 准备 API 请求 ---
    # 将目标列名列表转换为 JSON 字符串，用于 Prompt
    target_schema_json = json.dumps(target_columns, ensure_ascii=False)
    # 将源列标题转换为字符串列表，再转为 JSON 字符串
    source_headers_str = [str(h) for h in source_headers]
    source_headers_json = json.dumps(source_headers_str, ensure_ascii=False)

    # 构建 System Prompt (系统消息)
    system_content = f"""
你是一个专业的数据处理引擎。你的核心任务是从用户提供的源数据批次 (Source Batch Data) 中，为每一行数据提取信息，并严格按照指定的目标模式 (Target Schema) 进行标准化。
**重要：你的输出必须是一个有效的 JSON 数组 (列表)，数组中的每个元素都是一个对应输入行的、符合 Target Schema 的 JSON 对象。元素的数量必须与输入数组完全一致。**
**目标模式 (Target Schema):**
```json
{target_schema_json}
```
**当源数据中电话/手机号列包含多个号码时（可能用分号、逗号、空格等分隔），你必须将该行拆分为多行，每行对应一个手机号，其他信息保持一致。**
**手机号分隔符识别规则：**
- 支持的分隔符包括：中英文分号（;；）、中英文逗号（,，）、空格、斜杠（/）
- 一行中可能包含多个不同类型的分隔符

**处理规则:**
1.独立处理: 独立分析 "Source Batch Data" 数组中的每一个 JSON 对象。
2.提取与映射: 结合 "Source Headers" 上下文，将信息映射到 "Target Schema" 字段。找不到信息则对应值设为 ""。所有值必须为字符串。
3.最终输出格式: 你的最终输出必须且只能是一个有效的 JSON 数组 (列表)。
4.数组内容: 此数组包含 {len(batch_rows)} 个元素，每个元素是符合 "Target Schema" 的 JSON 对象。
5.绝对禁止: 绝对不要将最终的 JSON 数组包装在任何其他 JSON 对象或键中（例如，不要像 {{ "results": [...] }} 或 {{ "processed_data": [...] }} 这样）。直接输出 [ 开头，] 结尾的数组本身。
6.无额外内容: 不要包含任何解释、注释或标记。
请严格按照要求，直接生成最终的 JSON 数组。
        """

    # 将当前批次的数据行转换为 JSON 字符串，确保所有值为字符串
    batch_data_json = json.dumps(
        [{k: str(v) for k, v in row.items()} for row in batch_rows],
        ensure_ascii=False,  # 允许非 ASCII 字符
        indent=None,  # 不进行缩进，减少 token 占用
    )

    print(f"source_headers_json: {source_headers_json}")
    print(f"batch_data_json: {batch_data_json}")
    # 构建 User Prompt (用户消息)
    user_content = f"""
        这是本次需要处理的源数据表格的列标题 (Source Headers):
        {source_headers_json}
        这是包含 {len(batch_rows)} 行源数据的 JSON 数组 (Source Batch Data):
        {batch_data_json}
        请根据你在 System 指令中被赋予的角色和规则，处理这个批次的数据，并返回标准化的 JSON 数组。
        """

    # 设置请求头
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
    # 构建请求体 (payload)
    payload = {
        "model": "deepseek-chat",  # 可以考虑将其也设为可配置项
        "messages": [
            {"role": "system", "content": system_content},
            {"role": "user", "content": user_content},
        ],
        "max_tokens": max_tokens,
        "temperature": 1,  # 可以考虑设为可配置项，1 表示较高的创造性/随机性
        # "stream": False, # 流式输出目前不适用于批处理解析
    }

    # --- API 调用与错误处理 ---
    max_retries = 2  # 最大重试次数
    retry_delay = 5  # 重试间隔时间 (秒)

    # 循环尝试调用 API
    for attempt in range(max_retries):
        try:
            print(
                f"      ... 发送批次数据到 LLM API (尝试 {attempt + 1}/{max_retries})..."
            )
            # 发送 POST 请求
            response = requests.post(
                api_endpoint,
                headers=headers,
                json=payload,
                timeout=timeout,  # 使用传入的超时时间
            )
            # 检查 HTTP 状态码，如果不是 2xx，则抛出异常
            response.raise_for_status()
            # 解析返回的 JSON 数据
            result_json = response.json()
            # (调试用) 打印完整的返回 JSON
            # print(json.dumps(result_json, ensure_ascii=False, indent=2))

            content_str = None
            # 提取 API 使用情况 (token 数量)
            usage_info = result_json.get("usage")
            if usage_info:
                print(
                    f"      [API 使用情况]: Prompt: {usage_info.get('prompt_tokens', 'N/A')}, Completion: {usage_info.get('completion_tokens', 'N/A')}, Total: {usage_info.get('total_tokens', 'N/A')}"
                )

            # 从返回结果中提取模型生成的内容
            if "choices" in result_json and result_json["choices"]:
                message = result_json["choices"][0].get("message", {})
                content_str = message.get("content")
            print(f"content_str: {content_str}")
            # --- 解析和验证 LLM 返回的内容 ---
            if content_str:
                try:
                    # 清理返回内容两端的空白字符和可能的代码块标记
                    content_str_cleaned = content_str.strip()
                    if content_str_cleaned.startswith("```json"):
                        content_str_cleaned = content_str_cleaned[7:-3].strip()
                    elif content_str_cleaned.startswith("```"):
                        content_str_cleaned = content_str_cleaned[3:-3].strip()

                    # 尝试将清理后的字符串解析为 JSON (预期是一个列表)
                    standardized_batch = json.loads(content_str_cleaned)

                    # 验证返回的是否为列表，且列表长度大于等于输入批次长度（支持多手机号拆分）
                    if isinstance(standardized_batch, list) and len(
                        standardized_batch
                    ) >= len(batch_rows):
                        if len(standardized_batch) > len(batch_rows):
                            print(
                                f"      -> LLM返回了 {len(standardized_batch)} 行（原始输入 {len(batch_rows)} 行），可能包含手机号拆分后的多行数据。"
                            )

                        # 建立原始行索引到可能的拆分行映射关系
                        original_row_mapping = {}
                        processed_rows = 0

                        # 遍历LLM返回的每一行结果
                        for result_idx, result_row in enumerate(standardized_batch):
                            if isinstance(result_row, dict):
                                # 确定此行对应的原始行索引
                                # 对于拆分行，多个结果行会对应同一个原始行
                                original_idx = min(result_idx, len(batch_rows) - 1)

                                # 当处理到新的原始行时，增加计数
                                if original_idx not in original_row_mapping:
                                    original_row_mapping[original_idx] = 0

                                # 获取这个原始行的local_id
                                local_id = batch_rows[original_idx]["local_row_id"]

                                # 保存结果行信息
                                result_row["local_row_id"] = local_id
                                source_name = local_id_to_source_map.get(
                                    local_id, "UNKNOWN_SOURCE"
                                )
                                result_row[source_column_name] = source_name
                                processed_batch_with_ids_and_source.append(result_row)

                                # 增加这个原始行的处理计数
                                original_row_mapping[original_idx] += 1
                                processed_rows += 1
                            else:
                                # 处理非字典项
                                print(
                                    f"      ⚠️ 警告: LLM 返回列表项不是字典 (索引 {result_idx})。添加错误占位符。"
                                )
                                # 确定最相近的原始行索引
                                original_idx = min(result_idx, len(batch_rows) - 1)
                                local_id = batch_rows[original_idx]["local_row_id"]

                                error_row = {
                                    col: "LLM_ITEM_FORMAT_ERROR"
                                    for col in target_columns
                                }
                                error_row["local_row_id"] = local_id
                                error_row[source_column_name] = (
                                    local_id_to_source_map.get(
                                        local_id, "UNKNOWN_SOURCE"
                                    )
                                )
                                processed_batch_with_ids_and_source.append(error_row)

                        # 打印拆分结果统计
                        split_stat = ", ".join(
                            [
                                f"行{idx+1}: {count}条"
                                for idx, count in original_row_mapping.items()
                            ]
                        )
                        print(f"      拆分统计：{split_stat}")

                        # Filter for valid results *after* adding ID and Source
                        valid_results_in_batch = [
                            res
                            for res in processed_batch_with_ids_and_source
                            if not any(
                                str(v).startswith(("LLM_", "API_KEY_"))
                                for k, v in res.items()
                                # Exclude local_id and 来源 from error check
                                if k not in ["local_row_id", source_column_name]
                            )
                        ]
                        consolidated_data.extend(
                            processed_batch_with_ids_and_source
                        )  # Add results with ID and Source
                        rows_processed_in_file_success += len(valid_results_in_batch)
                        total_rows_successfully_processed += len(valid_results_in_batch)
                        print(
                            f"      批次 {batch_num+1} 处理完成。收到并添加 ID 和来源到 {len(processed_batch_with_ids_and_source)} 条结果 ({len(valid_results_in_batch)} 条有效)。"
                        )

                    else:
                        # Handle length mismatch
                        print(
                            f"      ❌ 警告: LLM 返回列表长度不匹配... 添加错误标记。"
                        )
                        for local_id in batch_rows:
                            error_row = {
                                col: "BATCH_LENGTH_MISMATCH" for col in target_columns
                            }
                            error_row["local_row_id"] = local_id
                            error_row[source_column_name] = local_id_to_source_map.get(
                                local_id, "UNKNOWN_SOURCE"
                            )
                            consolidated_data.append(error_row)
                except json.JSONDecodeError as json_err:
                    # 如果返回的内容无法解析为 JSON
                    print(
                        f"      ❌ LLM 返回内容不是有效的 JSON 数组 (尝试 {attempt + 1}): {json_err}"
                    )
                    # 打印部分原始返回内容，帮助诊断
                    print(
                        f"         原始返回内容 (前 500 字符): {content_str[:500]}..."
                    )
                    # 进入重试流程
                except Exception as e:
                    # 捕获解析过程中其他未预料的错误
                    print(f"      ❌ 解析 LLM 返回的 JSON 数组时出错: {e}")
                    print(traceback.format_exc())
                    # 遇到未知解析错误，不再重试此批次
                    break

            else:
                # 如果 API 返回结果中没有 'content'
                print(f"      ❌ LLM 返回结果缺少 'content' (尝试 {attempt + 1})。")
                if "error" in result_json:
                    print(f"         API 错误信息: {result_json['error']}")
                # 进入重试流程

        # --- 捕获网络和 API 相关的异常 ---
        except requests.exceptions.Timeout:
            print(f"      ❌ LLM API 请求超时 (尝试 {attempt + 1})。")
            # 进入重试流程
        except requests.exceptions.HTTPError as http_err:
            # 捕获 HTTP 错误 (如 4xx, 5xx)
            print(f"      ❌ LLM API HTTP 错误 (尝试 {attempt + 1}): {http_err}")
            if http_err.response is not None:
                status_code = http_err.response.status_code
                response_text = http_err.response.text
                print(f"         状态码: {status_code}")
                # 特殊处理：上下文长度超限错误 (400 Bad Request)
                if status_code == 400 and (
                    "context_length_exceeded" in response_text.lower()
                    or "prompt is too long" in response_text.lower()
                    or "maximum context length" in response_text.lower()
                ):
                    print(
                        "      ❌ 错误: 输入内容可能超过模型上下文长度限制! 请减小 BATCH_SIZE。"
                    )
                    # 抛出 ValueError，这将终止整个任务的处理
                    raise ValueError("上下文长度超限")
                # 特殊处理：认证/授权错误 (401, 403)
                elif status_code in [401, 403]:
                    print(
                        f"      ❌ API 认证/授权错误 ({status_code})。请检查 API Key。"
                    )
                    raise ValueError("API 认证/授权错误")
                # 特殊处理：速率限制错误 (429)
                elif status_code == 429:
                    print(f"      ❌ 达到 API 速率限制 ({status_code})。")
                    # 如果还有重试次数，则等待更长时间后重试
                    if attempt < max_retries - 1:
                        wait_time = retry_delay * (attempt + 2)  # 增加等待时间
                        print(f"         将在 {wait_time} 秒后重试...")
                        time.sleep(wait_time)
                        continue  # 继续下一次循环尝试
                    else:
                        # 重试次数用尽
                        print("      ❌ 重试后仍然达到速率限制。")
                        raise ValueError("API 速率限制")
                else:
                    # 其他 HTTP 错误
                    print(f"         响应内容 (前 500 字符): {response_text[:500]}...")
            else:
                print("      ❌ HTTP 错误但没有响应对象。")
            # 对于 HTTP 错误 (非 429 且非致命错误)，进入重试流程 (如果还有次数)
        except requests.exceptions.RequestException as e:
            # 捕获其他网络请求相关的错误 (如 DNS 解析失败、连接错误等)
            print(f"      ❌ LLM API 请求失败 (尝试 {attempt + 1}): {e}")
            # 进入重试流程
        except Exception as e:
            # 捕获调用 API 过程中的其他未知错误
            print(f"      ❌ 调用 LLM API 时发生未知错误 (批处理): {e}")
            print(traceback.format_exc())
            # 重新抛出未知错误，终止处理
            raise e

        # --- 重试逻辑 ---
        # 如果当前尝试失败且还有重试次数
        if attempt < max_retries - 1:
            print(f"      将在 {retry_delay} 秒后重试...")
            time.sleep(retry_delay)
        else:
            # 重试次数已用完
            print(f"      ❌ 此批次已达到最大重试次数。")

    # 如果所有重试都失败了
    print(f"      ⚠️ 处理失败，为此批次返回 None。")
    return None  # 返回 None 表示此批次处理失败


# === 主处理函数 ===
def process_files_and_consolidate(
    input_files: List[str],  # 输入文件路径列表
    output_file_path: str,  # 输出 Excel 文件路径
    config: dict,  # 包含配置项的字典 (API Key, Target Columns 等)
    update_progress_callback=None,  # 用于报告进度的回调函数 (可选)
) -> Union[str, Tuple[str, str]]:
    """
    核心处理流程函数，由 app.py 在后台线程中调用。
    修改为使用阿里云百炼 Batch API 处理。

    新流程:
    1. 遍历输入文件列表并读取。
    2. 为每个文件创建百炼 Batch API 输入文件。
    3. 提交给百炼 Batch API 进行处理。
    4. 返回 batch_id 供 app.py 后续轮询。

    Args:
        input_files: 包含一个或多个输入文件完整路径的列表。
        output_file_path: 要保存的最终 Excel 文件的完整路径。
        config: 包含所有配置的字典。
        update_progress_callback: 一个函数，接受 (状态消息, 进度百分比, 已处理文件数, 总文件数) 参数。

    Returns:
        Union[str, Tuple[str, str]]:
            - 当使用阿里云百炼 API 时，返回 (batch_id, task_id) 元组
            - 当处理完成时，返回输出文件的路径。
    """

    # --- 1. 解包配置项 ---
    api_mode = "bailian"  # 默认使用阿里云百炼 API

    # 检查是否有有效的阿里云百炼配置
    print(f"config详情: {json.dumps(config, ensure_ascii=False)}")
    # 检查百炼API相关配置
    print(
        f"百炼API配置: DASHSCOPE_API_KEY存在: {bool(config.get('DASHSCOPE_API_KEY'))}"
    )
    print(
        f"百炼API配置: DASHSCOPE_API_KEY格式正确: {bool(config.get('DASHSCOPE_API_KEY') and config.get('DASHSCOPE_API_KEY').startswith('sk-'))}"
    )
    print(
        f"百炼API配置: BAILIAN_MODEL_NAME: {config.get('BAILIAN_MODEL_NAME', '未配置')}"
    )
    print(
        f"百炼API配置: BAILIAN_COMPLETION_WINDOW: {config.get('BAILIAN_COMPLETION_WINDOW', '未配置')}"
    )

    if not config.get("DASHSCOPE_API_KEY") or not config.get(
        "DASHSCOPE_API_KEY"
    ).startswith("sk-"):
        api_mode = "deepseek"  # 回退到 DeepSeek API
        print(f"   ⚠️ 警告: 未找到有效的阿里云百炼 API Key，将使用 DeepSeek API。")

    # 生成任务ID作为本次处理的唯一标识
    task_id = str(uuid.uuid4())
    print(f"--- 开始处理，任务ID: {task_id} ---")

    # --- 2. 检查输入文件 ---
    if not input_files:
        print("❌ 错误: 未提供任何输入文件。")
        if update_progress_callback:
            update_progress_callback("错误: 未提供输入文件", 100, 0, 0)
        raise ValueError("未提供任何输入文件")

    total_files = len(input_files)
    print(f"   📄 处理 {total_files} 个输入文件...")

    # --- 3. 准备数据处理 ---
    all_dataframes = []  # 用于存储所有读取的 DataFrames

    for i, file_path in enumerate(input_files):
        current_file_num = i + 1
        file_basename = os.path.basename(file_path)
        print(f"\n📄 读取文件 {current_file_num}/{total_files}: {file_basename}")

        # 报告开始处理当前文件
        if update_progress_callback:
            progress_pct = int((i / total_files) * 50)  # 文件读取占总进度的50%
            update_progress_callback(
                f"读取文件 {current_file_num}/{total_files}: {file_basename}",
                progress_pct,
                i,
                total_files,
            )

        # 读取文件
        df_source = read_input_file(file_path)

        # 如果文件读取失败或为空
        if df_source is None or df_source.empty:
            print(f"   ⚠️ 跳过文件 {file_basename} (读取失败或为空)。")
            continue

        # 添加源文件名称到数据中
        df_source["来源"] = file_basename

        # 将DataFrame添加到列表中
        all_dataframes.append(df_source)
        print(f"   ✓ 成功读取 {len(df_source)} 行数据。")

    # 合并所有DataFrame
    if not all_dataframes:
        print("❌ 错误: 没有成功读取任何文件。")
        if update_progress_callback:
            update_progress_callback("错误: 没有成功读取任何文件", 100, 0, 0)
        raise ValueError("没有成功读取任何文件")

    merged_df = pd.concat(all_dataframes, ignore_index=True)
    print(f"   ✓ 成功合并 {len(merged_df)} 行数据。")

    # 创建任务目录
    task_dir = os.path.join("uploads", task_id)
    os.makedirs(task_dir, exist_ok=True)

    # 保存任务信息到文件
    task_info = {
        "task_id": task_id,
        "input_files": [os.path.basename(f) for f in input_files],
        "output_file": output_file_path,
        "row_count": len(merged_df),
        "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        "api_mode": api_mode,
    }
    with open(os.path.join(task_dir, "task_info.json"), "w") as f:
        json.dump(task_info, f, ensure_ascii=False, indent=2)

    # 保存原始合并数据，以便后续处理
    original_df_path = os.path.join(task_dir, "original_merged.csv")
    merged_df.to_csv(original_df_path, index=False)

    # --- 4. 根据API模式处理数据 ---
    if api_mode == "bailian":
        print(f"   >> 使用阿里云百炼 Batch API 处理数据...")

        if update_progress_callback:
            update_progress_callback(
                "准备百炼Batch API输入文件...",
                60,
                total_files,
                total_files,
            )

        # 创建百炼Batch API输入文件
        jsonl_path = create_bailian_batch_input(merged_df, task_id, config)

        if update_progress_callback:
            update_progress_callback(
                "提交百炼Batch API任务...",
                70,
                total_files,
                total_files,
            )

        # 提交百炼Batch任务
        batch_id = submit_bailian_batch_job(jsonl_path, task_id, config)

        if update_progress_callback:
            update_progress_callback(
                f"已提交百炼Batch任务(ID: {batch_id})，等待处理...",
                80,
                total_files,
                total_files,
            )

        # 返回batch_id和task_id，供app.py后续轮询
        return (batch_id, task_id)

    else:
        # 保留原有的DeepSeek API处理逻辑，用于备选
        print("   ⚠️ 使用原有DeepSeek API处理未实现。请先配置阿里云百炼API。")
        if update_progress_callback:
            update_progress_callback(
                "错误: DeepSeek API处理未实现，请配置阿里云百炼API",
                100,
                total_files,
                total_files,
            )
        raise NotImplementedError("当前版本仅支持阿里云百炼API处理")


# === 用于直接测试 processor.py 的示例代码 (注释掉) ===
# def print_progress(status, progress, files_done, total_files):
#     print(f"[进度更新] 状态: {status} | 进度: {progress}% | 文件: {files_done}/{total_files}")

# if __name__ == '__main__':
#     # 用于直接运行此脚本进行测试的虚拟配置
#     test_config = {
#         "TARGET_COLUMNS": ["公司名称", "姓名", "职务", "电话"],
#         "DEEPSEEK_API_KEY": os.environ.get("DEEPSEEK_API_KEY", "YOUR_DUMMY_KEY_FOR_TESTING"), # 从环境变量获取或使用虚拟 Key
#         "BATCH_SIZE": 5, # 使用较小的批处理大小进行测试
#         "DEEPSEEK_API_ENDPOINT": "https://api.deepseek.com/chat/completions",
#         "MAX_COMPLETION_TOKENS": 2048,
#         "API_TIMEOUT": 60, # 测试用较短超时
#     }
#     # 指定测试用的输入文件列表 (你需要创建这些文件或修改路径)
#     test_input_files = ['./test_data/input1.xlsx', './test_data/input2.csv']
#     # 指定测试用的输出文件路径
#     test_output = './test_output/consolidated_test_output.xlsx'

#     # 确保测试输出目录存在
#     os.makedirs(os.path.dirname(test_output), exist_ok=True)

#     print(f"--- 开始直接测试 processor.py ---")
#     print(f"测试输入文件: {test_input_files}")
#     print(f"测试输出文件: {test_output}")

#     try:
#         # 调用主处理函数进行测试，并传入打印进度的回调函数
#         result = process_files_and_consolidate(test_input_files, test_output, test_config, print_progress)
#         print(f"--- 测试完成，结果保存在: {result} ---")
#     except Exception as e:
#         print(f"--- 测试过程中发生错误 ---: {e}")
#         print(traceback.format_exc())
