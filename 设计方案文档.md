# 设计方案文档

本文档基于[需求文档.md](mdc:需求文档.md)中提出的功能，为其中三个核心需求提供详细的设计方案。

## 1. 公司关联性检查批量处理

**需求背景**：
需求文档 FR3.4 (校验3) 要求对仅手机号重复但公司名不同的情况，调用 LLM API 判断公司名是否关联。当前逐条调用LLM API比较公司名称的方案效率较低。

**目标**：
将公司关联性检查改造为批量模式，提高处理效率。

**设计方案**：

*   **影响模块**：`postprocessor.py`
*   **核心函数**：修改 `check_related_companies_for_duplicate_phones_llm` 或新增专门的批量处理函数 (例如 `batch_check_company_similarity_llm`)。

**详细步骤**：

1.  **收集待比较数据**：
    *   在 `check_related_companies_for_duplicate_phones_llm` 中，遍历所有需要检查关联性的手机号。
    *   收集每个手机号对应的公司名称列表，并为每组分配唯一标识（如 `local_row_id` 或手机号）。
    *   形成任务列表，例如：`tasks_to_batch = [{'id': 'phone_123', 'names': ['公司A', '公司B']}, ...]`

2.  **构造批量API请求**：
    *   新的批量处理函数接收 `tasks_to_batch`。
    *   根据配置的 `BATCH_SIZE` 将任务分割成批次。
    *   为每个批次构造统一的LLM API请求。Prompt需引导LLM对批内每组公司名进行独立判断，并返回结构化结果。
    *   **Prompt示例**：
        ```
        请判断以下各组公司名称是否指代同一家公司或存在紧密关联。针对每一组，请返回结果，格式为：{'id': '组的ID', 'related': true/false, 'related_names': ['相关公司名1', '相关公司名2']}。请确保返回一个包含所有组结果的JSON列表。

        组列表：
        1. {'id': 'phone_123', 'names': ['公司A', '公司B']}
        2. {'id': 'phone_456', 'names': ['公司C', '公司D', '公司E']}
        ...
        ```
    *   根据`config.json`调用百炼。

3.  **解析批量响应**：
    *   LLM API返回包含批内所有任务结果的列表。
    *   根据ID将判断结果（是否关联、关联公司名列表）映射回原始手机号或`local_row_id`。

4.  **更新DataFrame**：
    *   批量更新DataFrame中的`关联公司名称(LLM)`列。

**预期效果**：
*   显著减少API调用次数，提升处理效率。
*   可能需调整`config.json`中LLM调用相关参数（超时、批处理大小）。

## 2. 独立操作流程：上传符合格式的文件 → 直接导入到飞书

**需求背景**：
需求文档FR1.4 和 FR8.2提出允许用户跳过LLM处理和后处理，直接将格式正确的文件导入飞书。

**目标**：
提供灵活的操作路径，支持用户按需直接导入数据到飞书，简化特定场景下的操作。

**设计方案**：

*   **影响模块**：`app.py` (前端交互、路由、核心逻辑)，`templates/index.html` (界面调整)。

**详细步骤**：

1.  **前端界面调整 (`templates/index.html`)**：
    *   不引入显式的"模式选择"按钮。
    *   保持各主要操作按钮（"开始处理"、"上传已编辑/待同步文件"、"同步到飞书"）的可见和可用状态。

2.  **核心文件上传逻辑 (按钮："上传已编辑/待同步文件")**：
    *   此按钮的用途扩展，既可上传本地编辑过的 `final_[task_id].xlsx`，也可上传全新的、符合直接导入飞书格式的Excel文件。
    *   **文件关联逻辑 (采纳方案一)**：
        *   在 `app.py` 中维护一个全局或应用上下文变量 `LATEST_COMPLETED_PREPROCESSING_TASK_ID`，记录最后一个成功完成第一阶段（LLM处理+后处理）的任务ID。
        *   当用户通过"上传已编辑/待同步文件"按钮上传文件时：
            *   前端调用新的无特定`task_id`的上传路由，例如 `/upload_new_or_associate_file` (或在原 `/upload_edited/<task_id>` 路由中增加无ID时的处理逻辑)。
            *   后端检查 `LATEST_COMPLETED_PREPROCESSING_TASK_ID` 是否有效（任务存在且`final_...xlsx`文件存在）。
                *   **若有效**：将上传文件视为对该`LATEST_COMPLETED_PREPROCESSING_TASK_ID`任务的编辑。保存文件到 `outputs/<LATEST_TASK_ID>/edited_<LATEST_TASK_ID>.xlsx`。更新任务字典。返回此`LATEST_TASK_ID`给前端。
                *   **若无效或不存在**：将上传文件视为全新的直接导入飞书的源文件。创建**新的任务ID** (`new_task_id`)。创建任务目录 `uploads/<new_task_id>/`。保存文件为 `uploads/<new_task_id>/direct_import_source.xlsx`。在`tasks`字典和历史记录中为此新任务创建条目。返回此`new_task_id`给前端。
        *   前端接收到后端返回的（旧的或新的）任务ID，并向用户提示文件已关联到哪个任务。

3.  **文件存储**：
    *   对于"全新的待导入文件"，即使跳过LLM处理，也为其创建任务ID，并将文件存储在 `uploads/<新任务ID>/direct_import_source.xlsx`。
    *   这样做可以统一文件管理、历史记录和错误追踪。

4.  **同步到飞书逻辑 (`/sync_to_feishu/<task_id>`)**：
    *   前端调用此接口时，传递当前活动的任务ID。
    *   后端根据 `task_id` 查找关联的文件：优先检查 `edited_file_path` (编辑流程)，若无则检查 `direct_import_source_file_path` (直接导入流程，需在任务字典中记录此路径)。
    *   读取文件，并执行相应的飞书同步操作（读取"新增"、"更新" sheet或处理单sheet文件）。

**预期效果**：
*   用户操作流程更自然和灵活。
*   后端通过 `LATEST_COMPLETED_PREPROCESSING_TASK_ID` 和新生成的 `task_id` 智能管理文件和任务关联。
*   所有用户操作均有任务ID追踪，便于管理。

## 3. 历史记录管理

**需求背景**：
需求文档FR7 (原FR8) 要求提供历史记录管理功能，显示最近任务信息并支持文件下载。

**目标**：
为用户提供一个查看近期操作记录的界面，方便追踪和获取历史结果。

**设计方案 (采纳方案A - JSON文件)**：

*   **影响模块**：`app.py` (记录存储与读取，新API路由)，`templates/` (新增`history.html`页面)。
*   **存储文件**：`task_history.json` (存储一个包含最近10条任务对象的列表)。

**详细步骤**：

1.  **存储信息结构 (每个任务对象)**：
    ```json
    {
      "task_id": "task_20250507_103000_abc123",
      "upload_time": "2025-05-07 10:30:00",
      "operation_type": "完整处理", // "完整处理" 或 "直接导入飞书"
      "status": "成功", // "处理中", "成功", "失败", "部分成功"
      "source_files": ["data_batch_1.xlsx"],
      "processed_file_name": "final_task_20250507_103000_abc123.xlsx", // 若适用
      "processed_file_path_relative": "outputs/task_20250507_103000_abc123/final_task_20250507_103000_abc123.xlsx", // 若适用
      "edited_file_name": "edited_task_20250507_103000_abc123.xlsx", // 若适用
      "edited_file_path_relative": "outputs/task_20250507_103000_abc123/edited_task_20250507_103000_abc123.xlsx", // 若适用
      "direct_import_source_file_name": "direct_import_data.xlsx", // 若适用
      "direct_import_source_file_path_relative": "uploads/task_xxxxxxxxxxxx/direct_import_data.xlsx", // 若适用
      "completion_time": "2025-05-07 10:45:15",
      "sync_status": "同步成功: 新增 10, 更新 5", // 若执行同步
      "error_message": "" // 若失败
    }
    ```

2.  **记录与更新逻辑 (`app.py`)**：
    *   **初始化**：应用启动时读取 `task_history.json`，若不存在或无效则初始化为空列表。
    *   **添加新记录**：新任务启动时（完整处理或直接导入），在历史列表头部添加新记录，状态为"处理中"。
    *   * *更新记录**：任务各阶段完成（LLM处理完成、用户上传编辑文件、同步操作结束）时，更新对应`task_id`的历史记录条目。
    *   **维护列表长度**：确保历史记录列表不超过10条，超过则移除最旧记录（列表末尾）。
    *   **保存历史**：每次更新后，将整个列表写回 `task_history.json`。

3.  **后端API (`app.py`)**：
    *   创建新GET路由 `/history`。
    *   该路由从内存中的历史列表（或重新从文件加载）读取数据，返回JSON格式的最近10条记录。

4.  **前端展示页面 (`templates/history.html`)**：
    *   通过JavaScript调用`/history` API获取数据。
    *   以表格形式展示历史任务，包含所有定义的字段。
    *   "处理后的结果文件"、"编辑后的文件"、"直接导入源文件"等应提供下载链接，指向 `/download/<task_id>/<filename>`。

5.  **文件下载**：
    *   确保 `/download/<task_id>/<filename>` 路由能正确处理来自历史记录的下载请求，使用记录中的相对路径构建完整文件路径。

**预期效果**：
*   用户可以方便地查看最近的操作历史和结果。
*   通过JSON文件实现简单持久化，避免内存数据丢失。
*   下载功能与现有机制集成。

## 4.多Sheet页数据处理逻辑

**需求背景**：

在企业客户信息管理与飞书多维表格同步场景中，用户通常需要将来自多个来源（如本地Excel、飞书表格等）的数据进行整合、清洗和去重，并最终生成结构化的多Sheet页Excel文件，便于后续批量同步、数据追溯和人工复核。

实际业务中存在如下典型痛点与挑战：

1. **数据合并后信息丢失**  
   传统的合并与去重逻辑（如简单按手机号合并）容易导致原始数据大量丢失。尤其当本地与飞书数据存在重复手机号但公司名不同、来源不同等复杂情况时，部分本地数据在最终结果中被覆盖或遗漏，影响数据完整性和可追溯性。

2. **业务规则复杂多变**  
   不同Sheet页（如"原始数据""新增""更新"）对数据的处理要求不同：  
   - "原始数据"Sheet需完整保留所有合并后的原始数据，便于溯源和复查。  
   - "新增"Sheet需智能筛选真正需要新增到飞书的数据，避免无效或重复推送。  
   - "更新"Sheet需精准识别哪些飞书已有数据因合并、去重等操作需要被更新，减少不必要的API调用。

3. **后处理链路复杂且易错**  
   数据清洗、去重、合并、智能相关性判断（如LLM公司名相关性）、格式校验等后处理步骤多且环环相扣，任何环节处理不当都可能导致数据丢失、冗余或同步失败。

4. **用户体验与可维护性**  
   用户希望在结果文件中能一目了然地看到原始数据、需新增数据、需更新数据的全貌，并能追溯每条数据的来源和处理过程。系统需具备良好的可扩展性和可维护性，便于后续业务规则调整和功能升级。

**综上所述，系统需设计一套科学、可扩展的多Sheet页数据处理逻辑，既保证原始数据的完整性，又能智能筛选和合并新增/更新数据，提升数据同步质量和用户体验。**

**目标**：
优化多Sheet页Excel生成逻辑，保留原始数据的完整性，同时对"新增"和"更新"sheet页应用智能去重和合并规则，提升数据质量与飞书同步效率。

### 4.1 处理流程总览

1. 合并本地与飞书数据，生成合并后原始DataFrame（combined_df），并保存为`raw_combined_{task_id}.csv`。
2. 对合并后的DataFrame进行后处理（apply_post_processing），包括清洗、去重、合并、标记等多步操作，输出处理后DataFrame和需更新的行ID集合。
3. 生成多Sheet页Excel文件：
   - "原始数据"Sheet：以合并后原始数据为主表，merge后处理结果（如备注、LLM判断等字段），按唯一ID对齐，既保留原始字段又包含所有校验和智能判断结果。
   - "新增"Sheet：后处理后，筛选record_id为空、手机号格式正确、有效性通过、合并规则后的数据。
   - "更新"Sheet：后处理后，筛选需更新的有record_id的行，且确实因合并操作需要更新的记录。
   - 若"新增"Sheet数据量超过飞书单表5万行限制，自动分表（新增、新增-2、...）。

### 4.2 后处理步骤与Sheet分类（优化版）

1. **全局数据清理**
   - 去除所有字符串字段的首尾空格、换行符，标准化数据格式。
2. **手机号格式校验**
   - 剔除手机号为空、非11位、非数字等无效数据。
   - 备注中标记"手机号格式错误"，这些数据不参与后续任何Sheet分类。
3. **重复手机号合并**
   - 对同一手机号的多条数据，合并企业名称（分号连接），只保留一条，优先保留有record_id的。
   - 来源字段仅取一条（不再合并）。
4. **LLM企业名称去重**
   - 对合并后的企业名称串，调用LLM判断哪些名称属于同一主体，进行智能去重和合并。
   - LLM处理后，企业名称字段为唯一化、去重后的结果。
   - **实际例子：**
     - **输入示例：**
       - 企业名称列表：['上海汉得', '汉得信息', '甄零科技']
       - LLM Prompt：
         ```
         以下是同一手机号下的企业名称列表，请判断哪些名称属于同一实际公司主体，并将属于同一主体的名称合并为一个分号分隔的字符串，输出所有唯一主体的合并结果列表。例如：['上海汉得', '甄零科技']。
         企业名称列表：['上海汉得', '汉得信息', '甄零科技']
         ```
     - **输出示例：**
       - ['上海汉得', '甄零科技']
     - **输入示例2：**
       - 企业名称列表：['上海汉得', '上海汉得', '汉得信息']
       - LLM Prompt：
         ```
         以下是同一手机号下的企业名称列表，请判断哪些名称属于同一实际公司主体，并将属于同一主体的名称合并为一个分号分隔的字符串，输出所有唯一主体的合并结果列表。例如：['上海汉得']。
         企业名称列表：['上海汉得', '上海汉得', '汉得信息']
         ```
     - **输出示例2：**
       - ['上海汉得']
5. **生成新增和更新Sheet**
   - 有record_id的，内容有变化则进"更新"Sheet。
   - 无record_id的，进"新增"Sheet。
   - 每个手机号在结果Sheet中只出现一次，内容为最终合并结果。

### 4.3 优化与可扩展建议
- 数据有效性过滤、手机号格式校验等规则可参数化，支持自定义。
- 前端或日志中展示各Sheet过滤、合并、去重的统计信息，提升用户透明度。
- 支持更多业务规则的灵活扩展，如企业名称模糊合并、特殊字段处理等。
- 对大体量数据建议分批、流式、限流、异步等优化，提升系统健壮性。

### 4.4 流程图（建议）

```
合并数据（combined_df）
   ↓
保存raw_combined_{task_id}.csv
   ↓
apply_post_processing（后处理多步）
   ↓
生成多Sheet页Excel：
 ├─ 原始数据Sheet（df_original）
 ├─ 新增Sheet（后处理+筛选+合并）
 └─ 更新Sheet（后处理+筛选+合并）
```

**本方案确保原始数据完整性，新增/更新Sheet智能去重合并，提升数据质量与飞书同步效率，具备良好可扩展性和可维护性。**

## 5. 飞书API字段过滤优化

**需求背景**：
在向飞书多维表格同步数据时，系统当前将Excel文件中的所有字段直接发送给飞书API，包括一些内部使用或数据管理用的字段（如`table_id`）。当这些字段与飞书API保留字段冲突时，会导致错误码`1254045` (FieldNameNotFound)，进而造成整批次数据同步失败，影响数据导入成功率和用户体验。

**目标**：
优化飞书数据同步逻辑，确保只有必要的目标字段被发送到飞书API，过滤掉所有可能与API保留字段冲突的字段，提高数据同步的成功率和稳定性。

**设计方案**：

* **影响模块**：`app.py`（`sync_to_feishu`函数）和`feishu_utils.py`（`batch_add_records`和`batch_update_records`函数）
* **核心思路**：实现双层保护机制，采用"白名单+黑名单"的策略确保传递给API的字段安全无冲突

**详细步骤**：

1. **修复配置加载方式**：
   * 问题：`sync_to_feishu`函数使用`load_config()`函数获取配置，该函数返回元组，但代码错误地尝试对其使用字典方法
   * 解决方案：使用全局配置变量而非重新加载配置
   ```python
   # 原来的代码（错误）
   config = load_config()
   feishu_config = config.get("feishu_config", {})
   target_columns = config.get("llm_config", {}).get("TARGET_COLUMNS", [])
   
   # 修改后的代码（正确）
   feishu_config = CURRENT_FEISHU_CONFIG
   target_columns = CURRENT_LLM_CONFIG.get("TARGET_COLUMNS", [])
   ```
   * 优点：不仅修复了类型错误，还避免了重复加载配置文件，提高了性能

2. **实现白名单过滤（`app.py`中）**：
   * 在`sync_to_feishu`函数中，从配置中获取`TARGET_COLUMNS`作为白名单
   * 修改处理新增和更新记录的逻辑，增加字段过滤条件：
     ```python
     # 白名单过滤：只保留目标列
     if target_columns and col not in target_columns:
         continue
     ```
   * 添加日志记录，显示过滤前后每条记录的平均字段数，方便监控
   * 确保ID字段（如`record_id`）不被错误地发送给API

3. **实现黑名单过滤（`feishu_utils.py`中）**：
   * 在API调用函数中定义保留字段黑名单，涵盖各种大小写和命名风格变体
     ```python
     BLACKLIST_FIELDS = [
         "table_id", "Table_ID", "tableid", "TableID", "tableID", "tableId",
         "record_id", "Record_ID", "recordid", "RecordID",
         # 更多可能与API保留字段冲突的字段名...
     ]
     ```
   * 在批量添加和更新函数中，实现字段过滤逻辑：
     ```python
     # 复制原始字段，排除黑名单字段
     for field_name, field_value in record.get("fields", {}).items():
         if field_name not in BLACKLIST_FIELDS:
             filtered_fields[field_name] = field_value
     ```
   * 添加动态黑名单更新机制：当API返回字段错误时，自动将问题字段添加到黑名单
   * 提供详细日志，记录过滤过程和结果

4. **增强错误处理**：
   * 在API错误响应中提取详细信息，特别是字段名相关错误
   * 使用正则表达式从错误消息中识别具体的问题字段名
   * 记录完整的错误日志用于诊断和问题追踪

**预期效果**：
* 系统能够成功处理包含特殊字段名（如`table_id`）的数据文件
* 批量添加和更新操作的成功率显著提高
* 日志系统提供清晰的统计信息，便于审计和问题排查
* 整体方案具有低侵入性，不影响现有功能，仅添加必要的数据过滤和保护层

该优化将使飞书同步功能更加健壮，应对各种数据结构，并确保所有API调用符合飞书多维表格API的规范和限制。

## 6. 飞书多表数据导入优化

**需求背景**：
飞书多维表格有单表50000行的容量限制，当用户需要导入大量数据(可能超过单表容量)时，当前系统采用"全有或全无"的策略，导致无法处理超过单表容量的数据导入需求。例如，当用户尝试导入60000行数据，系统会检查每个目标表格，如果没有找到一个能容纳全部60000行的表格(这实际上是不可能的，因为限制是50000行)，则整个导入操作会失败。

**目标**：
改进大数据量导入机制，支持自动分表导入，确保数据完整性，并为用户提供清晰的容量预检查和引导。

**设计方案**：

* **影响模块**：`app.py`（`sync_to_feishu`函数）
* **核心思路**：实现两阶段导入策略：先检查总容量是否足够，再进行智能分表导入

**详细步骤**：

1. **增强容量预检查机制**：
   * 扫描所有目标表格，计算总可用容量而非仅检查单个表格
   ```python
   total_available_space = 0
   table_spaces = {}
   
   for table_id in add_target_table_ids:
       try:
           current_count = feishu_utils.get_table_record_count(access_token, app_token, table_id)
           available_space = FEISHU_ROW_LIMIT - current_count
           total_available_space += available_space
           table_spaces[table_id] = available_space
           print(f"表格 {table_id} 当前有 {current_count} 条记录，可用空间 {available_space} 行")
       except Exception as count_err:
           print(f"检查表格 {table_id} 记录数时出错: {count_err}")
   ```
   
   * 与待导入的数据总量进行比较，而非查找单个能容纳全部数据的表格
   ```python
   if total_available_space < records_count:
       error_msg = f"所有目标表格的总可用空间({total_available_space}行)不足，无法添加 {records_count} 条记录，请添加更多表格后重试"
       return jsonify({"success": False, "error": error_msg, "required_space": records_count}), 400
   ```

2.  **实现智能分表导入策略**：
   * 按表格可用空间从大到小排序，优先使用空间更大的表格
   ```python
   sorted_tables = sorted(table_spaces.items(), key=lambda x: x[1], reverse=True)
   ```
   
   * 制定分表计划，计算每个表格应该导入多少条记录
   ```python
   distribution_plan = []
   remaining_records = records_count
   
   for table_id, available_space in sorted_tables:
       if remaining_records <= 0:
           break
           
       records_for_table = min(available_space, remaining_records)
       if records_for_table > 0:
           distribution_plan.append({
               "table_id": table_id,
               "records_count": records_for_table,
               "start_index": records_count - remaining_records,
               "end_index": records_count - remaining_records + records_for_table
           })
           remaining_records -= records_for_table
   ```

3.  **事务性导入实现**：
   * 使用两阶段提交模式确保全有或全无
   * 阶段一：创建临时结果集，跟踪每个导入操作的结果
   ```python
   import_results = []
   any_failure = False
   
   for plan in distribution_plan:
       table_id = plan["table_id"]
       start_idx = plan["start_index"]
       end_idx = plan["end_index"]
       records_to_add_subset = records_to_add[start_idx:end_idx]
       
       try:
           print(f"开始导入 {len(records_to_add_subset)} 条记录到表格 {table_id}...")
           add_results = feishu_utils.batch_add_records(
               app_token, table_id, records_to_add_subset, app_id, app_secret
           )
           
           if add_results.get("error_count", 0) > 0:
               any_failure = True
               
           import_results.append({
               "table_id": table_id,
               "plan": plan,
               "results": add_results
           })
       except Exception as e:
           any_failure = True
           import_results.append({
               "table_id": table_id,
               "plan": plan,
               "error": str(e)
           })
   ```
   
   * 阶段二：如果有任何失败，回滚所有导入操作
   ```python
   if any_failure:
       # 回滚所有已导入的记录
       for result in import_results:
           if "results" in result and result["results"].get("success_count", 0) > 0:
               table_id = result["table_id"]
               # 提取刚导入的记录ID进行删除
               # 这里需要feishu_utils提供批量删除API支持
               # ...
       
       error_msg = "部分数据导入失败，已回滚所有操作，请检查错误日志并修正问题后重试"
       return jsonify({"success": False, "error": error_msg, "import_results": import_results}), 500
   ```

4.  **增强用户交互和反馈**：
   * 在预检查阶段提供清晰的容量信息和建议
   * 在导入成功后，提供详细的分表导入报告
   ```python
   # 构建导入成功摘要
   distribution_summary = []
   for result in import_results:
       table_id = result["table_id"]
       success_count = result.get("results", {}).get("success_count", 0)
       distribution_summary.append(f"表格 {table_id}: {success_count} 条")
   
   summary = "分表导入成功: " + ", ".join(distribution_summary)
   ```

**预期效果**：
* 系统能够自动检测总体容量是否足够，提前提示用户添加表格
* 支持超过单表容量限制的大数据量导入，自动进行分表处理
* 保持数据的完整性，所有记录要么全部导入成功，要么全部失败
* 为用户提供清晰的分表导入结果报告，详细说明每个表格的导入情况

该优化将大幅提升系统处理大数据量的能力，使飞书同步功能更加健壮和实用，同时保持用户操作的简单性，不需要用户手动进行数据拆分和多次导入。

## 7. 飞书数据列过滤方案

**需求背景**：
当前实现中，在执行数据合并阶段，系统会保留飞书数据中的所有列，导致"原始数据"Sheet页包含了大量无关列（如"一级行业"、"二级行业"、"创建日期"等）。这些列占用了大量空间，增加了Excel文件的复杂度，降低了用户使用体验。实际上，只需要保留业务关键列和少数系统标识列（如`record_id`和`table_id`）即可完成数据处理和同步操作。

**目标**：
实现对从飞书拉取的数据进行列过滤，只保留必要的系统列和与业务相关的目标列，提高数据清洁度和用户体验。

**设计方案**：

* **影响模块**：`app.py`（调用飞书数据拉取函数）和`feishu_utils.py`（提供数据拉取和过滤功能）
* **核心思路**：在现有`fetch_and_prepare_feishu_data`函数中增加目标列过滤功能

**详细步骤**：

1. **增强`fetch_and_prepare_feishu_data`函数**：
   * 修改函数签名，增加可选参数`target_columns`
   ```python
   def fetch_and_prepare_feishu_data(feishu_config, target_columns=None):
       """
       获取所有指定飞书表格的数据，并将其合并、准备成 DataFrame。
       如果提供了target_columns参数，将只保留record_id、table_id和这些目标列，过滤掉其他飞书特有列。
       
       Args:
           feishu_config (dict): 包含飞书 API 配置的字典。
           target_columns (list, optional): 需要保留的目标业务列列表。默认为None，表示保留所有列。
       
       Returns:
           pd.DataFrame: 包含所有表格数据的合并 DataFrame，如果出错则返回空 DataFrame。
                         DataFrame 包含 record_id 列。
       """
   ```
   
   * 在数据转换为DataFrame之后，增加列过滤逻辑：
   ```python
   # 新增：根据target_columns过滤列
   if target_columns and isinstance(target_columns, list):
       # 确保始终保留系统必要列
       required_cols = ["record_id", "table_id"]
       cols_to_keep = required_cols + [
           col for col in target_columns if col in df_feishu.columns
       ]
       
       # 记录过滤前后的列数量
       original_cols = list(df_feishu.columns)
       original_col_count = len(original_cols)
       
       # 应用过滤
       available_cols = [col for col in cols_to_keep if col in df_feishu.columns]
       if available_cols:
           df_feishu = df_feishu[available_cols]
           filtered_col_count = len(df_feishu.columns)
           removed_cols = set(original_cols) - set(available_cols)
           
           print(f"   🔍 列过滤: 原始列数 {original_col_count} -> 过滤后列数 {filtered_col_count}")
           print(f"   🔍 保留的列: {list(df_feishu.columns)}")
           print(f"   🔍 过滤掉的列: {list(removed_cols)}")
       else:
           print("   ⚠️ 过滤后没有保留任何列，返回原始DataFrame")
   else:
       print("   ℹ️ 未提供目标列表，返回所有列")
   ```

2. **修改`app.py`中的调用点**：
   * 找到所有调用`fetch_and_prepare_feishu_data`的位置
   * 传递目标列参数（从配置中获取）：
   ```python
   # 修改前
   df_feishu = feishu_utils.fetch_and_prepare_feishu_data(config["feishu_config"])
   
   # 修改后
   df_feishu = feishu_utils.fetch_and_prepare_feishu_data(
       config["feishu_config"], 
       config.get("llm_config", {}).get("TARGET_COLUMNS", [])
   )
   ```

**预期效果**：
* 从飞书拉取的数据将只包含必要的系统列（`record_id`, `table_id`）和用户指定的目标业务列
* "原始数据"Sheet页更加简洁，不再包含大量无关的飞书特有列
* Excel文件大小减小，用户处理体验大幅提升
* 系统日志清晰展示列过滤前后的统计信息，便于调试和优化

该设计方案确保对现有代码的修改保持最小侵入性，同时通过参数默认值（`target_columns=None`）保持了向后兼容性，不会破坏已有功能。

## 8. 数据有效性过滤方案

**需求背景**：
在当前实现中，部分数据行的关键字段（如企业名称和电话）为空，这些无效数据会在数据处理流程中被保留并进入"新增"Sheet页。这些无效数据占用了飞书表格空间，且对业务没有实际价值。用户需要一种机制来过滤掉这些无效数据，以提高数据质量和减少资源浪费。

**目标**：
实现数据有效性过滤功能，在数据分类阶段（决定哪些数据进入"新增"Sheet页时）自动过滤掉企业名称和电话均为空的无效记录，同时保留原始数据的完整性。

**设计方案**：

* **影响模块**：`postprocessor.py`中的`create_multi_sheet_excel`函数
* **核心思路**：在将数据分类到"新增"Sheet页时添加有效性检查逻辑

**详细步骤**：

1. **修改`create_multi_sheet_excel`函数的新增Sheet数据处理逻辑**：
   ```python
   # 在df_new的处理逻辑中，添加有效性过滤机制
   # df_new是准备放入"新增"Sheet的数据子集
   
   # 记录过滤前的行数
   original_new_count = len(df_new)
   
   # 添加数据质量过滤：企业名称或电话至少有一个不为空
   company_col = "企业名称"  # 企业名称列
   phone_col = "电话"  # 电话列
   
   # 构建过滤条件：企业名称或电话至少有一个非空
   mask_company = df_new[company_col].notna() & df_new[company_col].astype(str).str.strip().ne("")
   mask_phone = df_new[phone_col].notna() & df_new[phone_col].astype(str).str.strip().ne("")
   
   # 应用过滤条件
   df_new_filtered = df_new[mask_company | mask_phone].copy()
   
   # 计算被过滤掉的行数
   filtered_count = original_new_count - len(df_new_filtered)
   
   print(f"数据有效性过滤: 从'新增'Sheet过滤掉 {filtered_count} 条无效数据（企业名称和电话均为空）")
   ```

2. **将过滤结果记录到日志**：
   ```python
   # 记录过滤结果，用于任务状态和日志
   if filtered_count > 0:
       print(f"数据有效性过滤完成: 原始'新增'数据 {original_new_count} 条，有效数据 {len(df_new_filtered)} 条")
   ```

3. **使用过滤后的新增数据写入Excel**：
   ```python
   # 在保存到Excel文件时使用过滤后的数据
   if not df_new_filtered.empty:
       df_new_filtered.to_excel(writer, sheet_name="新增", index=False)
   ```

4. **保持原始数据Sheet页的完整性**：
   ```python
   # 原始数据保持完整，不进行过滤
   df_original.to_excel(writer, sheet_name="原始数据", index=False)
   ```

**预期效果**：
* 保留"原始数据"Sheet中的所有记录，包括无效数据，保持数据完整性
* "新增"Sheet中只包含有效的业务数据（企业名称或电话至少有一个不为空）
* 提高飞书同步数据的质量，减少无效数据对表格资源的占用
* 用户体验优化：用户只需查看和处理有业务价值的数据
* 操作流程不变：用户无需手动过滤数据，系统自动完成

**扩展优化**：
1. **可配置的过滤规则**：将过滤条件添加到配置文件中，允许用户自定义必填字段
   ```json
   "filter_config": {
     "required_fields": ["企业名称", "电话"],
     "require_all": false
   }
   ```

2. **更精细的有效性判断**：除了非空检查外，还可以增加更复杂的数据有效性检查
   * 电话号码格式验证（如必须为11位数字）
   * 企业名称最小字符长度要求
   * 特定字段的取值范围限制

3. **过滤统计展示**：在前端界面显示过滤统计信息，使用户了解数据质量情况

该设计方案可以有效解决无效数据问题，显著提高数据质量，同时保持低侵入性，不影响现有功能和用户操作流程。
 