# -*- coding: utf-8 -*-
import requests
import pandas as pd
import time
import numpy as np
import os
import json
import traceback  # ç”¨äºæ‰“å°è¯¦ç»†çš„é”™è¯¯å †æ ˆä¿¡æ¯
import math  # ç”¨äºè®¡ç®—æ‰¹å¤„ç†æ•°é‡
import uuid  # Added import
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, Dict, List, Optional, Tuple, Union

# === Logger Setup ===
# Create a logger instance
logger = logging.getLogger(__name__)
# Set the logging level (e.g., INFO, DEBUG, WARNING)
logger.setLevel(logging.INFO)
# Create a handler (e.g., StreamHandler to output to console)
if not logger.handlers:  # Avoid adding multiple handlers if reloaded
    handler = logging.StreamHandler()
    # Create a formatter and set it for the handler
    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    handler.setFormatter(formatter)
    # Add the handler to the logger
    logger.addHandler(handler)

# === é…ç½®é¡¹ (è¿™äº›å€¼ç°åœ¨ç”± app.py ä¼ å…¥) ===
# DEEPSEEK_API_KEY = "YOUR_API_KEY"       # ä» app.py è·å–
# TARGET_COLUMNS = [...]                  # ä» app.py è·å–
# BATCH_SIZE = 160                        # ä» app.py è·å–
# MAX_COMPLETION_TOKENS = 8192            # ä» app.py è·å–
# DEEPSEEK_API_ENDPOINT = "https://api.deepseek.com/chat/completions" # ä» app.py è·å–


def read_input_file(
    file_path: str,  # required_columns: Optional[List[str]] = None # Removed this parameter
) -> pd.DataFrame:
    """
    Reads an Excel or CSV file into a Pandas DataFrame.
    # Removed validation of required columns.
    Adds a unique local_row_id to each row.
    *** Removed adding 'æ¥æº' column here. ***
    """
    logger.info(f"Reading file: {file_path}")
    try:
        if file_path.endswith((".xlsx", ".xls")):
            # æ·»åŠ converterså‚æ•°ç¡®ä¿ç”µè¯åˆ—ä¸ºå­—ç¬¦ä¸²
            # æ³¨æ„ï¼šç”±äºè¿™é‡Œæ— æ³•è®¿é—®configï¼Œä½¿ç”¨æ‰€æœ‰å¯èƒ½çš„ç”µè¯åˆ—åä½œä¸ºå­—ç¬¦ä¸²è½¬æ¢
            possible_phone_cols = [
                "ç”µè¯",
                "æ‰‹æœº",
                "ç”µè¯å·ç ",
                "æ‰‹æœºå·ç ",
                "è”ç³»æ–¹å¼",
                "ç”µè¯/æ‰‹æœº",
            ]
            converters = {col: str for col in possible_phone_cols}
            df = pd.read_excel(file_path, engine="openpyxl", converters=converters)
        elif file_path.endswith(".csv"):
            try:
                df = pd.read_csv(file_path, encoding="utf-8")
            except UnicodeDecodeError:
                logger.warning(f"UTF-8 decoding failed for {file_path}, trying gbk.")
                df = pd.read_csv(file_path, encoding="gbk")
        else:
            raise ValueError(
                "Unsupported file format. Please use Excel (.xlsx, .xls) or CSV (.csv)."
            )

        logger.info(f"Successfully read {len(df)} rows from {file_path}.")

        # Clean column names (strip whitespace)
        df.columns = df.columns.str.strip()
        logger.debug(f"Original columns: {list(df.columns)}")

        # Validate required columns - REMOVED
        # if required_columns:
        #     missing_cols = [col for col in required_columns if col not in df.columns]
        #     if missing_cols:
        #         raise ValueError(f"Missing required columns in {file_path}: {', '.join(missing_cols)}")
        #     logger.info(f"All required columns {required_columns} found in {file_path}.")

        # Add local_row_id
        if "local_row_id" not in df.columns:
            df["local_row_id"] = [str(uuid.uuid4()) for _ in range(len(df))]
            logger.info(f"Added 'local_row_id' column to DataFrame from {file_path}.")
        else:
            # Handle case where column might exist but contain NaNs or duplicates
            existing_ids = df["local_row_id"].dropna().astype(str)
            if len(existing_ids) != len(df) or existing_ids.duplicated().any():
                logger.warning(
                    f"'local_row_id' column exists in {file_path} but contains nulls or duplicates. Regenerating IDs."
                )
                df["local_row_id"] = [str(uuid.uuid4()) for _ in range(len(df))]

        # *** REMOVED adding 'æ¥æº' column here ***
        # df["æ¥æº"] = os.path.basename(file_path)

        # Fill NaN values with empty strings BEFORE processing
        df = df.fillna("")  # Important: Prevents issues with NaN in string operations

        return df

    except FileNotFoundError:
        # æ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨
        print(f"   âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        return None
    except ImportError as e:
        # ç¼ºå°‘æ ¸å¿ƒåº“ (é€šå¸¸æ˜¯ pandas)
        print(f"   âŒ ç¼ºå°‘å¿…è¦çš„åº“: {e}ã€‚è¯·ç¡®ä¿å·²å®‰è£… pandas, openpyxl, xlrdã€‚")
        return None
    except Exception as e:
        # æ•è·æ‰€æœ‰å…¶ä»–æœªé¢„æ–™åˆ°çš„å¼‚å¸¸
        print(f"   âŒ è¯»å–æ–‡ä»¶ '{file_path}' æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯:")
        # æ‰“å°è¯¦ç»†çš„é”™è¯¯å †æ ˆä¿¡æ¯
        print(traceback.format_exc())
        return None


def extract_standardize_batch_with_llm(
    batch_rows: list[dict],  # å½“å‰æ‰¹æ¬¡çš„æ•°æ®è¡Œ (å­—å…¸åˆ—è¡¨)
    source_headers: list,  # æºæ–‡ä»¶çš„åˆ—æ ‡é¢˜åˆ—è¡¨
    target_columns: list,  # ç›®æ ‡è¾“å‡ºåˆ—ååˆ—è¡¨
    api_key: str,  # DeepSeek API Key
    api_endpoint: str,  # DeepSeek API ç«¯ç‚¹ URL
    max_tokens: int,  # API è°ƒç”¨å…è®¸çš„æœ€å¤§å®Œæˆ token æ•°
    timeout: int,  # API è¯·æ±‚çš„è¶…æ—¶æ—¶é—´ (ç§’)
) -> Union[List[Dict], None]:
    """
    ä½¿ç”¨ DeepSeek LLM API å¯¹ä¸€æ‰¹æ•°æ®è¿›è¡Œä¿¡æ¯æå–å’Œæ ‡å‡†åŒ–ã€‚
    åŒ…å«é‡è¯•é€»è¾‘å’Œå¸¸è§çš„ API é”™è¯¯å¤„ç†ã€‚

    Args:
        batch_rows: å½“å‰æ‰¹æ¬¡çš„æ•°æ®ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œä»£è¡¨ä¸€è¡Œã€‚
        source_headers: æº Excel/CSV çš„åˆ—æ ‡é¢˜åˆ—è¡¨ã€‚
        target_columns: éœ€è¦æå–å’Œç”Ÿæˆçš„æ ‡å‡†åˆ—ååˆ—è¡¨ã€‚
        api_key: DeepSeek API å¯†é’¥ã€‚
        api_endpoint: DeepSeek API çš„ URLã€‚
        max_tokens: API è°ƒç”¨æ—¶ `max_tokens` å‚æ•°ã€‚
        timeout: API è¯·æ±‚çš„è¶…æ—¶æ—¶é—´ (ç§’)ã€‚

    Returns:
        Union[List[Dict], None]: æˆåŠŸå¤„ç†åˆ™è¿”å›æ ‡å‡†åŒ–åçš„æ•°æ®åˆ—è¡¨ (å¯èƒ½åŒ…å«é”™è¯¯æ ‡è®°)ï¼Œ
                           å¦‚æœ API è°ƒç”¨å¤±è´¥æˆ–å‘ç”Ÿä¸¥é‡é”™è¯¯ï¼Œåˆ™è¿”å› Noneã€‚
                           å¦‚æœ API Key æ— æ•ˆï¼Œåˆ™è¿”å›åŒ…å« API_KEY_ERROR æ ‡è®°çš„åˆ—è¡¨ã€‚
    """
    # å¦‚æœè¾“å…¥æ‰¹æ¬¡ä¸ºç©ºï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨
    if not batch_rows:
        return []

    # æ£€æŸ¥ API Key æ˜¯å¦æœ‰æ•ˆ
    if not api_key or not api_key.startswith("sk-"):
        print("âš ï¸ DeepSeek API Key ç¼ºå¤±æˆ–æ— æ•ˆã€‚è·³è¿‡ LLM å¤„ç†ã€‚")
        # ä¸ºæ‰¹æ¬¡ä¸­çš„æ¯ä¸€è¡Œè¿”å›ä¸€ä¸ªé”™è¯¯æ ‡è®°å­—å…¸
        return [{col: "API_KEY_ERROR" for col in target_columns} for _ in batch_rows]

    # --- å‡†å¤‡ API è¯·æ±‚ ---
    # å°†ç›®æ ‡åˆ—ååˆ—è¡¨è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²ï¼Œç”¨äº Prompt
    target_schema_json = json.dumps(target_columns, ensure_ascii=False)
    # å°†æºåˆ—æ ‡é¢˜è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œå†è½¬ä¸º JSON å­—ç¬¦ä¸²
    source_headers_str = [str(h) for h in source_headers]
    source_headers_json = json.dumps(source_headers_str, ensure_ascii=False)

    # æ„å»º System Prompt (ç³»ç»Ÿæ¶ˆæ¯)
    system_content = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®å¤„ç†å¼•æ“ã€‚ä½ çš„æ ¸å¿ƒä»»åŠ¡æ˜¯ä»ç”¨æˆ·æä¾›çš„æºæ•°æ®æ‰¹æ¬¡ (Source Batch Data) ä¸­ï¼Œä¸ºæ¯ä¸€è¡Œæ•°æ®æå–ä¿¡æ¯ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šçš„ç›®æ ‡æ¨¡å¼ (Target Schema) è¿›è¡Œæ ‡å‡†åŒ–ã€‚
**é‡è¦ï¼šä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ JSON æ•°ç»„ (åˆ—è¡¨)ï¼Œæ•°ç»„ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªå¯¹åº”è¾“å…¥è¡Œçš„ã€ç¬¦åˆ Target Schema çš„ JSON å¯¹è±¡ã€‚å…ƒç´ çš„æ•°é‡å¿…é¡»ä¸è¾“å…¥æ•°ç»„å®Œå…¨ä¸€è‡´ã€‚**
**ç›®æ ‡æ¨¡å¼ (Target Schema):**
```json
{target_schema_json}
```
**å½“æºæ•°æ®ä¸­ç”µè¯/æ‰‹æœºå·åˆ—åŒ…å«å¤šä¸ªå·ç æ—¶ï¼ˆå¯èƒ½ç”¨åˆ†å·ã€é€—å·ã€ç©ºæ ¼ç­‰åˆ†éš”ï¼‰ï¼Œä½ å¿…é¡»å°†è¯¥è¡Œæ‹†åˆ†ä¸ºå¤šè¡Œï¼Œæ¯è¡Œå¯¹åº”ä¸€ä¸ªæ‰‹æœºå·ï¼Œå…¶ä»–ä¿¡æ¯ä¿æŒä¸€è‡´ã€‚**
**æ‰‹æœºå·åˆ†éš”ç¬¦è¯†åˆ«è§„åˆ™ï¼š**
- æ”¯æŒçš„åˆ†éš”ç¬¦åŒ…æ‹¬ï¼šä¸­è‹±æ–‡åˆ†å·ï¼ˆ;ï¼›ï¼‰ã€ä¸­è‹±æ–‡é€—å·ï¼ˆ,ï¼Œï¼‰ã€ç©ºæ ¼ã€æ–œæ ï¼ˆ/ï¼‰
- ä¸€è¡Œä¸­å¯èƒ½åŒ…å«å¤šä¸ªä¸åŒç±»å‹çš„åˆ†éš”ç¬¦

**å¤„ç†è§„åˆ™:**
1.ç‹¬ç«‹å¤„ç†: ç‹¬ç«‹åˆ†æ "Source Batch Data" æ•°ç»„ä¸­çš„æ¯ä¸€ä¸ª JSON å¯¹è±¡ã€‚
2.æå–ä¸æ˜ å°„: ç»“åˆ "Source Headers" ä¸Šä¸‹æ–‡ï¼Œå°†ä¿¡æ¯æ˜ å°„åˆ° "Target Schema" å­—æ®µã€‚æ‰¾ä¸åˆ°ä¿¡æ¯åˆ™å¯¹åº”å€¼è®¾ä¸º ""ã€‚æ‰€æœ‰å€¼å¿…é¡»ä¸ºå­—ç¬¦ä¸²ã€‚
3.æœ€ç»ˆè¾“å‡ºæ ¼å¼: ä½ çš„æœ€ç»ˆè¾“å‡ºå¿…é¡»ä¸”åªèƒ½æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ JSON æ•°ç»„ (åˆ—è¡¨)ã€‚
4.æ•°ç»„å†…å®¹: æ­¤æ•°ç»„åŒ…å« {len(batch_rows)} ä¸ªå…ƒç´ ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ç¬¦åˆ "Target Schema" çš„ JSON å¯¹è±¡ã€‚
5.ç»å¯¹ç¦æ­¢: ç»å¯¹ä¸è¦å°†æœ€ç»ˆçš„ JSON æ•°ç»„åŒ…è£…åœ¨ä»»ä½•å…¶ä»– JSON å¯¹è±¡æˆ–é”®ä¸­ï¼ˆä¾‹å¦‚ï¼Œä¸è¦åƒ {{ "results": [...] }} æˆ– {{ "processed_data": [...] }} è¿™æ ·ï¼‰ã€‚ç›´æ¥è¾“å‡º [ å¼€å¤´ï¼Œ] ç»“å°¾çš„æ•°ç»„æœ¬èº«ã€‚
6.æ— é¢å¤–å†…å®¹: ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šã€æ³¨é‡Šæˆ–æ ‡è®°ã€‚
è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚ï¼Œç›´æ¥ç”Ÿæˆæœ€ç»ˆçš„ JSON æ•°ç»„ã€‚
        """

    # å°†å½“å‰æ‰¹æ¬¡çš„æ•°æ®è¡Œè½¬æ¢ä¸º JSON å­—ç¬¦ä¸²ï¼Œç¡®ä¿æ‰€æœ‰å€¼ä¸ºå­—ç¬¦ä¸²
    batch_data_json = json.dumps(
        [{k: str(v) for k, v in row.items()} for row in batch_rows],
        ensure_ascii=False,  # å…è®¸é ASCII å­—ç¬¦
        indent=None,  # ä¸è¿›è¡Œç¼©è¿›ï¼Œå‡å°‘ token å ç”¨
    )

    print(f"source_headers_json: {source_headers_json}")
    print(f"batch_data_json: {batch_data_json}")
    # æ„å»º User Prompt (ç”¨æˆ·æ¶ˆæ¯)
    user_content = f"""
        è¿™æ˜¯æœ¬æ¬¡éœ€è¦å¤„ç†çš„æºæ•°æ®è¡¨æ ¼çš„åˆ—æ ‡é¢˜ (Source Headers):
        {source_headers_json}
        è¿™æ˜¯åŒ…å« {len(batch_rows)} è¡Œæºæ•°æ®çš„ JSON æ•°ç»„ (Source Batch Data):
        {batch_data_json}
        è¯·æ ¹æ®ä½ åœ¨ System æŒ‡ä»¤ä¸­è¢«èµ‹äºˆçš„è§’è‰²å’Œè§„åˆ™ï¼Œå¤„ç†è¿™ä¸ªæ‰¹æ¬¡çš„æ•°æ®ï¼Œå¹¶è¿”å›æ ‡å‡†åŒ–çš„ JSON æ•°ç»„ã€‚
        """

    # è®¾ç½®è¯·æ±‚å¤´
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
    # æ„å»ºè¯·æ±‚ä½“ (payload)
    payload = {
        "model": "deepseek-chat",  # å¯ä»¥è€ƒè™‘å°†å…¶ä¹Ÿè®¾ä¸ºå¯é…ç½®é¡¹
        "messages": [
            {"role": "system", "content": system_content},
            {"role": "user", "content": user_content},
        ],
        "max_tokens": max_tokens,
        "temperature": 1,  # å¯ä»¥è€ƒè™‘è®¾ä¸ºå¯é…ç½®é¡¹ï¼Œ1 è¡¨ç¤ºè¾ƒé«˜çš„åˆ›é€ æ€§/éšæœºæ€§
        # "stream": False, # æµå¼è¾“å‡ºç›®å‰ä¸é€‚ç”¨äºæ‰¹å¤„ç†è§£æ
    }

    # --- API è°ƒç”¨ä¸é”™è¯¯å¤„ç† ---
    max_retries = 2  # æœ€å¤§é‡è¯•æ¬¡æ•°
    retry_delay = 5  # é‡è¯•é—´éš”æ—¶é—´ (ç§’)

    # å¾ªç¯å°è¯•è°ƒç”¨ API
    for attempt in range(max_retries):
        try:
            print(
                f"      ... å‘é€æ‰¹æ¬¡æ•°æ®åˆ° LLM API (å°è¯• {attempt + 1}/{max_retries})..."
            )
            # å‘é€ POST è¯·æ±‚
            response = requests.post(
                api_endpoint,
                headers=headers,
                json=payload,
                timeout=timeout,  # ä½¿ç”¨ä¼ å…¥çš„è¶…æ—¶æ—¶é—´
            )
            # æ£€æŸ¥ HTTP çŠ¶æ€ç ï¼Œå¦‚æœä¸æ˜¯ 2xxï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
            response.raise_for_status()
            # è§£æè¿”å›çš„ JSON æ•°æ®
            result_json = response.json()
            # (è°ƒè¯•ç”¨) æ‰“å°å®Œæ•´çš„è¿”å› JSON
            # print(json.dumps(result_json, ensure_ascii=False, indent=2))

            content_str = None
            # æå– API ä½¿ç”¨æƒ…å†µ (token æ•°é‡)
            usage_info = result_json.get("usage")
            if usage_info:
                print(
                    f"      [API ä½¿ç”¨æƒ…å†µ]: Prompt: {usage_info.get('prompt_tokens', 'N/A')}, Completion: {usage_info.get('completion_tokens', 'N/A')}, Total: {usage_info.get('total_tokens', 'N/A')}"
                )

            # ä»è¿”å›ç»“æœä¸­æå–æ¨¡å‹ç”Ÿæˆçš„å†…å®¹
            if "choices" in result_json and result_json["choices"]:
                message = result_json["choices"][0].get("message", {})
                content_str = message.get("content")
            print(f"content_str: {content_str}")
            # --- è§£æå’ŒéªŒè¯ LLM è¿”å›çš„å†…å®¹ ---
            if content_str:
                try:
                    # æ¸…ç†è¿”å›å†…å®¹ä¸¤ç«¯çš„ç©ºç™½å­—ç¬¦å’Œå¯èƒ½çš„ä»£ç å—æ ‡è®°
                    content_str_cleaned = content_str.strip()
                    if content_str_cleaned.startswith("```json"):
                        content_str_cleaned = content_str_cleaned[7:-3].strip()
                    elif content_str_cleaned.startswith("```"):
                        content_str_cleaned = content_str_cleaned[3:-3].strip()

                    # å°è¯•å°†æ¸…ç†åçš„å­—ç¬¦ä¸²è§£æä¸º JSON (é¢„æœŸæ˜¯ä¸€ä¸ªåˆ—è¡¨)
                    standardized_batch = json.loads(content_str_cleaned)

                    # éªŒè¯è¿”å›çš„æ˜¯å¦ä¸ºåˆ—è¡¨ï¼Œä¸”åˆ—è¡¨é•¿åº¦å¤§äºç­‰äºè¾“å…¥æ‰¹æ¬¡é•¿åº¦ï¼ˆæ”¯æŒå¤šæ‰‹æœºå·æ‹†åˆ†ï¼‰
                    if isinstance(standardized_batch, list) and len(
                        standardized_batch
                    ) >= len(batch_rows):
                        if len(standardized_batch) > len(batch_rows):
                            print(
                                f"      -> LLMè¿”å›äº† {len(standardized_batch)} è¡Œï¼ˆåŸå§‹è¾“å…¥ {len(batch_rows)} è¡Œï¼‰ï¼Œå¯èƒ½åŒ…å«æ‰‹æœºå·æ‹†åˆ†åçš„å¤šè¡Œæ•°æ®ã€‚"
                            )
                        validated_batch = []
                        # è¿›ä¸€æ­¥éªŒè¯åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ æ˜¯å¦ä¸ºå­—å…¸ï¼Œå¹¶æŒ‰ç›®æ ‡åˆ—æ ¼å¼åŒ–
                        for i, item in enumerate(standardized_batch):
                            if isinstance(item, dict):
                                # ç¡®ä¿æ‰€æœ‰ç›®æ ‡åˆ—éƒ½å­˜åœ¨ï¼Œä¸”å€¼ä¸ºå­—ç¬¦ä¸²ï¼Œç¼ºå¤±å€¼ç”¨ç©ºå­—ç¬¦ä¸²å¡«å……
                                validated_item = {
                                    col: str(item.get(col, ""))
                                    for col in target_columns
                                }
                                validated_batch.append(validated_item)
                            else:
                                # å¦‚æœæŸä¸€é¡¹ä¸æ˜¯å­—å…¸ï¼Œè®°å½•é”™è¯¯
                                print(
                                    f"      âš ï¸ LLM è¿”å›ç»“æœçš„ç¬¬ {i+1} é¡¹ä¸æ˜¯å­—å…¸: {item}"
                                )
                                validated_batch.append(
                                    {
                                        col: "LLM_ITEM_FORMAT_ERROR"  # æ ‡è®°æ ¼å¼é”™è¯¯
                                        for col in target_columns
                                    }
                                )
                        print(f"      -> LLM æ‰¹å¤„ç†æˆåŠŸ ({len(validated_batch)} è¡Œ)ã€‚")
                        return validated_batch  # æˆåŠŸå¤„ç†ï¼Œè¿”å›ç»“æœ
                    else:
                        # å¦‚æœè¿”å›çš„ä¸æ˜¯åˆ—è¡¨æˆ–é•¿åº¦ä¸åŒ¹é…
                        details = f"ç±»å‹: {type(standardized_batch).__name__}" + (
                            f", é•¿åº¦: {len(standardized_batch)}"
                            if isinstance(standardized_batch, list)
                            else ""
                        )
                        print(
                            f"      âŒ LLM è¿”å›äº†æ— æ•ˆåˆ—è¡¨æˆ–é•¿åº¦å¼‚å¸¸ (é¢„æœŸè‡³å°‘ {len(batch_rows)} è¡Œ)ã€‚å®é™… -> {details} (å°è¯• {attempt + 1})ã€‚"
                        )
                        # è¿›å…¥é‡è¯•æµç¨‹ (å¦‚æœè¿˜æœ‰é‡è¯•æ¬¡æ•°)

                except json.JSONDecodeError as json_err:
                    # å¦‚æœè¿”å›çš„å†…å®¹æ— æ³•è§£æä¸º JSON
                    print(
                        f"      âŒ LLM è¿”å›å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ•°ç»„ (å°è¯• {attempt + 1}): {json_err}"
                    )
                    # æ‰“å°éƒ¨åˆ†åŸå§‹è¿”å›å†…å®¹ï¼Œå¸®åŠ©è¯Šæ–­
                    print(
                        f"         åŸå§‹è¿”å›å†…å®¹ (å‰ 500 å­—ç¬¦): {content_str[:500]}..."
                    )
                    # è¿›å…¥é‡è¯•æµç¨‹
                except Exception as e:
                    # æ•è·è§£æè¿‡ç¨‹ä¸­å…¶ä»–æœªé¢„æ–™çš„é”™è¯¯
                    print(f"      âŒ è§£æ LLM è¿”å›çš„ JSON æ•°ç»„æ—¶å‡ºé”™: {e}")
                    print(traceback.format_exc())
                    # é‡åˆ°æœªçŸ¥è§£æé”™è¯¯ï¼Œä¸å†é‡è¯•æ­¤æ‰¹æ¬¡
                    break

            else:
                # å¦‚æœ API è¿”å›ç»“æœä¸­æ²¡æœ‰ 'content'
                print(f"      âŒ LLM è¿”å›ç»“æœç¼ºå°‘ 'content' (å°è¯• {attempt + 1})ã€‚")
                if "error" in result_json:
                    print(f"         API é”™è¯¯ä¿¡æ¯: {result_json['error']}")
                # è¿›å…¥é‡è¯•æµç¨‹

        # --- æ•è·ç½‘ç»œå’Œ API ç›¸å…³çš„å¼‚å¸¸ ---
        except requests.exceptions.Timeout:
            print(f"      âŒ LLM API è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1})ã€‚")
            # è¿›å…¥é‡è¯•æµç¨‹
        except requests.exceptions.HTTPError as http_err:
            # æ•è· HTTP é”™è¯¯ (å¦‚ 4xx, 5xx)
            print(f"      âŒ LLM API HTTP é”™è¯¯ (å°è¯• {attempt + 1}): {http_err}")
            if http_err.response is not None:
                status_code = http_err.response.status_code
                response_text = http_err.response.text
                print(f"         çŠ¶æ€ç : {status_code}")
                # ç‰¹æ®Šå¤„ç†ï¼šä¸Šä¸‹æ–‡é•¿åº¦è¶…é™é”™è¯¯ (400 Bad Request)
                if status_code == 400 and (
                    "context_length_exceeded" in response_text.lower()
                    or "prompt is too long" in response_text.lower()
                    or "maximum context length" in response_text.lower()
                ):
                    print(
                        "      âŒ é”™è¯¯: è¾“å…¥å†…å®¹å¯èƒ½è¶…è¿‡æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶! è¯·å‡å° BATCH_SIZEã€‚"
                    )
                    # æŠ›å‡º ValueErrorï¼Œè¿™å°†ç»ˆæ­¢æ•´ä¸ªä»»åŠ¡çš„å¤„ç†
                    raise ValueError("ä¸Šä¸‹æ–‡é•¿åº¦è¶…é™")
                # ç‰¹æ®Šå¤„ç†ï¼šè®¤è¯/æˆæƒé”™è¯¯ (401, 403)
                elif status_code in [401, 403]:
                    print(
                        f"      âŒ API è®¤è¯/æˆæƒé”™è¯¯ ({status_code})ã€‚è¯·æ£€æŸ¥ API Keyã€‚"
                    )
                    raise ValueError("API è®¤è¯/æˆæƒé”™è¯¯")
                # ç‰¹æ®Šå¤„ç†ï¼šé€Ÿç‡é™åˆ¶é”™è¯¯ (429)
                elif status_code == 429:
                    print(f"      âŒ è¾¾åˆ° API é€Ÿç‡é™åˆ¶ ({status_code})ã€‚")
                    # å¦‚æœè¿˜æœ‰é‡è¯•æ¬¡æ•°ï¼Œåˆ™ç­‰å¾…æ›´é•¿æ—¶é—´åé‡è¯•
                    if attempt < max_retries - 1:
                        wait_time = retry_delay * (attempt + 2)  # å¢åŠ ç­‰å¾…æ—¶é—´
                        print(f"         å°†åœ¨ {wait_time} ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue  # ç»§ç»­ä¸‹ä¸€æ¬¡å¾ªç¯å°è¯•
                    else:
                        # é‡è¯•æ¬¡æ•°ç”¨å°½
                        print("      âŒ é‡è¯•åä»ç„¶è¾¾åˆ°é€Ÿç‡é™åˆ¶ã€‚")
                        raise ValueError("API é€Ÿç‡é™åˆ¶")
                else:
                    # å…¶ä»– HTTP é”™è¯¯
                    print(f"         å“åº”å†…å®¹ (å‰ 500 å­—ç¬¦): {response_text[:500]}...")
            else:
                print("      âŒ HTTP é”™è¯¯ä½†æ²¡æœ‰å“åº”å¯¹è±¡ã€‚")
            # å¯¹äº HTTP é”™è¯¯ (é 429 ä¸”éè‡´å‘½é”™è¯¯)ï¼Œè¿›å…¥é‡è¯•æµç¨‹ (å¦‚æœè¿˜æœ‰æ¬¡æ•°)
        except requests.exceptions.RequestException as e:
            # æ•è·å…¶ä»–ç½‘ç»œè¯·æ±‚ç›¸å…³çš„é”™è¯¯ (å¦‚ DNS è§£æå¤±è´¥ã€è¿æ¥é”™è¯¯ç­‰)
            print(f"      âŒ LLM API è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
            # è¿›å…¥é‡è¯•æµç¨‹
        except Exception as e:
            # æ•è·è°ƒç”¨ API è¿‡ç¨‹ä¸­çš„å…¶ä»–æœªçŸ¥é”™è¯¯
            print(f"      âŒ è°ƒç”¨ LLM API æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ (æ‰¹å¤„ç†): {e}")
            print(traceback.format_exc())
            # é‡æ–°æŠ›å‡ºæœªçŸ¥é”™è¯¯ï¼Œç»ˆæ­¢å¤„ç†
            raise e

        # --- é‡è¯•é€»è¾‘ ---
        # å¦‚æœå½“å‰å°è¯•å¤±è´¥ä¸”è¿˜æœ‰é‡è¯•æ¬¡æ•°
        if attempt < max_retries - 1:
            print(f"      å°†åœ¨ {retry_delay} ç§’åé‡è¯•...")
            time.sleep(retry_delay)
        else:
            # é‡è¯•æ¬¡æ•°å·²ç”¨å®Œ
            print(f"      âŒ æ­¤æ‰¹æ¬¡å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚")

    # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
    print(f"      âš ï¸ å¤„ç†å¤±è´¥ï¼Œä¸ºæ­¤æ‰¹æ¬¡è¿”å› Noneã€‚")
    return None  # è¿”å› None è¡¨ç¤ºæ­¤æ‰¹æ¬¡å¤„ç†å¤±è´¥


# === ä¸»å¤„ç†å‡½æ•° ===
def process_files_and_consolidate(
    input_files: List[str],  # è¾“å…¥æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    output_file_path: str,  # è¾“å‡º Excel æ–‡ä»¶è·¯å¾„
    config: dict,  # åŒ…å«é…ç½®é¡¹çš„å­—å…¸ (API Key, Target Columns ç­‰)
    update_progress_callback=None,  # ç”¨äºæŠ¥å‘Šè¿›åº¦çš„å›è°ƒå‡½æ•° (å¯é€‰)
):
    """
    æ ¸å¿ƒå¤„ç†æµç¨‹å‡½æ•°ï¼Œç”± app.py åœ¨åå°çº¿ç¨‹ä¸­è°ƒç”¨ã€‚
    1. éå†è¾“å…¥æ–‡ä»¶åˆ—è¡¨ã€‚
    2. è¯»å–æ¯ä¸ªæ–‡ä»¶ã€‚
    3. å°†æ–‡ä»¶å†…å®¹åˆ†æ‰¹ã€‚
    4. è°ƒç”¨ LLM API å¤„ç†æ¯ä¸ªæ‰¹æ¬¡ã€‚
    5. åˆå¹¶å¤„ç†ç»“æœã€‚
    6. å°†æœ€ç»ˆç»“æœä¿å­˜åˆ° Excel æ–‡ä»¶ã€‚
    7. é€šè¿‡å›è°ƒå‡½æ•°æŠ¥å‘Šè¿›åº¦ã€‚

    Args:
        input_files: åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥æ–‡ä»¶å®Œæ•´è·¯å¾„çš„åˆ—è¡¨ã€‚
        output_file_path: è¦ä¿å­˜çš„æœ€ç»ˆ Excel æ–‡ä»¶çš„å®Œæ•´è·¯å¾„ã€‚
        config: åŒ…å«æ‰€éœ€é…ç½®çš„å­—å…¸ï¼Œå¦‚ TARGET_COLUMNS, DEEPSEEK_API_KEY ç­‰ã€‚
        update_progress_callback: ä¸€ä¸ªå‡½æ•°ï¼Œæ¥å— (çŠ¶æ€æ¶ˆæ¯, è¿›åº¦ç™¾åˆ†æ¯”, å·²å¤„ç†æ–‡ä»¶æ•°, æ€»æ–‡ä»¶æ•°) å‚æ•°ã€‚
                                  ç”¨äºå‘ app.py æŠ¥å‘Šè¿›åº¦ã€‚

    Returns:
        str: æˆåŠŸå¤„ç†æ—¶è¿”å›è¾“å‡ºæ–‡ä»¶çš„è·¯å¾„ã€‚

    Raises:
        ValueError: å¦‚æœæ²¡æœ‰è¾“å…¥æ–‡ä»¶ã€API è®¤è¯å¤±è´¥ã€ä¸Šä¸‹æ–‡è¶…é™æˆ–æœ€ç»ˆæ²¡æœ‰æå–åˆ°ä»»ä½•æ•°æ®ã€‚
        Exception: å…¶ä»–æœªå¤„ç†çš„å¼‚å¸¸ (å¦‚æ–‡ä»¶å†™å…¥å¤±è´¥)ã€‚
    """
    # --- 1. è§£åŒ…é…ç½®é¡¹ ---
    target_columns_config = config.get(
        "TARGET_COLUMNS",
        ["å…¬å¸åç§°", "è”ç³»äºº", "èŒä½", "ç”µè¯", "æ¥æº"],  # Keep 'æ¥æº' in config default
    )
    source_column_name = "æ¥æº"

    # *** ä¿®æ”¹: å‡†å¤‡ç»™ LLM çš„ç›®æ ‡åˆ— (ä¸å«æ¥æº) ***
    llm_target_columns = [
        col for col in target_columns_config if col != source_column_name
    ]
    # *** ä¿®æ”¹: æœ€ç»ˆè¾“å‡ºåˆ— (åŒ…å«æ¥æºå’Œ local_row_id) ***
    final_output_columns = target_columns_config + ["local_row_id"]
    # Ensure local_row_id is last and deduplicate if needed
    final_output_columns = list(dict.fromkeys(final_output_columns))
    if "local_row_id" in final_output_columns:
        final_output_columns.remove("local_row_id")
    final_output_columns.append("local_row_id")

    api_key = config.get("DEEPSEEK_API_KEY", "")
    api_endpoint = config.get(
        "DEEPSEEK_API_ENDPOINT", "https://api.deepseek.com/chat/completions"
    )
    batch_size = config.get("BATCH_SIZE", 160)
    max_tokens = config.get("MAX_COMPLETION_TOKENS", 8192)
    api_timeout = config.get("API_TIMEOUT", 180)

    print(f"--- å¼€å§‹å¤„ç†ï¼Œæ‰¹å¤„ç†å¤§å°: {batch_size} ---")
    print(f"   LLM ç›®æ ‡åˆ— (ä¸å«æ¥æº): {llm_target_columns}")
    print(f"   æœ€ç»ˆè¾“å‡ºåˆ—: {final_output_columns}")

    consolidated_data = []
    total_rows_attempted = 0
    total_rows_successfully_processed = 0
    files_processed_count = 0
    files_with_errors = []
    total_files = len(input_files)
    # *** æ–°å¢ï¼šç”¨äºå­˜å‚¨ local_id åˆ°æ¥æºæ–‡ä»¶åçš„æ˜ å°„ ***
    local_id_to_source_map = {}

    # --- 2. æ£€æŸ¥è¾“å…¥æ–‡ä»¶ ---
    if not input_files:
        print("âŒ é”™è¯¯: æœªæä¾›ä»»ä½•è¾“å…¥æ–‡ä»¶ã€‚")
        # å¦‚æœæœ‰å›è°ƒå‡½æ•°ï¼ŒæŠ¥å‘Šé”™è¯¯å¹¶è®¾ç½®è¿›åº¦ä¸º 100% (è¡¨ç¤ºç»“æŸ)
        if update_progress_callback:
            update_progress_callback("é”™è¯¯: æœªæä¾›è¾“å…¥æ–‡ä»¶", 100, 0, 0)
        # æŠ›å‡ºå¼‚å¸¸ï¼Œç»ˆæ­¢å¤„ç†
        raise ValueError("æœªæä¾›ä»»ä½•è¾“å…¥æ–‡ä»¶")

    # --- 3. å¾ªç¯å¤„ç†æ¯ä¸ªæ–‡ä»¶ ---
    for i, file_path in enumerate(input_files):
        current_file_num = i + 1
        file_basename = os.path.basename(file_path)
        print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶ {current_file_num}/{total_files}: {file_basename}")
        # æŠ¥å‘Šå¼€å§‹å¤„ç†å½“å‰æ–‡ä»¶
        if update_progress_callback:
            # è¿›åº¦åŸºäºå·²å¼€å§‹å¤„ç†çš„æ–‡ä»¶æ•°é‡è®¡ç®—
            progress_pct = int((i / total_files) * 100)
            update_progress_callback(
                f"è¯»å–æ–‡ä»¶ {current_file_num}/{total_files}: {file_basename}",
                progress_pct,
                i,  # å·²å®Œæˆçš„æ–‡ä»¶æ•° (ä» 0 å¼€å§‹)
                total_files,
            )

        # --- 3.1 è¯»å–æ–‡ä»¶ ---
        df_source = read_input_file(file_path)

        # å¦‚æœæ–‡ä»¶è¯»å–å¤±è´¥ (è¿”å› None)
        if df_source is None or df_source.empty:
            print(f"   âš ï¸ è·³è¿‡æ–‡ä»¶ {file_basename} (è¯»å–å¤±è´¥)ã€‚")
            files_with_errors.append(file_basename)
            # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶
            continue
        # å¦‚æœæ–‡ä»¶ä¸ºç©º
        if df_source.empty:
            print(f"   â„¹ï¸ æ–‡ä»¶ {file_basename} ä¸ºç©ºã€‚è·³è¿‡ã€‚")
            # å³ä½¿ä¸ºç©ºä¹Ÿç®—ä½œå·²å¤„ç† (å› ä¸ºå®ƒè¢«æˆåŠŸè¯»å–äº†)
            # files_processed_count += 1 # æ ¹æ®éœ€è¦å†³å®šæ˜¯å¦å°†ç©ºæ–‡ä»¶è®¡å…¥å·²å¤„ç†
            continue

        # æ–‡ä»¶æˆåŠŸè¯»å–ä¸”ä¸ä¸ºç©º
        files_processed_count += 1
        num_rows_in_file = len(df_source)  # å½“å‰æ–‡ä»¶è¡Œæ•°
        print(f"   æ–‡ä»¶åŒ…å« {num_rows_in_file} è¡Œã€‚")

        # *** æ–°å¢ï¼šå¡«å…… local_id åˆ°æ¥æºçš„æ˜ å°„ ***
        for local_id in df_source["local_row_id"].tolist():
            local_id_to_source_map[local_id] = file_basename
        print(
            f"   å·²è®°å½• {len(df_source)} ä¸ª local_id åˆ°æ¥æº '{file_basename}' çš„æ˜ å°„ã€‚"
        )

        # Get source headers
        source_headers = df_source.columns.astype(str).tolist()
        rows_processed_in_file_success = 0
        num_batches = math.ceil(num_rows_in_file / batch_size)

        # --- 3.2 åˆ†æ‰¹å¤„ç†æ–‡ä»¶å†…å®¹ ---
        for batch_num, batch_start_index in enumerate(
            range(0, num_rows_in_file, batch_size)  # æŒ‰ batch_size æ­¥é•¿ç”Ÿæˆèµ·å§‹ç´¢å¼•
        ):
            # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„ç»“æŸç´¢å¼•
            batch_end_index = min(batch_start_index + batch_size, num_rows_in_file)
            # ä» DataFrame ä¸­åˆ‡ç‰‡å‡ºå½“å‰æ‰¹æ¬¡çš„æ•°æ®
            current_batch_df = df_source.iloc[batch_start_index:batch_end_index]
            # Extract local_row_ids for this batch
            batch_local_ids = current_batch_df["local_row_id"].tolist()

            # å°†æ‰¹æ¬¡ DataFrame è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨ for LLM input, EXCLUDING local_row_id AND æ¥æº (if exists)
            llm_input_batch_list = (
                current_batch_df.drop(
                    columns=["local_row_id", source_column_name], errors="ignore"
                )
                .fillna("")
                .astype(str)
                .to_dict("records")
            )

            # ç”¨äºæ—¥å¿—è¾“å‡ºçš„èµ·å§‹å’Œç»“æŸè¡Œå· (ä» 1 å¼€å§‹)
            batch_start_row_num = batch_start_index + 1
            batch_end_row_num = batch_end_index
            print(
                f"   >> å¤„ç†æ‰¹æ¬¡ {batch_num + 1}/{num_batches} (è¡Œ {batch_start_row_num}-{batch_end_row_num})ï¼Œæ–‡ä»¶: {file_basename}"
            )

            # --- 3.3 æ›´æ–°è¯¦ç»†è¿›åº¦ (åŸºäºæ‰¹æ¬¡) ---
            if update_progress_callback:
                # è¿›åº¦åŸºäºå·² *å®Œæˆ* çš„æ‰¹æ¬¡æ•°è®¡ç®—
                batches_done_in_file = batch_num + 1
                file_progress_pct = (batches_done_in_file / num_batches) * 100
                # è®¡ç®—æ€»ä½“è¿›åº¦ï¼š(å·²å®Œæˆæ–‡ä»¶æ•° + å½“å‰æ–‡ä»¶å†…éƒ¨å®Œæˆåº¦) / æ€»æ–‡ä»¶æ•°
                # i ä»£è¡¨å·²å®Œæˆçš„æ–‡ä»¶æ•° (0-based), i+1 æ˜¯å½“å‰æ–‡ä»¶ç¼–å· (1-based)
                overall_progress_pct = int(
                    ((i + (batches_done_in_file / num_batches)) / total_files) * 100
                )
                # é™åˆ¶è¿›åº¦æœ€å¤§ä¸º 99%ï¼Œå› ä¸º 100% åº”è¯¥åœ¨ä¿å­˜å®Œæˆåæ‰æŠ¥å‘Š
                overall_progress_pct = min(overall_progress_pct, 99)

                # æ„é€ çŠ¶æ€æ¶ˆæ¯
                status_msg = f"å¤„ç†æ–‡ä»¶ {current_file_num}/{total_files} ({file_basename}) - å®Œæˆæ‰¹æ¬¡ {batches_done_in_file}/{num_batches}"
                # è°ƒç”¨å›è°ƒå‡½æ•°æ›´æ–°è¿›åº¦
                update_progress_callback(
                    status_msg,
                    overall_progress_pct,
                    i,
                    total_files,  # æ³¨æ„ï¼šfiles_processed å‚æ•°ä»ä¸º i (å·²å®Œæˆçš„æ–‡ä»¶æ•°)
                )

            total_rows_attempted += len(llm_input_batch_list)  # ç´¯åŠ å°è¯•å¤„ç†çš„è¡Œæ•°

            # --- 3.4 è°ƒç”¨ LLM å¤„ç†æ‰¹æ¬¡ ---
            try:
                standardized_batch_result = extract_standardize_batch_with_llm(
                    llm_input_batch_list,
                    source_headers,
                    llm_target_columns,  # *** ä¿®æ”¹ï¼šä¼ å…¥ä¸å«"æ¥æº"çš„åˆ— ***
                    api_key,
                    api_endpoint,
                    max_tokens,
                    api_timeout,
                )
            except ValueError as ve:
                # æ•è·ç”± extract_standardize_batch_with_llm æŠ›å‡ºçš„ä¸¥é‡é”™è¯¯ (å¦‚ä¸Šä¸‹æ–‡è¶…é™ã€è®¤è¯å¤±è´¥)
                print(f"   âŒ LLM è°ƒç”¨æœŸé—´å‘ç”Ÿä¸¥é‡é”™è¯¯ (æ‰¹æ¬¡ {batch_num+1}): {ve}")
                # æŠ¥å‘Šé”™è¯¯å¹¶åœæ­¢æ•´ä¸ªä»»åŠ¡
                if update_progress_callback:
                    update_progress_callback(
                        f"ä¸¥é‡é”™è¯¯: {ve}ã€‚å¤„ç†å·²åœæ­¢ã€‚", 100, i, total_files
                    )
                raise ve  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œç»ˆæ­¢ process_files_and_consolidate
            except Exception as e:
                # æ•è· LLM è°ƒç”¨æœŸé—´å…¶ä»–æœªé¢„æ–™çš„é”™è¯¯
                print(f"   âŒ LLM è°ƒç”¨æœŸé—´å‘ç”Ÿæ„å¤–é”™è¯¯ (æ‰¹æ¬¡ {batch_num+1}): {e}")
                if update_progress_callback:
                    update_progress_callback(
                        f"å¤„ç†æ‰¹æ¬¡æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯ã€‚å¤„ç†å·²åœæ­¢ã€‚",
                        100,
                        i,
                        total_files,
                    )
                raise e  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œç»ˆæ­¢

            # --- 3.5 å¤„ç† LLM è¿”å›ç»“æœ ---
            processed_batch_with_ids_and_source = []  # Renamed list

            if isinstance(standardized_batch_result, list):
                # ä¿®æ”¹åˆ¤æ–­æ¡ä»¶ï¼Œå…è®¸LLMè¿”å›æ¯”è¾“å…¥æ›´å¤šçš„è¡Œï¼ˆæ”¯æŒå¤šæ‰‹æœºå·æ‹†åˆ†ï¼‰
                if len(standardized_batch_result) >= len(llm_input_batch_list):
                    # å¦‚æœLLMè¿”å›äº†æ›´å¤šè¡Œï¼Œå¯èƒ½æ˜¯å› ä¸ºè¿›è¡Œäº†æ‰‹æœºå·æ‹†åˆ†
                    if len(standardized_batch_result) > len(llm_input_batch_list):
                        print(
                            f"      å¤„ç†æ‹†åˆ†çš„æ‰‹æœºå·æ•°æ®ï¼ŒåŸå§‹è¾“å…¥ {len(llm_input_batch_list)} è¡Œï¼ŒLLMè¿”å› {len(standardized_batch_result)} è¡Œ"
                        )

                    # å»ºç«‹åŸå§‹è¡Œç´¢å¼•åˆ°å¯èƒ½çš„æ‹†åˆ†è¡Œæ˜ å°„å…³ç³»
                    original_row_mapping = {}
                    processed_rows = 0

                    # éå†LLMè¿”å›çš„æ¯ä¸€è¡Œç»“æœ
                    for result_idx, result_row in enumerate(standardized_batch_result):
                        if isinstance(result_row, dict):
                            # ç¡®å®šæ­¤è¡Œå¯¹åº”çš„åŸå§‹è¡Œç´¢å¼•
                            # å¯¹äºæ‹†åˆ†è¡Œï¼Œå¤šä¸ªç»“æœè¡Œä¼šå¯¹åº”åŒä¸€ä¸ªåŸå§‹è¡Œ
                            original_idx = min(result_idx, len(batch_local_ids) - 1)

                            # å½“å¤„ç†åˆ°æ–°çš„åŸå§‹è¡Œæ—¶ï¼Œå¢åŠ è®¡æ•°
                            if original_idx not in original_row_mapping:
                                original_row_mapping[original_idx] = 0

                            # è·å–è¿™ä¸ªåŸå§‹è¡Œçš„local_id
                            local_id = batch_local_ids[original_idx]

                            # ä¿å­˜ç»“æœè¡Œä¿¡æ¯
                            result_row["local_row_id"] = local_id
                            source_name = local_id_to_source_map.get(
                                local_id, "UNKNOWN_SOURCE"
                            )
                            result_row[source_column_name] = source_name
                            processed_batch_with_ids_and_source.append(result_row)

                            # å¢åŠ è¿™ä¸ªåŸå§‹è¡Œçš„å¤„ç†è®¡æ•°
                            original_row_mapping[original_idx] += 1
                            processed_rows += 1
                        else:
                            # å¤„ç†éå­—å…¸é¡¹
                            print(
                                f"      âš ï¸ è­¦å‘Š: LLM è¿”å›åˆ—è¡¨é¡¹ä¸æ˜¯å­—å…¸ (ç´¢å¼• {result_idx})ã€‚æ·»åŠ é”™è¯¯å ä½ç¬¦ã€‚"
                            )
                            # ç¡®å®šæœ€ç›¸è¿‘çš„åŸå§‹è¡Œç´¢å¼•
                            original_idx = min(result_idx, len(batch_local_ids) - 1)
                            local_id = batch_local_ids[original_idx]

                            error_row = {
                                col: "LLM_ITEM_FORMAT_ERROR"
                                for col in llm_target_columns
                            }
                            error_row["local_row_id"] = local_id
                            error_row[source_column_name] = local_id_to_source_map.get(
                                local_id, "UNKNOWN_SOURCE"
                            )
                            processed_batch_with_ids_and_source.append(error_row)

                    # æ‰“å°æ‹†åˆ†ç»“æœç»Ÿè®¡
                    split_stat = ", ".join(
                        [
                            f"è¡Œ{idx+1}: {count}æ¡"
                            for idx, count in original_row_mapping.items()
                        ]
                    )
                    print(f"      æ‹†åˆ†ç»Ÿè®¡ï¼š{split_stat}")

                    # Filter for valid results *after* adding ID and Source
                    valid_results_in_batch = [
                        res
                        for res in processed_batch_with_ids_and_source
                        if not any(
                            str(v).startswith(("LLM_", "API_KEY_"))
                            for k, v in res.items()
                            # Exclude local_id and æ¥æº from error check
                            if k not in ["local_row_id", source_column_name]
                        )
                    ]
                    consolidated_data.extend(
                        processed_batch_with_ids_and_source
                    )  # Add results with ID and Source
                    rows_processed_in_file_success += len(valid_results_in_batch)
                    total_rows_successfully_processed += len(valid_results_in_batch)
                    print(
                        f"      æ‰¹æ¬¡ {batch_num+1} å¤„ç†å®Œæˆã€‚æ”¶åˆ°å¹¶æ·»åŠ  ID å’Œæ¥æºåˆ° {len(processed_batch_with_ids_and_source)} æ¡ç»“æœ ({len(valid_results_in_batch)} æ¡æœ‰æ•ˆ)ã€‚"
                    )

                else:
                    # Handle length mismatch
                    print(f"      âŒ è­¦å‘Š: LLM è¿”å›åˆ—è¡¨é•¿åº¦ä¸åŒ¹é…... æ·»åŠ é”™è¯¯æ ‡è®°ã€‚")
                    for local_id in batch_local_ids:
                        error_row = {
                            col: "BATCH_LENGTH_MISMATCH" for col in llm_target_columns
                        }
                        error_row["local_row_id"] = local_id
                        error_row[source_column_name] = local_id_to_source_map.get(
                            local_id, "UNKNOWN_SOURCE"
                        )
                        consolidated_data.append(error_row)
            else:
                # Handle batch processing failure (LLM returned None)
                print(f"      âŒ æ‰¹æ¬¡ {batch_num+1} LLM å¤„ç†å¤±è´¥... æ·»åŠ é”™è¯¯æ ‡è®°ã€‚")
                for local_id in batch_local_ids:
                    error_row = {
                        col: "BATCH_PROCESSING_FAILED" for col in llm_target_columns
                    }
                    error_row["local_row_id"] = local_id
                    error_row[source_column_name] = local_id_to_source_map.get(
                        local_id, "UNKNOWN_SOURCE"
                    )
                    consolidated_data.append(error_row)

        # --- æ–‡ä»¶å¤„ç†å®Œæ¯• ---
        print(
            f"   âœ… å®Œæˆå¤„ç†æ–‡ä»¶ {file_basename}ã€‚å¤§çº¦æå–äº† {rows_processed_in_file_success} æ¡æœ‰æ•ˆè¡Œã€‚"
        )
        # (å¯é€‰) åœ¨å¤„ç†å®Œä¸€ä¸ªæ–‡ä»¶åï¼Œå¯ä»¥å†æ¬¡è°ƒç”¨å›è°ƒæŠ¥å‘Šè¿›åº¦
        # if update_progress_callback:
        #     overall_progress_pct = int(((i + 1) / total_files) * 100)
        #     update_progress_callback(f"å®Œæˆæ–‡ä»¶ {current_file_num}/{total_files}", overall_progress_pct, i + 1, total_files)

    # --- 4. æ‰€æœ‰æ–‡ä»¶å¤„ç†å¾ªç¯ç»“æŸ ---
    # æœ€ç»ˆè¿›åº¦æ›´æ–° (è®¾ç½®ä¸º 99%ï¼Œè¡¨ç¤ºå³å°†å®Œæˆä¿å­˜)
    # åœ¨å¾ªç¯ç»“æŸåä¸å†éœ€è¦æŠ¥å‘Š 99%ï¼Œæœ€åçš„ 100% åœ¨ä¿å­˜æˆåŠŸåæŠ¥å‘Š
    # if update_progress_callback:
    #     update_progress_callback(
    #         "åˆå¹¶ç»“æœå¹¶å‡†å¤‡ä¿å­˜...", 99, total_files, total_files
    #     )

    # æ‰“å°å¤„ç†æ€»ç»“ä¿¡æ¯
    print("\n--- å¤„ç†æ€»ç»“ ---")
    print(f"    æä¾›çš„è¾“å…¥æ–‡ä»¶æ€»æ•°: {total_files}")
    print(f"    æˆåŠŸè¯»å–å¹¶å¤„ç†çš„æ–‡ä»¶æ•°: {files_processed_count}")
    if files_with_errors:
        print(f"    è¯»å–å¤±è´¥çš„æ–‡ä»¶: {', '.join(files_with_errors)}")
    print(f"    å°è¯•å¤„ç†çš„æ€»è¡Œæ•°: {total_rows_attempted}")
    print(f"    å¤§çº¦æˆåŠŸæå–çš„è¡Œæ•°: {total_rows_successfully_processed}")
    print(f"    æœ€ç»ˆè¾“å‡ºè¡Œæ•°: {len(consolidated_data)} (å¯èƒ½åŒ…å«é”™è¯¯æ ‡è®°è¡Œ)")

    # --- 5. æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®éœ€è¦ä¿å­˜ ---
    if not consolidated_data:
        print("\nâŒ æœªæå–åˆ°ä»»ä½•æ•°æ®ã€‚æ— æ³•åˆ›å»ºè¾“å‡ºæ–‡ä»¶ã€‚")
        if update_progress_callback:
            update_progress_callback(
                "é”™è¯¯: æœªæå–åˆ°æœ‰æ•ˆæ•°æ®", 100, total_files, total_files
            )
        raise ValueError("æœªèƒ½æˆåŠŸå¤„ç†ä»»ä½•æ•°æ®ã€‚")

    # --- 6. åˆ›å»º DataFrame å¹¶ä¿å­˜åˆ° Excel ---
    # Ensure all dicts in consolidated_data have 'local_row_id' and 'æ¥æº', even if errors occurred
    for row_dict in consolidated_data:
        row_dict.setdefault("local_row_id", f"missing_uuid_{uuid.uuid4()}")
        row_dict.setdefault(source_column_name, "UNKNOWN_SOURCE_FINAL")

    # *** ä¿®æ”¹ï¼šä½¿ç”¨åŒ…å«"æ¥æº"çš„ final_output_columns ***
    df_final = pd.DataFrame(consolidated_data, columns=final_output_columns)

    print(f"\nğŸ’¾ ä¿å­˜æ•´åˆåçš„æ•°æ®åˆ°: {output_file_path}")
    try:
        # å°† DataFrame ä¿å­˜ä¸º Excel æ–‡ä»¶
        # index=False è¡¨ç¤ºä¸å°† DataFrame çš„ç´¢å¼•å†™å…¥æ–‡ä»¶
        # engine="openpyxl" æŒ‡å®šä½¿ç”¨ openpyxl å¼•æ“ (æ”¯æŒ .xlsx)
        # na_rep="" å°† NaN å€¼åœ¨ Excel ä¸­è¡¨ç¤ºä¸ºç©ºå­—ç¬¦ä¸²
        df_final.to_excel(output_file_path, index=False, engine="openpyxl", na_rep="")
        print(f"ğŸ‰ æ•°æ®æˆåŠŸä¿å­˜!")
        # æŠ¥å‘Šå¤„ç†å®Œæˆ
        if update_progress_callback:
            update_progress_callback("å¤„ç†å®Œæˆ", 100, total_files, total_files)
        # è¿”å›è¾“å‡ºæ–‡ä»¶çš„è·¯å¾„
        return output_file_path
    except ImportError:
        print("âŒ ä¿å­˜ Excel æ–‡ä»¶éœ€è¦ 'openpyxl' åº“ã€‚è¯·è¿è¡Œ: pip install openpyxl")
        raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸
    except PermissionError:
        print(
            f"âŒ å†™å…¥æ–‡ä»¶ '{output_file_path}' æ—¶å‘ç”Ÿæƒé™é”™è¯¯ã€‚è¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¢«å…¶ä»–ç¨‹åºæ‰“å¼€æˆ–æ˜¯å¦æœ‰å†™å…¥æƒé™ã€‚"
        )
        raise
    except Exception as e:
        print(f"âŒ ä¿å­˜ Excel æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        print(traceback.format_exc())
        raise


# === ç”¨äºç›´æ¥æµ‹è¯• processor.py çš„ç¤ºä¾‹ä»£ç  (æ³¨é‡Šæ‰) ===
# def print_progress(status, progress, files_done, total_files):
#     print(f"[è¿›åº¦æ›´æ–°] çŠ¶æ€: {status} | è¿›åº¦: {progress}% | æ–‡ä»¶: {files_done}/{total_files}")

# if __name__ == '__main__':
#     # ç”¨äºç›´æ¥è¿è¡Œæ­¤è„šæœ¬è¿›è¡Œæµ‹è¯•çš„è™šæ‹Ÿé…ç½®
#     test_config = {
#         "TARGET_COLUMNS": ["å…¬å¸åç§°", "å§“å", "èŒåŠ¡", "ç”µè¯"],
#         "DEEPSEEK_API_KEY": os.environ.get("DEEPSEEK_API_KEY", "YOUR_DUMMY_KEY_FOR_TESTING"), # ä»ç¯å¢ƒå˜é‡è·å–æˆ–ä½¿ç”¨è™šæ‹Ÿ Key
#         "BATCH_SIZE": 5, # ä½¿ç”¨è¾ƒå°çš„æ‰¹å¤„ç†å¤§å°è¿›è¡Œæµ‹è¯•
#         "DEEPSEEK_API_ENDPOINT": "https://api.deepseek.com/chat/completions",
#         "MAX_COMPLETION_TOKENS": 2048,
#         "API_TIMEOUT": 60, # æµ‹è¯•ç”¨è¾ƒçŸ­è¶…æ—¶
#     }
#     # æŒ‡å®šæµ‹è¯•ç”¨çš„è¾“å…¥æ–‡ä»¶åˆ—è¡¨ (ä½ éœ€è¦åˆ›å»ºè¿™äº›æ–‡ä»¶æˆ–ä¿®æ”¹è·¯å¾„)
#     test_input_files = ['./test_data/input1.xlsx', './test_data/input2.csv']
#     # æŒ‡å®šæµ‹è¯•ç”¨çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„
#     test_output = './test_output/consolidated_test_output.xlsx'

#     # ç¡®ä¿æµ‹è¯•è¾“å‡ºç›®å½•å­˜åœ¨
#     os.makedirs(os.path.dirname(test_output), exist_ok=True)

#     print(f"--- å¼€å§‹ç›´æ¥æµ‹è¯• processor.py ---")
#     print(f"æµ‹è¯•è¾“å…¥æ–‡ä»¶: {test_input_files}")
#     print(f"æµ‹è¯•è¾“å‡ºæ–‡ä»¶: {test_output}")

#     try:
#         # è°ƒç”¨ä¸»å¤„ç†å‡½æ•°è¿›è¡Œæµ‹è¯•ï¼Œå¹¶ä¼ å…¥æ‰“å°è¿›åº¦çš„å›è°ƒå‡½æ•°
#         result = process_files_and_consolidate(test_input_files, test_output, test_config, print_progress)
#         print(f"--- æµ‹è¯•å®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {result} ---")
#     except Exception as e:
#         print(f"--- æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ ---: {e}")
#         print(traceback.format_exc())
